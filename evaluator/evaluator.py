#!/usr/bin/env python3
"""
vulnerability-engine evaluator
"""
import json
import asyncio
import ssl
import asyncpg
from prometheus_client import Counter, Histogram

from common import mqueue
from common.config import Config
from common.logging import init_logging, get_logger
from common.failed_cache import FailedCache
from common.utils import send_msg_to_payload_tracker, send_remediations_update, a_ensure_minimal_schema_version, get_available_remediation_type, \
    release_semaphore
from common.vmaas_client import vmaas_request
from common.status_app import create_status_app, create_status_runner

LOGGER = get_logger(__name__)
CFG = Config()

PROMETHEUS_PORT = CFG.prometheus_port or str(CFG.evaluator_prometheus_port)

# database pool instance
DB_POOL = None

# prometheus probes
# times
VMAAS_EVAL_TIME = Histogram('ve_evaluator_vmaas_evaluation_seconds', 'Time spent checking a system for vmaas hits')
# counts
VMAAS_COUNT = Counter('ve_evaluator_vmaas_calls', 'Number of VMaaS-evaluations attempted')
INV_ID_NOT_FOUND = Counter('ve_evaluator_inventory_not_found', 'Number of times inventory-id not in SystemPlatform')
UNKNOWN_MSG = Counter('ve_evaluator_unknown_msg', 'Number of unrecognized messages delivered from queue')
UNKNOWN_TOPIC = Counter('ve_evaluator_unknown_topic', 'Number of times message delivered from unsupported topic')
MESSAGE_PARSE_ERROR = Counter('ve_evaluator_message_parse_error', '# of message parse errors')
VMAAS_ERRORS_SKIP = Counter('ve_evaluator_vmaas_errors_skip', '# of evaluations skipped due to VMaaS errors')

CONSUMER_QUEUE = mqueue.MQReader(CFG.evaluator_topics)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(CFG.payload_tracker_topic)
REMEDIATIONS_PRODUCER = mqueue.MQWriter(CFG.remediation_updates_topic)
EVALUATOR_RESULTS = mqueue.MQWriter(CFG.evaluator_results_topic)

MAIN_LOOP = asyncio.get_event_loop()
MAX_MESSAGES_SEMAPHORE = asyncio.BoundedSemaphore(CFG.max_loaded_evaluator_msgs)


async def _load_cves_for_inventory_id(rh_account_id, system_id, conn):
    system_cves_map = {}
    async for record in conn.cursor("""select cm.id, cm.cve, sv.when_mitigated, sv.mitigation_reason, ir.active, ir.playbook_count
                                         from system_vulnerabilities sv
                                         join cve_metadata cm on sv.cve_id = cm.id
                                         left outer join insights_rule ir on sv.rule_id = ir.id
                                        where sv.system_id = $1 and sv.rh_account_id = $2""", system_id, rh_account_id):
        system_cves_map[record['cve']] = {'cve_id': record['id'],
                                          'when_mitigated': record['when_mitigated'],
                                          'mitigation_reason': record['mitigation_reason'],
                                          'active_rule': record['active'],
                                          'playbook_count': record['playbook_count']}
    return system_cves_map


async def _register_missing_cves(missing_cves, conn):
    new_cves = set()
    for cve in sorted(missing_cves):
        # do nothing if row exists - other evaluator or vmaas-sync just inserted it
        record = await conn.fetchrow("""INSERT INTO cve_metadata (cve, description, impact_id)
                                        VALUES ($1, $2, $3)
                                   ON CONFLICT (cve) DO UPDATE set cve = $1
                                     returning id""", cve, 'unknown', 0)
        cve_id = record[0]
        new_cves.add(cve_id)
    return new_cves


async def _store_new_cves(rh_account_id, system_id, new_cves, conn, advisory_available=True):
    new_sys_vulns = []
    new_cves_ids = set()
    if not new_cves:
        return new_sys_vulns

    cves_in_db = set()

    # FIXME: getting metadata from DB on every system update should be optimized
    async for record in conn.cursor("""select id, cve
                                         from cve_metadata
                                        where cve = any($1::text[])""", new_cves):
        new_cves_ids.add(record['id'])
        cves_in_db.add(record['cve'])

    cve_metadata_missing = [cve for cve in new_cves if cve not in cves_in_db]

    # Insert new CVEs into db
    if cve_metadata_missing:
        new_cves_ids.update(await _register_missing_cves(cve_metadata_missing, conn))

    # prepare each member in row separately as array, so asyncpg can insert it in bulk and return ids
    rh_acc_ids = []
    system_ids = []
    cve_ids = []
    adv_avails = []
    remed_ids = []
    for cve_id in new_cves_ids:
        remediation_type_id = get_available_remediation_type(advisory_available, None, None, 0)
        rh_acc_ids.append(rh_account_id)
        system_ids.append(system_id)
        cve_ids.append(cve_id)
        adv_avails.append(advisory_available)
        remed_ids.append(remediation_type_id)

    for row in await conn.fetch("""insert into system_vulnerabilities (rh_account_id, system_id, cve_id, advisory_available, remediation_type_id)
                                   (select * from unnest($1::int[], $2::int[], $3::int[], $4::boolean[], $5::int[]))
                                   returning id, cve_id""", rh_acc_ids, system_ids, cve_ids, adv_avails, remed_ids):
        new_sys_vulns.append((row[0], row[1]))
    return new_sys_vulns


async def _update_mitigated_cves(rh_account_id, system_id, mitigated_cves, conn, system_cves_map):
    mit_sys_vulns = []
    if not mitigated_cves:
        return mit_sys_vulns

    # prepare each member in row separately as array, so asyncpg can insert it in bulk and return ids
    rh_acc_ids = []
    system_ids = []
    cve_ids = []
    adv_avails = []
    remed_ids = []
    for cve in mitigated_cves:
        advisory_available = False  # No advisory available when CVE is mitigated from VMaaS side
        remediation_type_id = get_available_remediation_type(advisory_available, "now()",
                                                             system_cves_map[cve]["mitigation_reason"],
                                                             system_cves_map[cve]["playbook_count"])
        rh_acc_ids.append(rh_account_id)
        system_ids.append(system_id)
        cve_ids.append(system_cves_map[cve]["cve_id"])
        adv_avails.append(advisory_available)
        remed_ids.append(remediation_type_id)

    for row in await conn.fetch("""update system_vulnerabilities as sv
                                   set when_mitigated = now(),
                                       advisory_available = r.advisory_available,
                                       remediation_type_id = r.remediation_type_id
                                   from (select * from unnest($1::int[], $2::int[], $3::int[], $4::boolean[], $5::int[]))
                                        as r(rh_account_id, system_id, cve_id, advisory_available, remediation_type_id)
                                   where sv.rh_account_id = r.rh_account_id
                                    and  sv.system_id = r.system_id
                                    and  sv.cve_id = r.cve_id
                                   returning sv.id, sv.cve_id""", rh_acc_ids, system_ids, cve_ids, adv_avails, remed_ids):
        mit_sys_vulns.append((row[0], row[1]))
    return mit_sys_vulns


async def _update_unmitigated_cves(rh_account_id, system_id, unmitigated_cves, conn, system_cves_map, advisory_available=True):
    unmit_sys_vulns = []
    if not unmitigated_cves:
        return unmit_sys_vulns

    # prepare each member in row separately as array, so asyncpg can insert it in bulk and return ids
    rh_acc_ids = []
    system_ids = []
    cve_ids = []
    adv_avails = []
    remed_ids = []
    for cve in unmitigated_cves:
        remediation_type_id = get_available_remediation_type(advisory_available, None,
                                                             system_cves_map[cve]["mitigation_reason"],
                                                             system_cves_map[cve]["playbook_count"])
        rh_acc_ids.append(rh_account_id)
        system_ids.append(system_id)
        cve_ids.append(system_cves_map[cve]["cve_id"])
        adv_avails.append(advisory_available)
        remed_ids.append(remediation_type_id)

    for row in await conn.fetch("""update system_vulnerabilities as sv
                                   set when_mitigated = null,
                                       advisory_available = r.advisory_available,
                                       remediation_type_id = r.remediation_type_id
                                   from (select * from unnest($1::int[], $2::int[], $3::int[], $4::boolean[], $5::int[]))
                                        as r(rh_account_id, system_id, cve_id, advisory_available, remediation_type_id)
                                   where sv.rh_account_id = r.rh_account_id
                                    and  sv.system_id = r.system_id
                                    and  sv.cve_id = r.cve_id
                                   returning sv.id, sv.cve_id""", rh_acc_ids, system_ids, cve_ids, adv_avails, remed_ids):
        unmit_sys_vulns.append((row[0], row[1]))
    return unmit_sys_vulns


async def _update_system(system_id, cve_count, conn):
    await conn.execute("""update system_platform
                             set last_evaluation = now(), cve_count_cache = $1
                           where id = $2""", cve_count, system_id)


async def _vmaas_request_cves(vmaas_request_json):
    """Make VMaaS request for cves"""
    playbook_cves = set()
    manually_fixable_cves = set()
    vulnerabilities_response_json = await vmaas_request(CFG.vmaas_vulnerabilities_endpoint,
                                                        vmaas_request_json)
    if vulnerabilities_response_json is not None:
        for cve in vulnerabilities_response_json['cve_list']:
            playbook_cves.add(cve)
        for cve in vulnerabilities_response_json['manually_fixable_cve_list']:
            manually_fixable_cves.add(cve)
    else:
        return None, None
    return playbook_cves, manually_fixable_cves


def send_notifications(new_sys_vulns, mit_sys_vulns, unmit_sys_vulns, rh_account_id, acc_num):
    """Sends kafka message to notificator with system_vulnerabilities"""
    msg = {
        "rh_account_id": rh_account_id,
        "account_number": acc_num,
        "new_system_vulnerabilities_ids": [{"sys_vuln_id": sys_vuln_id, "cve_id": cve_id} for sys_vuln_id, cve_id in new_sys_vulns],
        "mitigated_system_vulnerabilities_ids": [{"sys_vuln_id": sys_vuln_id, "cve_id": cve_id} for sys_vuln_id, cve_id in mit_sys_vulns],
        "unmitigated_system_vulnerabilities_ids": [{"sys_vuln_id": sys_vuln_id, "cve_id": cve_id} for sys_vuln_id, cve_id in unmit_sys_vulns]
    }
    LOGGER.debug("Sending evaluation result to notificator: %s", msg)
    EVALUATOR_RESULTS.send(msg)


# pylint: disable=too-many-branches
@VMAAS_EVAL_TIME.time()
async def evaluate_vmaas(system_platform, acc_num, conn):
    """Evaluates messages received from vmaas"""
    VMAAS_COUNT.inc()
    system_id = system_platform[0]
    inventory_id = system_platform[1]
    # JSON to POST requests to vmaas vulnerabilities endpoint
    vmaas_request_json = json.loads(system_platform[2])
    rh_account_id = system_platform[3]

    reported_playbook_cves, reported_manually_fixable_cves = await _vmaas_request_cves(vmaas_request_json)
    if reported_playbook_cves is None or reported_manually_fixable_cves is None:
        LOGGER.warning('Skipping evaluation of %s due to VMaaS errors.', inventory_id)
        VMAAS_ERRORS_SKIP.inc()
        return

    LOGGER.info("Evaluated vulnerabilities for inventory_id: %s (%s playbook, %s manual)", inventory_id,
                len(reported_playbook_cves), len(reported_manually_fixable_cves))
    system_cves_map = await _load_cves_for_inventory_id(rh_account_id, system_id, conn)
    unprocessed_cves = set(system_cves_map.keys())
    new_playbook_cves = set()
    new_manually_fixable_cves = set()
    mitigated_cves = set()
    unmitigated_playbook_cves = set()
    unmitigated_manual_cves = set()
    system_cves = []

    for cve in reported_playbook_cves:
        system_cves.append(cve)
        if cve in system_cves_map:
            unprocessed_cves.discard(cve)
            if system_cves_map[cve]['when_mitigated']:
                # it was mitigated, now its not
                unmitigated_playbook_cves.add(cve)
        else:
            new_playbook_cves.add(cve)

    if CFG.evaluator_manual_cves:
        for cve in reported_manually_fixable_cves:
            system_cves.append(cve)
            if cve in system_cves_map:
                unprocessed_cves.discard(cve)
                if system_cves_map[cve]['when_mitigated']:
                    # it was mitigated, now its not
                    unmitigated_manual_cves.add(cve)
            else:
                new_manually_fixable_cves.add(cve)

    for cve in unprocessed_cves:  # unprocessed_cves is rest of the system_vulnerabilities which has not been reported by VMaaS
        if not system_cves_map[cve]['when_mitigated']:
            mitigated_cves.add(cve)
            if system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:  # rule is active
                system_cves.append(cve)
        elif system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:
            # we have already mitigated CVE, but has rule accociated which is not mitigation -> include in system count cache
            system_cves.append(cve)

    new_sys_vulns = await _store_new_cves(rh_account_id, system_id, new_playbook_cves, conn, advisory_available=True)
    new_sys_vulns += await _store_new_cves(rh_account_id, system_id, new_manually_fixable_cves, conn, advisory_available=False)
    mit_sys_vulns = await _update_mitigated_cves(rh_account_id, system_id, mitigated_cves, conn, system_cves_map)
    unmit_sys_vulns = await _update_unmitigated_cves(rh_account_id, system_id, unmitigated_playbook_cves, conn, system_cves_map, advisory_available=True)
    unmit_sys_vulns += await _update_unmitigated_cves(rh_account_id, system_id, unmitigated_manual_cves, conn, system_cves_map, advisory_available=False)
    await _update_system(system_id, len(system_cves), conn)

    send_remediations_update(REMEDIATIONS_PRODUCER, str(inventory_id), system_cves)
    send_notifications(new_sys_vulns, mit_sys_vulns, unmit_sys_vulns, rh_account_id, acc_num)

    LOGGER.debug("Finished storing vulnerabilities for inventory_id: %s", inventory_id)


async def process_upload_or_re_evaluate(msg_dict: dict):
    """
    Process function to upload new file or re-evaluate system
    """
    async with DB_POOL.acquire() as conn:
        try:
            async with conn.transaction():
                LOGGER.info("Received message type: %s", msg_dict['type'])
                # Lock the system for processing
                system_platform = await conn.fetchrow("""SELECT id,
                                                                inventory_id,
                                                                vmaas_json,
                                                                rh_account_id
                                                           FROM system_platform
                                                          WHERE inventory_id = $1
                                                            AND when_deleted IS NULL
                                                     FOR UPDATE""", msg_dict['host']['id'])
                if system_platform is not None:
                    await evaluate_vmaas(system_platform, msg_dict["host"]["account"], conn)
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'success', loop=MAIN_LOOP)

                else:
                    INV_ID_NOT_FOUND.inc()
                    LOGGER.error("System with inventory_id not found in DB: %s", msg_dict['host']['id'])
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error',
                                                    status_msg='System with inventory_id not found in DB: %s' % msg_dict['host']['id'],
                                                    loop=MAIN_LOOP)

        # pylint: disable=broad-except
        except Exception:
            LOGGER.exception("Unable to store data: ")
            FailedCache.push(FailedCache.upload_cache, msg_dict)
            LOGGER.info("Remembered failed upload: %s", str(msg_dict))


# pylint: disable=too-many-branches
@release_semaphore(MAX_MESSAGES_SEMAPHORE)
async def process_message(message):
    """Message procession logic"""
    try:
        msg_dict = json.loads(message.value.decode('utf-8'))
        # Can't use FailedCache.process_failed_cache here because it's tied
        # to ThreadExecutor. So do it the asyncio way
        if FailedCache.upload_cache:
            cache = FailedCache.upload_cache
            LOGGER.info("Start processing %d failed uploads", len(cache))
            for msg in cache:
                LOGGER.info("Processing failed upload: %s", str(msg))
                await process_upload_or_re_evaluate(msg)
            LOGGER.info("Cleared failed cache")
            FailedCache.clear_cache(cache)
    except json.decoder.JSONDecodeError:
        MESSAGE_PARSE_ERROR.inc()
        LOGGER.exception("Unable to parse message: ")
        return
    if message.topic in CFG.evaluator_topics:
        if 'type' not in msg_dict:
            LOGGER.error("Received message is missing type field: %s", msg_dict)
            return
        if msg_dict['type'] in ['upload_new_file', 're-evaluate_system']:
            await process_upload_or_re_evaluate(msg_dict)
            if msg_dict['type'] == 'upload_new_file':
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'processing',
                                            status_msg='Scheduled for evaluation', loop=MAIN_LOOP)
        else:
            UNKNOWN_MSG.inc()
            LOGGER.error("Received unknown message type: %s", msg_dict['type'])
    else:
        UNKNOWN_TOPIC.inc()
        LOGGER.error("Received message on unsupported topic: %s", message.topic)


async def run() -> None:
    """
    Start everything and begin processing messages
    """
    await CONSUMER_QUEUE.start()
    await PAYLOAD_TRACKER_PRODUCER.start()
    await REMEDIATIONS_PRODUCER.start()

    dsn = f"postgres://{CFG.db_user}:{CFG.db_pass}@{CFG.db_host}:{CFG.db_port}/{CFG.db_name}?sslmode={CFG.db_ssl_mode}"
    # Validate the CA certificate before sending to asyncpg
    try:
        ssl.create_default_context(cafile=CFG.db_ssl_root_cert_path)
        dsn = f"{dsn}&sslrootcert={CFG.db_ssl_root_cert_path}"
    except (FileNotFoundError, ssl.SSLError):
        pass

    # pylint: disable=global-statement
    global DB_POOL
    DB_POOL = await asyncpg.create_pool(
        dsn=dsn,
        loop=MAIN_LOOP,
        min_size=CFG.db_min_pool_size,
        max_size=CFG.db_max_pool_size
    )
    try:
        async for msg in CONSUMER_QUEUE.client:
            await MAX_MESSAGES_SEMAPHORE.acquire()
            MAIN_LOOP.create_task(process_message(msg))
    finally:
        LOGGER.info("Shutting down.")
        await CONSUMER_QUEUE.stop()
        await DB_POOL.close()
        await PAYLOAD_TRACKER_PRODUCER.stop()
        await REMEDIATIONS_PRODUCER.stop()


def main():
    """Sets up and run whole application"""
    # Set up endpoint for prometheus monitoring
    LOGGER.info("Opening port [%s] for prometheus", PROMETHEUS_PORT)
    init_logging()

    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, MAIN_LOOP)
    MAIN_LOOP.run_until_complete(status_site.start())

    MAIN_LOOP.run_until_complete(a_ensure_minimal_schema_version())

    LOGGER.info("Using BOOTSTRAP_SERVERS: %s", CFG.bootstrap_servers)
    LOGGER.info("Using GROUP_ID: %s", CFG.group_id)
    LOGGER.info("Using TOPICS: %s", ", ".join(CFG.evaluator_topics))
    MAIN_LOOP.run_until_complete(run())


if __name__ == "__main__":
    main()
