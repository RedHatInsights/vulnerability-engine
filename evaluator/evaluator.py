#!/usr/bin/env python3
"""
vulnerability-engine evaluator
"""
import json
import asyncio
import asyncpg
from prometheus_client import Counter, Histogram

from common import mqueue
from common.config import Config
from common.logging import init_logging, get_logger
from common.failed_cache import FailedCache
from common.utils import send_msg_to_payload_tracker, send_remediations_update, a_ensure_minimal_schema_version
from common.vmaas_client import vmaas_post_request
from common.status_app import create_status_app, create_status_runner

LOGGER = get_logger(__name__)
CFG = Config()

PROMETHEUS_PORT = CFG.prometheus_port or str(CFG.evaluator_prometheus_port)

# database pool instance
DB_POOL = None

# prometheus probes
# times
VMAAS_EVAL_TIME = Histogram('ve_evaluator_vmaas_evaluation_seconds', 'Time spent checking a system for vmaas hits')
# counts
VMAAS_COUNT = Counter('ve_evaluator_vmaas_calls', 'Number of VMaaS-evaluations attempted')
INV_ID_NOT_FOUND = Counter('ve_evaluator_inventory_not_found', 'Number of times inventory-id not in SystemPlatform')
UNKNOWN_MSG = Counter('ve_evaluator_unknown_msg', 'Number of unrecognized messages delivered from queue')
UNKNOWN_TOPIC = Counter('ve_evaluator_unknown_topic', 'Number of times message delivered from unsupported topic')
MESSAGE_PARSE_ERROR = Counter('ve_evaluator_message_parse_error', '# of message parse errors')
VMAAS_ERRORS_SKIP = Counter('ve_evaluator_vmaas_errors_skip', '# of evaluations skipped due to VMaaS errors')

CONSUMER_QUEUE = mqueue.MQReader(CFG.evaluator_topics)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(CFG.payload_tracker_topic)
REMEDIATIONS_PRODUCER = mqueue.MQWriter(CFG.remediation_updates_topic)

MAIN_LOOP = asyncio.get_event_loop()


async def _load_cves_for_inventory_id(rh_account_id, system_id, conn):
    system_cves_map = {}
    async for record in conn.cursor("""select cm.id, cm.cve, sv.when_mitigated, sv.mitigation_reason, ir.active
                                         from system_vulnerabilities sv
                                         join cve_metadata cm on sv.cve_id = cm.id
                                         left outer join insights_rule ir on sv.rule_id = ir.id
                                        where sv.system_id = $1 and sv.rh_account_id = $2""", system_id, rh_account_id):
        system_cves_map[record['cve']] = {'cve_id': record['id'],
                                          'when_mitigated': record['when_mitigated'],
                                          'mitigation_reason': record['mitigation_reason'],
                                          'active_rule': record['active']}
    return system_cves_map


async def _register_missing_cves(missing_cves, conn):
    new_cves = set()
    for cve in missing_cves:
        # do nothing if row exists - other evaluator or vmaas-sync just inserted it
        record = await conn.fetchrow("""INSERT INTO cve_metadata (cve, description, impact_id)
                                        VALUES ($1, $2, $3)
                                   ON CONFLICT (cve) DO UPDATE set cve = $1
                                     returning id""", cve, 'unknown', 0)
        cve_id = record[0]
        new_cves.add(cve_id)
    return new_cves


async def _store_new_cves(rh_account_id, system_id, new_cves, conn):
    new_cves_ids = set()
    if not new_cves:
        return

    cves_in_db = set()

    # FIXME: getting metadata from DB on every system update should be optimized
    async for record in conn.cursor("""select id, cve
                                         from cve_metadata
                                        where cve = any($1::text[])""", new_cves):
        new_cves_ids.add(record['id'])
        cves_in_db.add(record['cve'])

    cve_metadata_missing = [cve for cve in new_cves if cve not in cves_in_db]

    # Insert new CVEs into db
    if cve_metadata_missing:
        new_cves_ids.update(await _register_missing_cves(cve_metadata_missing, conn))

    cve_system_list = [(rh_account_id, system_id, cve_id) for cve_id in new_cves_ids]
    await conn.executemany("""insert into system_vulnerabilities (rh_account_id, system_id, cve_id)
                              values ($1, $2, $3)""", cve_system_list)


async def _update_mitigated_cves(rh_account_id, system_id, mitigated_cves_ids, conn):
    if not mitigated_cves_ids:
        return

    await conn.execute("""update system_vulnerabilities
                             set when_mitigated = now()
                           where system_id = $1 and cve_id = any($2::int[]) and rh_account_id = $3""",
                       system_id, mitigated_cves_ids, rh_account_id)


async def _update_unmitigated_cves(rh_account_id, system_id, unmitigated_cves_ids, conn):
    if not unmitigated_cves_ids:
        return

    await conn.execute("""update system_vulnerabilities
                             set when_mitigated = null
                           where system_id = $1 and cve_id = any($2::int[]) and rh_account_id = $3""",
                       system_id, unmitigated_cves_ids, rh_account_id)


async def _update_system(system_id, conn):
    await conn.execute("""update system_platform
                             set last_evaluation = now()
                           where id = $1""", system_id)


async def _vmaas_request_cves(vmaas_request_json):
    """Make VMaaS request for cves"""
    system_cves = set()
    vmaas_request_json["oval"] = CFG.vmaas_oval_enable
    vmaas_request_json["oval_only"] = False  # Join results from repodata evaluation and OVAL evaluation
    vulnerabilities_response_json = await vmaas_post_request(CFG.vmaas_vulnerabilities_endpoint,
                                                             vmaas_request_json)
    if vulnerabilities_response_json is not None:
        for cve in vulnerabilities_response_json['cve_list']:
            system_cves.add(cve)
    else:
        return None
    return system_cves


@VMAAS_EVAL_TIME.time()
async def evaluate_vmaas(system_platform, conn):
    """Evaluates messages received from vmaas"""
    VMAAS_COUNT.inc()
    system_id = system_platform[0]
    inventory_id = system_platform[1]
    LOGGER.info("Evaluating vulnerabilities for inventory_id: %s", inventory_id)
    # JSON to POST requests to vmaas vulnerabilities endpoint
    vmaas_request_json = json.loads(system_platform[2])
    rh_account_id = system_platform[3]

    reported_cves = await _vmaas_request_cves(vmaas_request_json)
    if reported_cves is None:
        LOGGER.warning('Skipping evaluation of %s due to VMaaS errors.', inventory_id)
        VMAAS_ERRORS_SKIP.inc()
        return

    system_cves_map = await _load_cves_for_inventory_id(rh_account_id, system_id, conn)
    unprocessed_cves = set(system_cves_map.keys())
    new_cves = set()
    mitigated_cves_ids = set()
    unmitigated_cves_ids = set()
    system_cves = []

    for cve in reported_cves:
        system_cves.append(cve)
        if cve in system_cves_map:
            unprocessed_cves.discard(cve)
            if system_cves_map[cve]['when_mitigated']:
                # it was mitigated, now its not
                unmitigated_cves_ids.add(system_cves_map[cve]['cve_id'])
        else:
            new_cves.add(cve)

    for cve in unprocessed_cves:  # unprocessed_cves is rest of the system_vulnerabilities which has not been reported by VMaaS
        if not system_cves_map[cve]['when_mitigated']:
            mitigated_cves_ids.add(system_cves_map[cve]['cve_id'])
            if system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:  # rule is active
                system_cves.append(cve)
        elif system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:
            # we have already mitigated CVE, but has rule accociated which is not mitigation -> include in system count cache
            system_cves.append(cve)

    await _store_new_cves(rh_account_id, system_id, new_cves, conn)
    await _update_mitigated_cves(rh_account_id, system_id, mitigated_cves_ids, conn)
    await _update_unmitigated_cves(rh_account_id, system_id, unmitigated_cves_ids, conn)
    await _update_system(system_id, conn)

    send_remediations_update(REMEDIATIONS_PRODUCER, str(inventory_id), system_cves)

    LOGGER.debug("Finished evaluating vulnerabilities for inventory_id: %s", inventory_id)


async def process_upload_or_re_evaluate(msg_dict: dict):
    """
    Process function to upload new file or re-evaluate system
    """
    async with DB_POOL.acquire() as conn:
        try:
            async with conn.transaction():
                LOGGER.info("Received message type: %s", msg_dict['type'])
                # Lock the system for processing
                system_platform = await conn.fetchrow("""SELECT id,
                                                                inventory_id,
                                                                vmaas_json,
                                                                rh_account_id
                                                           FROM system_platform
                                                          WHERE inventory_id = $1
                                                            AND when_deleted IS NULL
                                                     FOR UPDATE""", msg_dict['host']['id'])
                if system_platform is not None:
                    await evaluate_vmaas(system_platform, conn)
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'success', loop=MAIN_LOOP)

                else:
                    INV_ID_NOT_FOUND.inc()
                    LOGGER.error("System with inventory_id not found in DB: %s", msg_dict['host']['id'])
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error',
                                                    status_msg='System with inventory_id not found in DB: %s' % msg_dict['host']['id'],
                                                    loop=MAIN_LOOP)

        # pylint: disable=broad-except
        except Exception:
            LOGGER.exception("Unable to store data: ")
            FailedCache.push(FailedCache.upload_cache, msg_dict)
            LOGGER.info("Remembered failed upload: %s", str(msg_dict))


# pylint: disable=too-many-branches
async def process_message(message):
    """Message procession logic"""
    try:
        msg_dict = json.loads(message.value.decode('utf-8'))
        # Can't use FailedCache.process_failed_cache here because it's tied
        # to ThreadExecutor. So do it the asyncio way
        if FailedCache.upload_cache:
            cache = FailedCache.upload_cache
            LOGGER.info("Start processing %d failed uploads", len(cache))
            for msg in cache:
                LOGGER.info("Processing failed upload: %s", str(msg))
                await process_upload_or_re_evaluate(msg)
            LOGGER.info("Cleared failed cache")
            FailedCache.clear_cache(cache)
    except json.decoder.JSONDecodeError:
        MESSAGE_PARSE_ERROR.inc()
        LOGGER.exception("Unable to parse message: ")
        return
    if message.topic in CFG.evaluator_topics:
        if 'type' not in msg_dict:
            LOGGER.error("Received message is missing type field: %s", msg_dict)
            return
        if msg_dict['type'] in ['upload_new_file', 're-evaluate_system']:
            await process_upload_or_re_evaluate(msg_dict)
            if msg_dict['type'] == 'upload_new_file':
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'processing',
                                            status_msg='Scheduled for evaluation', loop=MAIN_LOOP)
        else:
            UNKNOWN_MSG.inc()
            LOGGER.error("Received unknown message type: %s", msg_dict['type'])
    else:
        UNKNOWN_TOPIC.inc()
        LOGGER.error("Received message on unsupported topic: %s", message.topic)


async def run() -> None:
    """
    Start everything and begin processing messages
    """
    await CONSUMER_QUEUE.start()
    await PAYLOAD_TRACKER_PRODUCER.start()
    await REMEDIATIONS_PRODUCER.start()

    # pylint: disable=global-statement
    global DB_POOL
    DB_POOL = await asyncpg.create_pool(
        host=CFG.db_host,
        port=CFG.db_port,
        user=CFG.db_user,
        password=CFG.db_pass,
        database=CFG.db_name,
        loop=MAIN_LOOP,
        min_size=CFG.db_min_pool_size,
        max_size=CFG.db_max_pool_size,
    )
    try:
        async for msg in CONSUMER_QUEUE.client:
            MAIN_LOOP.create_task(process_message(msg))
    finally:
        LOGGER.info("Shutting down.")
        await CONSUMER_QUEUE.stop()
        await DB_POOL.close()
        await PAYLOAD_TRACKER_PRODUCER.stop()
        await REMEDIATIONS_PRODUCER.stop()


def main():
    """Sets up and run whole application"""
    # Set up endpoint for prometheus monitoring
    LOGGER.info("Opening port [%s] for prometheus", PROMETHEUS_PORT)
    init_logging()

    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, MAIN_LOOP)
    MAIN_LOOP.run_until_complete(status_site.start())

    MAIN_LOOP.run_until_complete(a_ensure_minimal_schema_version())

    LOGGER.info("Using BOOTSTRAP_SERVERS: %s", CFG.bootstrap_servers)
    LOGGER.info("Using GROUP_ID: %s", CFG.group_id)
    LOGGER.info("Using TOPICS: %s", ", ".join(CFG.evaluator_topics))
    MAIN_LOOP.run_until_complete(run())


if __name__ == "__main__":
    main()
