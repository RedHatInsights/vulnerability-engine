#!/usr/bin/env python3
"""
vulnerability-engine evaluator
"""
import os
import json
import asyncio
import asyncpg
from prometheus_client import Counter, Histogram, start_http_server

from common import mqueue
from common.database_handler import DB_NAME, DB_USER, DB_PASS, DB_HOST, DB_PORT
from common.logging import init_logging, get_logger
from common.failed_cache import FailedCache
from common.utils import send_msg_to_payload_tracker, send_remediations_update, ensure_minimal_schema_version
from common.vmaas_client import vmaas_post_request

LOGGER = get_logger(__name__)

VMAAS_HOST = os.getenv('VMAAS_HOST', 'http://vmaas-webapp-1.vmaas-ci.svc:8080')
VMAAS_VULNERABILITIES_API = os.getenv("VMAAS_VULNERABILITIES_API", "/api/v1/vulnerabilities")
vmaas_vulnerabilities_endpoint = "%s%s" % (VMAAS_HOST, VMAAS_VULNERABILITIES_API)  # pylint: disable=invalid-name

kafka_evaluator_topic = os.getenv('EVALUATOR_TOPIC',  # pylint: disable=invalid-name
                                  'vulnerability.evaluator.upload,vulnerability.evaluator.recalc').split(",")
prometheus_port = os.getenv('PROMETHEUS_PORT', '8085')  # pylint: disable=invalid-name

# database pool sizes and instance
DB_MIN_POOL_SIZE = int(os.getenv('DB_MIN_POOL_SIZE', '10'))
DB_MAX_POOL_SIZE = int(os.getenv('DB_MAX_POOL_SIZE', '30'))
DB_POOL = None

# prometheus probes
# times
VMAAS_EVAL_TIME = Histogram('ve_evaluator_vmaas_evaluation_seconds', 'Time spent checking a system for vmaas hits')
# counts
VMAAS_COUNT = Counter('ve_evaluator_vmaas_calls', 'Number of VMaaS-evaluations attempted')
INV_ID_NOT_FOUND = Counter('ve_evaluator_inventory_not_found', 'Number of times inventory-id not in SystemPlatform')
UNKNOWN_MSG = Counter('ve_evaluator_unknown_msg', 'Number of unrecognized messages delivered from queue')
UNKNOWN_TOPIC = Counter('ve_evaluator_unknown_topic', 'Number of times message delivered from unsupported topic')
MESSAGE_PARSE_ERROR = Counter('ve_evaluator_message_parse_error', '# of message parse errors')
VMAAS_ERRORS_SKIP = Counter('ve_evaluator_vmaas_errors_skip', '# of evaluations skipped due to VMaaS errors')

CONSUMER_QUEUE = mqueue.MQReader(kafka_evaluator_topic)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(mqueue.PAYLOAD_TRACKER_TOPIC)
REMEDIATIONS_PRODUCER = mqueue.MQWriter(mqueue.REMEDIATION_UPDATES_TOPIC)

MAIN_LOOP = asyncio.get_event_loop()


async def _load_cves_for_inventory_id(system_id, conn):
    system_cves_map = {}
    async for record in conn.cursor("""select cm.id, cm.cve, sv.when_mitigated, sv.mitigation_reason, ir.active
                                         from system_vulnerabilities sv
                                         join cve_metadata cm on sv.cve_id = cm.id
                                         left outer join insights_rule ir on sv.rule_id = ir.id
                                        where sv.system_id = $1""", system_id):
        system_cves_map[record['cve']] = {'cve_id': record['id'],
                                          'when_mitigated': record['when_mitigated'],
                                          'mitigation_reason': record['mitigation_reason'],
                                          'active_rule': record['active']}
    return system_cves_map


async def _register_missing_cves(missing_cves, conn):
    new_cves = set()
    for cve in missing_cves:
        # do nothing if row exists - other evaluator or vmaas-sync just inserted it
        record = await conn.fetchrow("""INSERT INTO cve_metadata (cve, description, impact_id)
                                        VALUES ($1, $2, $3)
                                   ON CONFLICT (cve) DO UPDATE set cve = $1
                                     returning id""", cve, 'unknown', 0)
        cve_id = record[0]
        new_cves.add(cve_id)
    return new_cves


async def _store_new_cves(system_id, new_cves, conn):
    new_cves_ids = set()
    if not new_cves:
        return new_cves_ids

    cves_in_db = set()

    # FIXME: getting metadata from DB on every system update should be optimized
    async for record in conn.cursor("""select id, cve
                                         from cve_metadata
                                        where cve = any($1::text[])""", new_cves):
        new_cves_ids.add(record['id'])
        cves_in_db.add(record['cve'])

    cve_metadata_missing = [cve for cve in new_cves if cve not in cves_in_db]

    # Insert new CVEs into db
    if cve_metadata_missing:
        new_cves_ids.update(await _register_missing_cves(cve_metadata_missing, conn))

    cve_system_list = [(system_id, cve_id) for cve_id in new_cves_ids]
    await conn.executemany("""insert into system_vulnerabilities (system_id, cve_id)
                              values ($1, $2)""", cve_system_list)
    return new_cves_ids


async def _update_mitigated_cves(system_id, mitigated_cves_ids, cve_status_map, conn):
    if not mitigated_cves_ids:
        return

    async for record in conn.cursor("""update system_vulnerabilities
                                          set when_mitigated = now()
                                        where system_id = $1 and cve_id = any($2::int[])
                                    returning cve_id, status_id""", system_id, mitigated_cves_ids):
        cve_status_map[record['cve_id']] = record['status_id']


async def _update_unmitigated_cves(system_id, unmitigated_cves_ids, cve_status_map, conn):
    if not unmitigated_cves_ids:
        return

    async for record in conn.cursor("""update system_vulnerabilities
                                          set when_mitigated = null
                                        where system_id = $1 and cve_id = any($2::int[])
                                    returning cve_id, status_id""", system_id, unmitigated_cves_ids):
        cve_status_map[record['cve_id']] = record['status_id']


async def _update_system(system_id, cve_count, conn):
    await conn.execute("""update system_platform
                             set last_evaluation = now(), cve_count_cache = $1
                           where id = $2""", cve_count, system_id)


async def _update_cve_affected_systems_cache(new_cves_ids, unmitigated_cves_ids, mitigated_cves_ids,
                                             rh_account_id, cve_status_map, conn):
    # pylint: disable=too-many-branches
    if not new_cves_ids and not unmitigated_cves_ids and not mitigated_cves_ids:
        return
    all_cves_ids = new_cves_ids.copy()
    all_cves_ids.update(unmitigated_cves_ids)
    all_cves_ids.update(mitigated_cves_ids)
    inc_cves = []
    inc_cves_divergent_status = []
    dec_cves = []
    dec_cves_divergent_status = []
    delete_cves = []
    cve_global_status_map = {}
    async for record in conn.cursor("""SELECT cve_id, systems_affected, status_id
                                         FROM cve_account_data
                                        WHERE rh_account_id = $1
                                          AND cve_id = any($2::int[])
                                     ORDER BY cve_id
                                   FOR UPDATE""", rh_account_id, all_cves_ids):
        cve_id = record['cve_id']
        systems_affected = record['systems_affected']
        status_id = record['status_id']
        if cve_id in new_cves_ids or cve_id in unmitigated_cves_ids:
            if status_id == 0:
                inc_cves.append(cve_id)
            else:
                inc_cves_divergent_status.append(cve_id)
        elif cve_id in mitigated_cves_ids and systems_affected > 1:
            if status_id == cve_status_map.get(cve_id, 0):
                dec_cves.append(cve_id)
            else:
                dec_cves_divergent_status.append(cve_id)
        elif cve_id in mitigated_cves_ids:
            delete_cves.append(cve_id)
        cve_global_status_map[cve_id] = status_id
        # cves not in DB cache
        all_cves_ids.remove(cve_id)
    # filter out rest of cves and insert cache for new and unmitigated
    insert_cves = [(cve_id, rh_account_id, 1,
                    0 if cve_global_status_map.get(cve_id, 0) == cve_status_map.get(cve_id, 0) else 1)
                   for cve_id in all_cves_ids if cve_id in new_cves_ids or cve_id in unmitigated_cves_ids]
    if inc_cves:
        await conn.execute("""update cve_account_data set systems_affected = systems_affected + 1
                              where rh_account_id = $1 and cve_id = any($2::int[])""",
                           rh_account_id, inc_cves)
    if inc_cves_divergent_status:
        await conn.execute("""update cve_account_data set systems_affected = systems_affected + 1,
                              systems_status_divergent = systems_status_divergent + 1
                              where rh_account_id = $1 and cve_id = any($2::int[])""",
                           rh_account_id, inc_cves_divergent_status)
    if dec_cves:
        await conn.execute("""update cve_account_data set systems_affected = systems_affected - 1
                              where rh_account_id = $1 and cve_id = any($2::int[])""",
                           rh_account_id, dec_cves)
    if dec_cves_divergent_status:
        await conn.execute("""update cve_account_data set systems_affected = systems_affected - 1,
                              systems_status_divergent = systems_status_divergent - 1
                              where rh_account_id = $1 and cve_id = any($2::int[])""",
                           rh_account_id, dec_cves_divergent_status)
    if delete_cves:
        await conn.execute("""delete from cve_account_data
                              where rh_account_id = $1 and cve_id = any($2::int[])""",
                           rh_account_id, delete_cves)
    if insert_cves:
        await conn.executemany("""insert into cve_account_data
                                  (cve_id, rh_account_id, systems_affected, systems_status_divergent) values ($1, $2, $3, $4)
                                  on conflict (cve_id, rh_account_id) do update set
                                    systems_affected = cve_account_data.systems_affected
                                                     + excluded.systems_affected,
                                    systems_status_divergent = cve_account_data.systems_status_divergent
                                                             + excluded.systems_status_divergent""",
                               insert_cves)


async def _vmaas_request_cves(vmaas_request_json):
    """Make VMaaS request for cves"""
    system_cves = set()
    vulnerabilities_response_json = await vmaas_post_request(vmaas_vulnerabilities_endpoint,
                                                             vmaas_request_json)
    if vulnerabilities_response_json is not None:
        for cve in vulnerabilities_response_json['cve_list']:
            system_cves.add(cve)
    else:
        return None
    return system_cves


@VMAAS_EVAL_TIME.time()
async def evaluate_vmaas(system_platform, conn):
    """Evaluates messages received from vmaas"""
    VMAAS_COUNT.inc()
    system_id = system_platform[0]
    inventory_id = system_platform[1]
    LOGGER.info("Evaluating vulnerabilities for inventory_id: %s", inventory_id)
    # JSON to POST requests to vmaas vulnerabilities endpoint
    vmaas_request_json = json.loads(system_platform[2])
    rh_account_id = system_platform[3]
    opt_out = system_platform[4]
    stale = system_platform[5]

    reported_cves = await _vmaas_request_cves(vmaas_request_json)
    if reported_cves is None:
        LOGGER.warning('Skipping evaluation of %s due to VMaaS errors.', inventory_id)
        VMAAS_ERRORS_SKIP.inc()
        return

    system_cves_map = await _load_cves_for_inventory_id(system_id, conn)
    unprocessed_cves = set(system_cves_map.keys())
    new_cves = set()
    mitigated_cves_ids = set()
    unmitigated_cves_ids = set()
    inc_cves_ids = set()
    dec_cves_ids = set()
    system_cves = []

    for cve in reported_cves:
        system_cves.append(cve)
        if cve in system_cves_map:
            unprocessed_cves.discard(cve)
            if system_cves_map[cve]['when_mitigated']:
                # it was mitigated, now its not
                unmitigated_cves_ids.add(system_cves_map[cve]['cve_id'])
                if not system_cves_map[cve]['active_rule']:  # unmitigated only if there wasn't rule hit before, or there's a mitigation in place
                    inc_cves_ids.add(system_cves_map[cve]['cve_id'])  # in either case we do not have to increase counts
        else:
            new_cves.add(cve)

    rule_cves = 0
    for cve in unprocessed_cves:  # unprocessed_cves is rest of the system_vulnerabilities which has not been reported by VMaaS
        if not system_cves_map[cve]['when_mitigated']:
            mitigated_cves_ids.add(system_cves_map[cve]['cve_id'])
            if not system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:
                # mitigated only in case there was no rule hit before and no mitigation
                dec_cves_ids.add(system_cves_map[cve]['cve_id'])  # at this point the CVE was mitigated by VMaaS and does not have active rule
            elif system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:  # rule is active
                system_cves.append(cve)
                rule_cves += 1
        elif system_cves_map[cve]['active_rule'] and not system_cves_map[cve]['mitigation_reason']:
            # we have already mitigated CVE, but has rule accociated which is not mitigation -> include in system count cache
            system_cves.append(cve)
            rule_cves += 1

    new_cves_ids = await _store_new_cves(system_id, new_cves, conn)
    cve_status_map = {}
    await _update_mitigated_cves(system_id, mitigated_cves_ids, cve_status_map, conn)
    await _update_unmitigated_cves(system_id, unmitigated_cves_ids, cve_status_map, conn)
    await _update_system(system_id, len(reported_cves) + rule_cves, conn)
    # don't update cve cache for opted out or stale system
    if not opt_out and not stale:
        # pylint: disable=too-many-function-args
        await _update_cve_affected_systems_cache(new_cves_ids, inc_cves_ids, dec_cves_ids, rh_account_id, cve_status_map, conn)

    send_remediations_update(REMEDIATIONS_PRODUCER, inventory_id, system_cves)

    LOGGER.debug("Finished evaluating vulnerabilities for inventory_id: %s", inventory_id)


async def process_upload_or_re_evaluate(msg_dict: dict):
    """
    Process function to upload new file or re-evaluate system
    """
    async with DB_POOL.acquire() as conn:
        try:
            async with conn.transaction():
                LOGGER.info("Received message type: %s", msg_dict['type'])
                # Lock the system for processing
                system_platform = await conn.fetchrow("""SELECT id,
                                                                inventory_id,
                                                                vmaas_json,
                                                                rh_account_id,
                                                                opt_out,
                                                                stale
                                                           FROM system_platform
                                                          WHERE inventory_id = $1
                                                            AND when_deleted IS NULL
                                                     FOR UPDATE""", msg_dict['host']['id'])
                if system_platform is not None:
                    await evaluate_vmaas(system_platform, conn)
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'success', loop=MAIN_LOOP)

                else:
                    INV_ID_NOT_FOUND.inc()
                    LOGGER.error("System with inventory_id not found in DB: %s", msg_dict['host']['id'])
                    if msg_dict['type'] == 'upload_new_file':
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error',
                                                    status_msg='System with inventory_id not found in DB: %s' % msg_dict['host']['id'],
                                                    loop=MAIN_LOOP)

        # pylint: disable=broad-except
        except Exception:
            LOGGER.exception("Unable to store data: ")
            FailedCache.push(FailedCache.upload_cache, msg_dict)
            LOGGER.info("Remembered failed upload: %s", str(msg_dict))


# pylint: disable=too-many-branches
async def process_message(message):
    """Message procession logic"""
    try:
        msg_dict = json.loads(message.value.decode('utf-8'))
        # Can't use FailedCache.process_failed_cache here because it's tied
        # to ThreadExecutor. So do it the asyncio way
        if FailedCache.upload_cache:
            cache = FailedCache.upload_cache
            LOGGER.info("Start processing %d failed uploads", len(cache))
            for msg in cache:
                LOGGER.info("Processing failed upload: %s", str(msg))
                await process_upload_or_re_evaluate(msg)
            LOGGER.info("Cleared failed cache")
            FailedCache.clear_cache(cache)
    except json.decoder.JSONDecodeError:
        MESSAGE_PARSE_ERROR.inc()
        LOGGER.exception("Unable to parse message: ")
        return
    if message.topic in kafka_evaluator_topic:
        if 'type' not in msg_dict:
            LOGGER.error("Received message is missing type field: %s", msg_dict)
            return
        if msg_dict['type'] in ['upload_new_file', 're-evaluate_system']:
            await process_upload_or_re_evaluate(msg_dict)
            if msg_dict['type'] == 'upload_new_file':
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'processing',
                                            status_msg='Scheduled for evaluation', loop=MAIN_LOOP)
        else:
            UNKNOWN_MSG.inc()
            LOGGER.error("Received unknown message type: %s", msg_dict['type'])
    else:
        UNKNOWN_TOPIC.inc()
        LOGGER.error("Received message on unsupported topic: %s", message.topic)


async def run() -> None:
    """
    Start everything and begin processing messages
    """
    await CONSUMER_QUEUE.start()
    await PAYLOAD_TRACKER_PRODUCER.start()
    await REMEDIATIONS_PRODUCER.start()

    # pylint: disable=global-statement
    global DB_POOL
    DB_POOL = await asyncpg.create_pool(host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASS,
                                        database=DB_NAME, loop=MAIN_LOOP,
                                        min_size=DB_MIN_POOL_SIZE, max_size=DB_MAX_POOL_SIZE)
    try:
        async for msg in CONSUMER_QUEUE.client:
            MAIN_LOOP.create_task(process_message(msg))
    finally:
        LOGGER.info("Shutting down.")
        await CONSUMER_QUEUE.stop()
        await DB_POOL.close()
        await PAYLOAD_TRACKER_PRODUCER.stop()
        await REMEDIATIONS_PRODUCER.stop()


def main():
    """Sets up and run whole application"""
    # Set up endpoint for prometheus monitoring
    init_logging()
    ensure_minimal_schema_version()
    LOGGER.info("Using BOOTSTRAP_SERVERS: %s", mqueue.BOOTSTRAP_SERVERS)
    LOGGER.info("Using GROUP_ID: %s", mqueue.GROUP_ID)
    LOGGER.info("Using TOPICS: %s", ", ".join(kafka_evaluator_topic))
    LOGGER.info("Opening port [%s] for prometheus", prometheus_port)
    start_http_server(int(prometheus_port))
    MAIN_LOOP.run_until_complete(run())


if __name__ == "__main__":
    main()
