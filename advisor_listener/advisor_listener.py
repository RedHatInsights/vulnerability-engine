"""Rules engine results listener"""
import asyncio
import hashlib
import json
import signal
from typing import Optional

import flags
from prometheus_client import Counter
from psycopg2.extras import execute_values

from common import mqueue
from common.config import Config
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabaseError, DatabasePool, DatabasePoolConnection
from common.identity import get_identity, is_entitled_insights
from common.logging import init_logging, get_logger
from common.utils import send_remediations_update, send_msg_to_payload_tracker, a_ensure_minimal_schema_version, validate_kafka_msg
from common.status_app import create_status_app, create_status_runner

LOGGER = get_logger(__name__)

CFG = Config()

PROMETHEUS_PORT = CFG.prometheus_port or str(CFG.adv_listener_prometheus_port)

PROCESS_MESSAGES = Counter('ve_advisor_listener_messages_processed', '# of messages processed')
NEW_SYSTEM = Counter('ve_advisor_listener_upl_new_system', '# of new systems inserted by advisor')
UPDATE_SYSTEM = Counter('ve_advisor_listener_upl_update_system', '# of systems updated')
MESSAGE_PARSE_ERROR = Counter('ve_advisor_listener_message_parse_error', '# of message parse errors')
INVALID_IDENTITY = Counter('ve_advisor_listener_invalid_identity', '# of skipped uploads because of invalid identity')
MISSING_INSIGHTS_ENTITLEMENT = Counter('ve_advisor_listener_non_insights_entitlement', '# of skipped uploads because of entitlement check')
DATABASE_ERROR = Counter('ve_advisor_listener_database_error', '# of database errors')
DELETED_UPLOADED = Counter('ve_advisor_listener_deleted_uploaded', '# of systems uploaded after being deleted')
NEW_RH_ACCOUNT = Counter('ve_advisor_listener_upl_new_rh_account', '# of new rh accounts inserted')
INVALID_INSIGHTS_ACC = Counter('ve_advisor_listener_invalid_insights_acc', '# of non-insights messages')
UPLOAD_NO_RESULTS = Counter('ve_advisor_listener_upl_no_result', '# of systems ignored due to missing reports and passes')
UNCHANGED_SYSTEM = Counter('ve_advisor_listener_upl_unchanged_system', '# of system-updates with same advisor results info')


ADVISOR_QUEUE = mqueue.MQReader([CFG.advisor_results_topic])
REMEDIATIONS_PRODUCER = mqueue.MQWriter(CFG.remediation_updates_topic)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(CFG.payload_tracker_topic)

RULE_BLACKLIST = ['CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_ONLYKERNEL', 'CVE_2017_5715_cpu_virt']

RULES_CACHE = {}
CVES_CACHE = {}

REQUIRED_MESSAGE_FIELDS = {
    "input": [{"host": [("insights_id", False)]}]
}

PAYLOAD_TRACKER_SERVICE = "vulnerability-rules"


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info('Signal received, stopping kafka consumers.')
    await ADVISOR_QUEUE.stop()
    await REMEDIATIONS_PRODUCER.stop()
    await PAYLOAD_TRACKER_PRODUCER.stop()
    loop.stop()


def db_import_cve(cve: str):
    """Import missing CVE metadata into database"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""INSERT INTO cve_metadata (cve, description, impact_id) VALUES %s ON CONFLICT (cve) DO UPDATE SET cve = %s
                RETURNING id AS inserted""", ((cve, 'unknown', 0,), cve))
                conn.commit()
                inserted = cur.fetchone()
                CVES_CACHE[cve] = inserted[0]
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error during inserting CVE: ")


def db_import_rule(rule_id: str, rule_cves: list, rule_only=False):
    """Import single error key into database"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""INSERT INTO insights_rule (name, rule_only) VALUES(%s, %s) ON CONFLICT (name) DO UPDATE SET name = EXCLUDED.name,
                rule_only = EXCLUDED.rule_only RETURNING id AS inserted""", (rule_id, rule_only))
                inserted = cur.fetchone()
                RULES_CACHE[rule_id] = inserted[0]
                to_insert = []
                for rule_cve in rule_cves:
                    if rule_cve not in CVES_CACHE:
                        db_import_cve(rule_cve)
                    to_insert.append((inserted, CVES_CACHE[rule_cve]))
                execute_values(cur, """INSERT INTO cve_rule_mapping (rule_id, cve_id) VALUES %s ON CONFLICT DO NOTHING""",
                               to_insert, page_size=len(to_insert))
                conn.commit()
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error during inserting rule: ")
                conn.rollback()


def _update_system(system_id, cur):
    cur.execute("""update system_platform
                             set advisor_evaluated = now()
                           where id = %s""", (system_id,))


def db_import_rule_hits(cur, rh_account_id: int, inventory_id: str, system_id: int, rule_hits: dict, loop=None) -> None:
    """Associate rules hits with system"""
    # pylint: disable=too-many-branches, too-many-statements
    cur.execute("""SELECT ir.id FROM insights_rule ir
                   WHERE ir.active = 'T'""")
    active_rules = {row[0] for row in cur.fetchall()}

    to_update = []
    rule_hits_cves_ids = tuple(rule_hits.keys())
    system_cves = []

    cur.execute("""SELECT sv.id, sv.cve_id, sv.rule_id, sv.when_mitigated, cm.cve FROM system_vulnerabilities sv
                JOIN cve_metadata cm ON sv.cve_id = cm.id
                WHERE system_id = %s AND rh_account_id = %s""", (system_id, rh_account_id,))

    for row_id, cve_id, rule_id, when_mitigated, cve in cur.fetchall():
        if cve_id in rule_hits_cves_ids:
            if 'mitigation_reason' in rule_hits[cve_id]:
                if rule_hits[cve_id]['id'] not in active_rules and not when_mitigated:
                    system_cves.append(cve)
                to_update.append((row_id, rh_account_id, rule_hits[cve_id]['id'], None, rule_hits[cve_id]['mitigation_reason']))
            else:
                to_update.append((row_id, rh_account_id, rule_hits[cve_id]['id'], rule_hits[cve_id]['details'], None))
                if rule_hits[cve_id]['id'] in active_rules or not when_mitigated:
                    system_cves.append(cve)
            del rule_hits[cve_id]  # rule hits become dict of completely new system cves
        elif rule_id:
            to_update.append((row_id, rh_account_id, None, None, None))
            if not when_mitigated:
                system_cves.append(cve)
        elif not when_mitigated:
            system_cves.append(cve)

    if rule_hits:
        insert_new_cves(cur, rh_account_id, system_id, rule_hits)
        system_cves.extend([rule_hits[cve]['cve_name'] for cve in rule_hits if rule_hits[cve]['id'] in active_rules and 'details' in rule_hits[cve]])
    if to_update:
        update_cves(cur, to_update, rh_account_id)

    _update_system(system_id, cur)

    send_remediations_update(REMEDIATIONS_PRODUCER, inventory_id, system_cves, loop)


def db_import_system(msg_dict: dict, rule_hits: dict, advisor_json: str, loop=None):
    """Import results from advisor into DB"""
    status = ImportStatus.FAILED
    # TODO: insert system into database if it's 1st upload, shall we update last seen?
    system_data = {
        'rh_account': msg_dict['input']['host']['account'],
        'display_name': msg_dict['input']['host']['display_name'],
        'inventory_id': msg_dict['input']['host']['id'],
        'stale_timestamp': msg_dict['input']['host']['stale_timestamp'],
        'stale_warning_timestamp': msg_dict['input']['host']['stale_warning_timestamp'],
        'culled_timestamp': msg_dict['input']['host']['culled_timestamp']
    }

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                rh_account_id, system_id, import_status = db_import_system_platform(cur, system_data, advisor_json)

                if import_status is None:
                    conn.rollback()
                    LOGGER.debug("Skip recently deleted system: %s", system_data['inventory_id'])
                    return status

                status |= import_status
                if ImportStatus.CHANGED in status:
                    db_import_rule_hits(cur, rh_account_id, system_data['inventory_id'], system_id, rule_hits, loop)

                conn.commit()
                status -= ImportStatus.FAILED
            except DatabaseError:
                conn.rollback()
                status = ImportStatus.FAILED
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                            msg_dict['input'],
                                            'error',
                                            'Error importing system to vulnerability',
                                            service=PAYLOAD_TRACKER_SERVICE)
    return status


def db_import_system_platform(cur, system_data: dict, advisor_json: str):
    """Import/update system platform data"""
    cur.execute("""INSERT INTO rh_account (name) VALUES(%s) ON CONFLICT (name) DO NOTHING
                RETURNING (xmax = 0) AS inserted""", (system_data['rh_account'],))
    inserted = cur.fetchone()
    if inserted:
        NEW_RH_ACCOUNT.inc()

    cur.execute("""SELECT id FROM rh_account WHERE name = %s""", (system_data['rh_account'],))
    rh_account_id = cur.fetchone()[0]

    advisor_checksum = hashlib.sha256(advisor_json.encode('utf-8')).hexdigest()

    cur.execute("""INSERT INTO system_platform
                (inventory_id, rh_account_id, display_name, advisor_checksum,
                stale_timestamp, stale_warning_timestamp, culled_timestamp, stale)
                VALUES (%s, %s, %s, %s, %s, %s, %s, 'F')
                ON CONFLICT (inventory_id) DO UPDATE SET
                display_name = %s, advisor_checksum = %s, stale_timestamp = %s,
                stale_warning_timestamp = %s, culled_timestamp = %s, stale = 'F'
                RETURNING (xmax = 0) AS inserted, id, when_deleted, advisor_unchanged_since, advisor_evaluated""",
                (system_data['inventory_id'], rh_account_id, system_data['display_name'], advisor_checksum,
                 system_data['stale_timestamp'], system_data['stale_warning_timestamp'], system_data['culled_timestamp'],
                 system_data['display_name'], advisor_checksum, system_data['stale_timestamp'],
                 system_data['stale_warning_timestamp'], system_data['culled_timestamp']))

    inserted, system_id, when_deleted, advisor_unchanged_since, advisor_evaluated = cur.fetchone()
    if when_deleted:
        LOGGER.warning('Received recently deleted inventory id: %s', system_data['inventory_id'])
        DELETED_UPLOADED.inc()
        return None, None, None
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or not advisor_evaluated or (advisor_unchanged_since > advisor_evaluated):
        import_status |= ImportStatus.CHANGED

    return rh_account_id, system_id, import_status


def db_init_caches():
    """Init caches for faster lookups"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""SELECT id, name FROM insights_rule""")
                for rule_db_id, rule_name in cur.fetchall():
                    RULES_CACHE[rule_name] = rule_db_id
                cur.execute("""SELECT id, cve FROM cve_metadata""")
                for cve_id, cve_name in cur.fetchall():
                    CVES_CACHE[cve_name] = cve_id
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error initializing caches: ")
                conn.rollback()


def insert_new_cves(cur, rh_account_id, system_id, new_rule_hits):
    """Import completely new system_vulnerability record, not found by VMaaS"""
    cve_system_list = [(rh_account_id, system_id, cve, new_rule_hits[cve]['id'], new_rule_hits[cve].get('details'),
                        new_rule_hits[cve].get('mitigation_reason'), 'now()',) for cve in new_rule_hits]
    execute_values(cur, """INSERT INTO system_vulnerabilities
                        (rh_account_id, system_id, cve_id, rule_id, rule_hit_details, mitigation_reason, when_mitigated)
                        values %s""", cve_system_list, page_size=len(cve_system_list))


def update_cves(cur, to_update, rh_account_id):
    """Updates existing records in system_vulnerabilities, either adding new rule hit or updating it"""
    execute_values(cur, f"""UPDATE system_vulnerabilities AS sv SET rule_id = v.rule_id,
                            rule_hit_details = v.rule_hit_details, mitigation_reason = v.mitigation_reason
                        FROM (VALUES %s) AS v(id, rh_account_id, rule_id, rule_hit_details, mitigation_reason)
                        WHERE v.id = sv.id AND v.rh_account_id = sv.rh_account_id AND sv.rh_account_id = {rh_account_id}""",
                   to_update, template="(%s, %s, %s::integer, %s, %s)", page_size=len(to_update))


def _parse_reports(reports: dict, rule_hits: dict) -> bool:
    reported = False
    for report in reports:
        if 'cves' in report['details']:
            reported = True
            rule = report['rule_id']  # CVE_2021_3156_sudo|CVE_2021_3156_SUDO
            if rule in RULE_BLACKLIST:
                # TODO: remove this once CVE_2017_5753_4_cpu_kernel and CVE_2017_5715_cpu_virt are merged
                continue
            if rule not in RULES_CACHE:
                db_import_rule(rule, list(report['details']['cves'].keys()))
            for cve in report['details']['cves']:
                if cve not in CVES_CACHE:
                    db_import_cve(cve)
                if not report['details']['cves'][cve]:  # False in the CVE dict indicates failed rule
                    rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule],
                                                  'details': json.dumps(report['details']),
                                                  'cve_name': cve}
                elif report['details']['cves'][cve]:
                    rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule],
                                                  'mitigation_reason': report['details']['cves'][cve]}
    return reported


def _parse_passes(passes: dict, rule_hits: dict) -> bool:
    passed = False
    for pass_ in passes:
        if 'cves' in pass_['details']:
            passed = True
            rule_only = pass_['pass_id'].split("|")[0]
            if rule_only in RULE_BLACKLIST:
                # TODO: remove this once CVE_2017_5753_4_cpu_kernel and CVE_2017_5715_cpu_virt are merged
                continue
            if rule_only not in RULES_CACHE:
                db_import_rule(rule_only, list(pass_['details']['cves'].keys()), rule_only=True)
            for cve in pass_['details']['cves']:
                if cve not in CVES_CACHE:
                    db_import_cve(cve)
                rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule_only],
                                              'mitigation_reason': pass_['details']['cves'][cve]}
    return passed


def parse_inventory_data(upload_data: dict) -> (Optional[str], dict):
    """Parse inventory data from upload message."""
    advisor_json = None
    rule_hits = {}
    reports = upload_data['results'].get('reports', [])
    passes = upload_data['results'].get('pass', [])

    reported = _parse_reports(reports, rule_hits)
    passed = _parse_passes(passes, rule_hits)

    if passed or reported:
        advisor_json = json.dumps({"rule_hits": rule_hits})
    else:
        UPLOAD_NO_RESULTS.inc()
        LOGGER.error("Skipping inventory_id because of empty results: %s", upload_data['input']['host']['id'])
    return advisor_json, rule_hits


def main():
    # pylint: disable=too-many-branches, too-many-statements
    """Application entrypoint"""
    init_logging()

    loop = asyncio.get_event_loop()
    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, loop)
    loop.run_until_complete(status_site.start())

    loop.run_until_complete(a_ensure_minimal_schema_version())

    LOGGER.info('Starting advisor listener.')

    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = BoundedExecutor(CFG.max_queue_size, max_workers=CFG.worker_threads)

    def process_message(msg):
        """Message processing logic"""
        PROCESS_MESSAGES.inc()
        LOGGER.debug('Message from topic %s, body: %s', msg.topic, msg.value)

        try:
            msg_dict = json.loads(msg.value.decode('utf8'))
        except json.decoder.JSONDecodeError:
            MESSAGE_PARSE_ERROR.inc()
            LOGGER.exception('Unable to parse message: ')
            return

        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'processing', 'Starting advisor evaluation', service=PAYLOAD_TRACKER_SERVICE)

        if not validate_kafka_msg(msg_dict, REQUIRED_MESSAGE_FIELDS):
            INVALID_INSIGHTS_ACC.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                        'Skipped advisor result due to message coming from non-insights account.', service=PAYLOAD_TRACKER_SERVICE)
            LOGGER.debug('Skipped advisor result due to coming from non-insights account.')
            return
        identity = get_identity(msg_dict['input']['platform_metadata']['b64_identity'])
        if identity is None:
            INVALID_IDENTITY.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                        'Skipped advisor result due to invalid identity header.', service=PAYLOAD_TRACKER_SERVICE)
            LOGGER.debug('Skipped advisor result due to invalid identity header.')
            return
        if not is_entitled_insights(identity, allow_missing_section=True):
            MISSING_INSIGHTS_ENTITLEMENT.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                        'Skipped advisor result due to missing insights entitlement.', service=PAYLOAD_TRACKER_SERVICE)
            LOGGER.debug('Skipped advisor result due to missing insights entitlement.')
            return

        advisor_json, rule_hits = parse_inventory_data(msg_dict)
        if advisor_json:
            LOGGER.info("Evaluating rule hits for inventory_id: %s", msg_dict['input']['host']['id'])
            status = db_import_system(msg_dict, rule_hits, advisor_json, loop)
            if ImportStatus.CHANGED in status:
                LOGGER.debug("Finished evaluating rule hits for inventory_id: %s", msg_dict['input']['host']['id'])
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                            msg_dict['input'],
                                            'success',
                                            'System successfully uploaded and evaluated',
                                            service=PAYLOAD_TRACKER_SERVICE)
            elif ImportStatus.FAILED not in status:
                LOGGER.info("Skipping evaluating rule hits for inventory_id %s due to unchanged system", msg_dict['input']['host']['id'])
                UNCHANGED_SYSTEM.inc()
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                            msg_dict['input'],
                                            'success',
                                            'Unchanged system and not evaluated',
                                            service=PAYLOAD_TRACKER_SERVICE)

    with DatabasePool(CFG.worker_threads):
        db_init_caches()

        ADVISOR_QUEUE.listen(process_message)

        loop.run_forever()

        LOGGER.info('Shutting down.')
        executor.shutdown()


if __name__ == '__main__':
    main()
