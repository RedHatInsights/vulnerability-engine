"""Rules engine results listener"""
import asyncio
import hashlib
import json
import ssl
from typing import Optional
from datetime import datetime

import asyncpg
from asyncpg.connection import Connection
import flags
from dateutil.parser import isoparse
from prometheus_client import Counter

from common import mqueue
from common.config import Config
from common.identity import get_identity, is_entitled_insights
from common.logging import init_logging, get_logger
from common.utils import send_remediations_update, send_msg_to_payload_tracker, a_ensure_minimal_schema_version, validate_kafka_msg, \
    construct_cve_row, a_validate_system_inventory, get_available_remediation_type, release_semaphore
from common.status_app import create_status_app, create_status_runner
from common.constants import format_vmaas_cve_endpoint
from common.vmaas_client import vmaas_request

LOGGER = get_logger(__name__)

CFG = Config()

PROMETHEUS_PORT = CFG.prometheus_port or str(CFG.adv_listener_prometheus_port)

PROCESS_MESSAGES = Counter('ve_advisor_listener_messages_processed', '# of messages processed')
NEW_SYSTEM = Counter('ve_advisor_listener_upl_new_system', '# of new systems inserted by advisor')
UPDATE_SYSTEM = Counter('ve_advisor_listener_upl_update_system', '# of systems updated')
MESSAGE_PARSE_ERROR = Counter('ve_advisor_listener_message_parse_error', '# of message parse errors')
INVALID_IDENTITY = Counter('ve_advisor_listener_invalid_identity', '# of skipped uploads because of invalid identity')
MISSING_INSIGHTS_ENTITLEMENT = Counter('ve_advisor_listener_non_insights_entitlement', '# of skipped uploads because of entitlement check')
DATABASE_ERROR = Counter('ve_advisor_listener_database_error', '# of database errors')
DELETED_UPLOADED = Counter('ve_advisor_listener_deleted_uploaded', '# of systems uploaded after being deleted')
NEW_RH_ACCOUNT = Counter('ve_advisor_listener_upl_new_rh_account', '# of new rh accounts inserted')
INVALID_INSIGHTS_ACC = Counter('ve_advisor_listener_invalid_insights_acc', '# of non-insights messages')
UPLOAD_NO_RESULTS = Counter('ve_advisor_listener_upl_no_result', '# of systems ignored due to missing reports and passes')
UNCHANGED_SYSTEM = Counter('ve_advisor_listener_upl_unchanged_system', '# of system-updates with same advisor results info')
DELETED_SYSTEM_FROM_INVENTORY = Counter('ve_advisor_listener_deleted_from_inventory', '# of systems which are already deleted from inventory')

ADVISOR_QUEUE = mqueue.MQReader([CFG.advisor_results_topic])
REMEDIATIONS_PRODUCER = mqueue.MQWriter(CFG.remediation_updates_topic)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(CFG.payload_tracker_topic)

RULE_BLACKLIST = ['CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_ONLYKERNEL', 'CVE_2017_5715_cpu_virt']

ACCOUNTS_CACHE = {}
RULES_CACHE = {}
CVES_CACHE = {}

REQUIRED_MESSAGE_FIELDS = {
    "input": [{"host": [("insights_id", False)]}]
}

PAYLOAD_TRACKER_SERVICE = "vulnerability-rules"
DB_POOL: Optional[asyncpg.pool.Pool] = None
MAIN_LOOP = asyncio.get_event_loop()
MAX_MESSAGES_SEMAPHORE = asyncio.BoundedSemaphore(CFG.max_loaded_advisor_msgs)


class DeletedSystemException(Exception):
    """Deleted system exception"""


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def db_import_cve(cve: str, conn: Connection):
    """Import missing CVE metadata into database"""
    # Attempt to get CVE data from VMAAS
    LOGGER.info("Attempting to get %s data from VMAAS", cve)
    cve_request_endpoint = format_vmaas_cve_endpoint(cve)

    cve_page = await vmaas_request(cve_request_endpoint, method="GET")
    success = True

    try:
        cve_data = cve_page["cve_list"][cve]
    except (KeyError, TypeError):
        LOGGER.error("Error importing %s from VMAAS", cve)
        success = False

    if success:
        impact_id_map = {}
        async for row in conn.cursor("SELECT name, id FROM cve_impact"):
            impact_name, impact_id = row
            impact_id_map[impact_name] = impact_id

        cve_row = construct_cve_row(cve_data, impact_id_map, asyncpg_type=True)
        inserted = await conn.fetchrow("""INSERT INTO cve_metadata
                                (cve, description, impact_id, public_date, modified_date,
                                cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                secondary_url, advisories_list)
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                                ON CONFLICT (cve) DO UPDATE
                                SET
                                    description = EXCLUDED.description,
                                    impact_id = EXCLUDED.impact_id,
                                    public_date = EXCLUDED.public_date,
                                    modified_date = EXCLUDED.modified_date,
                                    cvss3_score = EXCLUDED.cvss3_score,
                                    cvss3_metrics = EXCLUDED.cvss3_metrics,
                                    cvss2_score = EXCLUDED.cvss2_score,
                                    cvss2_metrics = EXCLUDED.cvss2_metrics,
                                    redhat_url = EXCLUDED.redhat_url,
                                    secondary_url = EXCLUDED.secondary_url,
                                    advisories_list = EXCLUDED.advisories_list
                                RETURNING id AS inserted""", *cve_row)

    # if vmaas was not available insert empty cve
    else:
        inserted = await conn.fetchrow("""INSERT INTO cve_metadata (cve, description, impact_id)
                                            VALUES ($1, $2, $3)
                                            ON CONFLICT (cve) DO UPDATE
                                            SET cve = $1
                                            RETURNING id AS inserted""", cve, 'unknown', 0)
    CVES_CACHE[cve] = inserted[0]


async def db_import_rule(rule_id: str, rule_cves: list, conn: Connection, rule_only=False):
    """Import single error key into database"""
    inserted = await conn.fetchrow("""INSERT INTO insights_rule (name, rule_only)
                                        VALUES($1, $2)
                                        ON CONFLICT (name) DO UPDATE
                                        SET name = EXCLUDED.name, rule_only = EXCLUDED.rule_only
                                        RETURNING id AS inserted""", rule_id, rule_only)
    RULES_CACHE[rule_id] = inserted[0]
    to_insert = []
    for rule_cve in rule_cves:
        if rule_cve not in CVES_CACHE:
            await db_import_cve(rule_cve, conn)
        to_insert.append((inserted[0], CVES_CACHE[rule_cve]))
    await conn.executemany("""INSERT INTO cve_rule_mapping (rule_id, cve_id)
                                VALUES ($1, $2) ON CONFLICT DO NOTHING""", to_insert)


async def _update_system(system_id, cve_count, conn: Connection):
    await conn.execute("""UPDATE system_platform
                            SET advisor_evaluated = NOW(), cve_count_cache = $1
                            WHERE id = $2""", cve_count, system_id)


async def db_import_rule_hits(conn: Connection, rh_account_id: int, inventory_id: str, system_id: int, rule_hits: dict) -> None:
    """Associate rules hits with system"""
    # pylint: disable=too-many-branches, too-many-statements

    active_rules = set()
    rules_playbook_count = {}
    async for row in conn.cursor("""SELECT ir.id, ir.playbook_count, ir.active FROM insights_rule ir"""):
        rule_id, rule_playbook_count, rule_active = row
        rules_playbook_count[rule_id] = rule_playbook_count
        if rule_active:
            active_rules.add(rule_id)

    to_update = []
    rule_hits_cves_ids = tuple(rule_hits.keys())
    system_cves = []

    async for row in conn.cursor("""SELECT sv.id, sv.cve_id, sv.rule_id, sv.when_mitigated, sv.advisory_available, cm.cve
                                    FROM system_vulnerabilities sv
                                    JOIN cve_metadata cm ON sv.cve_id = cm.id
                                    WHERE system_id = $1 AND rh_account_id = $2""", system_id, rh_account_id):
        row_id, cve_id, rule_id, when_mitigated, advisory_available, cve = row
        playbook_count = rules_playbook_count.get(rule_hits.get(cve_id, {}).get('id'))
        mitigation_reason = rule_hits.get(cve_id, {}).get('mitigation_reason')
        remediation_type_id = get_available_remediation_type(advisory_available, when_mitigated, mitigation_reason, playbook_count)
        if cve_id in rule_hits_cves_ids:
            if 'mitigation_reason' in rule_hits[cve_id]:
                if rule_hits[cve_id]['id'] not in active_rules and not when_mitigated:
                    system_cves.append(cve)
                to_update.append((row_id, rh_account_id, rule_hits[cve_id]['id'], None, rule_hits[cve_id]['mitigation_reason'],
                                  advisory_available, remediation_type_id))
            else:
                to_update.append((row_id, rh_account_id, rule_hits[cve_id]['id'], rule_hits[cve_id]['details'], None,
                                  advisory_available, remediation_type_id))
                if rule_hits[cve_id]['id'] in active_rules or not when_mitigated:
                    system_cves.append(cve)
            del rule_hits[cve_id]  # rule hits become dict of completely new system cves
        elif rule_id:
            to_update.append((row_id, rh_account_id, None, None, None, advisory_available, remediation_type_id))
            if not when_mitigated:
                system_cves.append(cve)
        elif not when_mitigated:
            system_cves.append(cve)

    if rule_hits:
        await insert_new_cves(conn, rh_account_id, system_id, rule_hits, rules_playbook_count)
        system_cves.extend([rule_hits[cve]['cve_name'] for cve in rule_hits if rule_hits[cve]['id'] in active_rules and 'details' in rule_hits[cve]])
    if to_update:
        await update_cves(conn, to_update, rh_account_id)

    await _update_system(system_id, len(system_cves), conn)

    send_remediations_update(REMEDIATIONS_PRODUCER, inventory_id, system_cves)


async def db_import_system(msg_dict: dict, rule_hits: dict, advisor_json: str, conn: Connection):
    """Import results from advisor into DB"""
    status = ImportStatus.FAILED
    # TODO: insert system into database if it's 1st upload, shall we update last seen?
    system_data = {
        'rh_account': msg_dict['input']['host']['account'],
        'display_name': msg_dict['input']['host']['display_name'],
        'inventory_id': msg_dict['input']['host']['id'],
        'stale_timestamp': msg_dict['input']['host']['stale_timestamp'],
        'stale_warning_timestamp': msg_dict['input']['host']['stale_warning_timestamp'],
        'culled_timestamp': msg_dict['input']['host']['culled_timestamp']
    }

    try:
        rh_account_id, system_id, import_status = await db_import_system_platform(conn, system_data, advisor_json)

        if import_status is None:
            raise DeletedSystemException()

        status |= import_status
        if ImportStatus.CHANGED in status:
            await db_import_rule_hits(conn, rh_account_id, system_data['inventory_id'], system_id, rule_hits)

        status -= ImportStatus.FAILED
    except DeletedSystemException:
        LOGGER.debug("Skip recently deleted system: %s", system_data['inventory_id'])
    return status


async def db_import_system_platform(conn: Connection, system_data: dict, advisor_json: str):
    """Import/update system platform data"""
    rh_account_id = ACCOUNTS_CACHE.get(system_data['rh_account'])
    if rh_account_id is None:
        inserted = await conn.fetchrow("""INSERT INTO rh_account (name)
                                            VALUES($1)
                                            ON CONFLICT (name) DO NOTHING
                                            RETURNING (xmax = 0) AS inserted""", system_data['rh_account'])
        if inserted:
            NEW_RH_ACCOUNT.inc()

        record = await conn.fetchrow("""SELECT id
                                        FROM rh_account
                                        WHERE name = $1""", system_data['rh_account'])
        rh_account_id = record[0]
        ACCOUNTS_CACHE[system_data['rh_account']] = rh_account_id

    advisor_checksum = hashlib.sha256(advisor_json.encode('utf-8')).hexdigest()

    stale_timestamp = isoparse(system_data['stale_timestamp'])
    stale_warning_timestamp = isoparse(system_data['stale_warning_timestamp'])
    culled_timestamp = isoparse(system_data['culled_timestamp'])
    record = await conn.fetchrow("""INSERT INTO system_platform
                                        (inventory_id, rh_account_id, display_name, advisor_checksum,
                                        stale_timestamp, stale_warning_timestamp, culled_timestamp, stale)
                                    VALUES ($1, $2, $3, $4, $5, $6, $7, 'F')
                                    ON CONFLICT (inventory_id) DO UPDATE
                                    SET display_name = $8, advisor_checksum = $9, stale_timestamp = $10,
                                        stale_warning_timestamp = $11, culled_timestamp = $12, stale = 'F'
                                    RETURNING (xmax = 0)
                                    AS inserted, id, when_deleted, advisor_unchanged_since, advisor_evaluated""",
                                 system_data['inventory_id'], rh_account_id,
                                 system_data['display_name'], advisor_checksum,
                                 stale_timestamp, stale_warning_timestamp, culled_timestamp,
                                 system_data['display_name'], advisor_checksum,
                                 stale_timestamp, stale_warning_timestamp, culled_timestamp)

    inserted, system_id, when_deleted, advisor_unchanged_since, advisor_evaluated = record
    if when_deleted:
        LOGGER.warning('Received recently deleted inventory id: %s', system_data['inventory_id'])
        DELETED_UPLOADED.inc()
        return None, None, None
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or not advisor_evaluated or (advisor_unchanged_since > advisor_evaluated):
        import_status |= ImportStatus.CHANGED

    return rh_account_id, system_id, import_status


async def db_init_caches():
    """Init caches for faster lookups"""
    LOGGER.info('Initialize database cache.')

    async with DB_POOL.acquire() as conn:
        try:
            async with conn.transaction():
                async for row in conn.cursor("""SELECT id, name FROM insights_rule"""):
                    rule_db_id, rule_name = row
                    RULES_CACHE[rule_name] = rule_db_id
                async for row in conn.cursor("""SELECT id, cve FROM cve_metadata"""):
                    cve_id, cve_name = row
                    CVES_CACHE[cve_name] = cve_id
                async for row in conn.cursor("""SELECT id, name FROM rh_account"""):
                    account_id, account_name = row
                    ACCOUNTS_CACHE[account_name] = account_id
        # pylint: disable=broad-except
        except Exception:
            DATABASE_ERROR.inc()
            LOGGER.exception("Error initializing caches: ")


async def insert_new_cves(conn: Connection, rh_account_id, system_id, new_rule_hits, rules_playbook_count):
    """Import completely new system_vulnerability record, not found by VMaaS"""
    cve_system_list = []
    for cve in new_rule_hits:
        # advisory_available = False for rule only CVEs - not sure what is in the playbook, may be advisory or not
        advisory_available = False
        playbook_count = rules_playbook_count.get(new_rule_hits[cve]['id'])
        mitigation_reason = new_rule_hits[cve].get('mitigation_reason')
        remediation_type_id = get_available_remediation_type(advisory_available, "now()", mitigation_reason, playbook_count)
        cve_system_list.append((rh_account_id, system_id, cve, new_rule_hits[cve]['id'], new_rule_hits[cve].get('details'),
                                mitigation_reason, datetime.now(), advisory_available, remediation_type_id,))
    await conn.executemany("""INSERT INTO system_vulnerabilities
                            (rh_account_id, system_id, cve_id, rule_id, rule_hit_details, mitigation_reason, when_mitigated,
                            advisory_available, remediation_type_id)
                            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)""", cve_system_list)


async def update_cves(conn: Connection, to_update, rh_account_id):
    """Updates existing records in system_vulnerabilities, either adding new rule hit or updating it"""
    await conn.executemany(f"""UPDATE system_vulnerabilities AS sv
                                SET rule_id = v.rule_id, rule_hit_details = v.rule_hit_details, mitigation_reason = v.mitigation_reason,
                                    advisory_available = v.advisory_available, remediation_type_id = v.remediation_type_id
                                FROM (VALUES ($1::bigint, $2::int, $3::int, $4, $5, $6::boolean, $7::int))
                                AS v(id, rh_account_id, rule_id, rule_hit_details, mitigation_reason, advisory_available, remediation_type_id)
                                WHERE v.id = sv.id AND v.rh_account_id = sv.rh_account_id AND sv.rh_account_id = {rh_account_id}""",
                           to_update)


async def _parse_reports(reports: dict, rule_hits: dict, conn: Connection) -> bool:
    reported = False
    for report in reports:
        if 'cves' in report['details']:
            reported = True
            rule = report['rule_id']  # CVE_2021_3156_sudo|CVE_2021_3156_SUDO
            if rule in RULE_BLACKLIST:
                # TODO: remove this once CVE_2017_5753_4_cpu_kernel and CVE_2017_5715_cpu_virt are merged
                continue
            if rule not in RULES_CACHE:
                await db_import_rule(rule, list(report['details']['cves'].keys()), conn)
            for cve in report['details']['cves']:
                if cve not in CVES_CACHE:
                    await db_import_cve(cve, conn)
                if not report['details']['cves'][cve]:  # False in the CVE dict indicates failed rule
                    rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule],
                                                    'details': json.dumps(report['details']),
                                                    'cve_name': cve}
                elif report['details']['cves'][cve]:
                    rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule],
                                                    'mitigation_reason': report['details']['cves'][cve]}
    return reported


async def _parse_passes(passes: dict, rule_hits: dict, conn: Connection) -> bool:
    passed = False
    async with DB_POOL.acquire() as conn:
        for pass_ in passes:
            if 'cves' in pass_['details']:
                passed = True
                rule_only = pass_['pass_id'].split("|")[0]
                if rule_only in RULE_BLACKLIST:
                    # TODO: remove this once CVE_2017_5753_4_cpu_kernel and CVE_2017_5715_cpu_virt are merged
                    continue
                if rule_only not in RULES_CACHE:
                    await db_import_rule(rule_only, list(pass_['details']['cves'].keys()),
                                         conn, rule_only=True)
                for cve in pass_['details']['cves']:
                    if cve not in CVES_CACHE:
                        await db_import_cve(cve, conn)
                    rule_hits[CVES_CACHE[cve]] = {'id': RULES_CACHE[rule_only],
                                                  'mitigation_reason': pass_['details']['cves'][cve]}
    return passed


async def parse_inventory_data(upload_data: dict, conn: Connection) -> (Optional[str], dict):
    """Parse inventory data from upload message."""
    advisor_json = None
    rule_hits = {}
    reports = upload_data['results'].get('reports', [])
    passes = upload_data['results'].get('pass', [])

    reported = await _parse_reports(reports, rule_hits, conn)
    passed = await _parse_passes(passes, rule_hits, conn)

    if passed or reported:
        advisor_json = json.dumps({"rule_hits": rule_hits})
    return advisor_json, rule_hits


# pylint: disable=too-many-branches, too-many-statements
@release_semaphore(MAX_MESSAGES_SEMAPHORE)
async def process_message(msg):
    """Message processing logic"""
    PROCESS_MESSAGES.inc()
    LOGGER.debug('Message from topic %s, body: %s', msg.topic, msg.value)

    try:
        msg_dict = json.loads(msg.value.decode('utf8'))
    except json.decoder.JSONDecodeError:
        MESSAGE_PARSE_ERROR.inc()
        LOGGER.exception('Unable to parse message: ')
        return

    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'processing',
                                'Starting advisor evaluation', service=PAYLOAD_TRACKER_SERVICE,
                                loop=MAIN_LOOP)

    if not validate_kafka_msg(msg_dict, REQUIRED_MESSAGE_FIELDS):
        INVALID_INSIGHTS_ACC.inc()
        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                    'Skipped advisor result due to message coming from non-insights account.',
                                    service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
        LOGGER.debug('Skipped advisor result due to coming from non-insights account.')
        return
    identity = get_identity(msg_dict['input']['platform_metadata']['b64_identity'])
    if identity is None:
        INVALID_IDENTITY.inc()
        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                    'Skipped advisor result due to invalid identity header.',
                                    service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
        LOGGER.debug('Skipped advisor result due to invalid identity header.')
        return
    if not is_entitled_insights(identity, allow_missing_section=True):
        MISSING_INSIGHTS_ENTITLEMENT.inc()
        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                    'Skipped advisor result due to missing insights entitlement.',
                                    service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
        LOGGER.debug('Skipped advisor result due to missing insights entitlement.')
        return
    if not await a_validate_system_inventory(msg_dict["input"]["host"]["id"], msg_dict["input"]["timestamp"], DB_POOL):
        DELETED_SYSTEM_FROM_INVENTORY.inc()
        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict['input'], 'error',
                                    'Skipped advisor result due to system not valid in inventory anymore.',
                                    service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
        LOGGER.info('Skipped advisor result due to system not valid in inventory anymore.')
        return

    async with DB_POOL.acquire() as conn:
        try:
            async with conn.transaction():
                advisor_json, rule_hits = await parse_inventory_data(msg_dict, conn)
                if not advisor_json:
                    UPLOAD_NO_RESULTS.inc()
                    LOGGER.error("Skipping inventory_id because of empty results: %s", msg_dict['input']['host']['id'])
                    return

                LOGGER.info("Evaluating rule hits for inventory_id: %s", msg_dict['input']['host']['id'])
                status = await db_import_system(msg_dict, rule_hits, advisor_json, conn)
                if ImportStatus.CHANGED in status:
                    LOGGER.debug("Finished evaluating rule hits for inventory_id: %s", msg_dict['input']['host']['id'])
                    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                                msg_dict['input'],
                                                'success',
                                                'System successfully uploaded and evaluated',
                                                service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
                elif ImportStatus.FAILED not in status:
                    LOGGER.info("Skipping evaluating rule hits for inventory_id %s due to unchanged system", msg_dict['input']['host']['id'])
                    UNCHANGED_SYSTEM.inc()
                    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                                msg_dict['input'],
                                                'success',
                                                'Unchanged system and not evaluated',
                                                service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)
        except Exception:  # pylint: disable=broad-except
            DATABASE_ERROR.inc()
            LOGGER.exception("Unable to store data: ")
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER,
                                        msg_dict['input'],
                                        'error',
                                        'Error importing system to vulnerability',
                                        service=PAYLOAD_TRACKER_SERVICE, loop=MAIN_LOOP)


async def setup_db_pool():
    """Set up database connection pool"""
    LOGGER.info('Setup database pool.')

    dsn = f"postgres://{CFG.db_user}:{CFG.db_pass}@{CFG.db_host}:{CFG.db_port}/{CFG.db_name}?sslmode={CFG.db_ssl_mode}"
    # Validate the CA certificate before sending to asyncpg
    try:
        ssl.create_default_context(cafile=CFG.db_ssl_root_cert_path)
        dsn = f"{dsn}&sslrootcert={CFG.db_ssl_root_cert_path}"
    except (FileNotFoundError, ssl.SSLError):
        pass

    # pylint: disable=global-statement
    global DB_POOL
    DB_POOL = await asyncpg.create_pool(
        dsn=dsn,
        loop=asyncio.get_running_loop(),
        min_size=CFG.db_min_pool_size,
        max_size=CFG.db_max_pool_size
    )


async def run():
    """Run application"""
    await ADVISOR_QUEUE.start()
    await PAYLOAD_TRACKER_PRODUCER.start()
    await REMEDIATIONS_PRODUCER.start()

    try:
        async for msg in ADVISOR_QUEUE.client:
            await MAX_MESSAGES_SEMAPHORE.acquire()
            MAIN_LOOP.create_task(process_message(msg))
    finally:
        LOGGER.info("Shutting down.")
        await ADVISOR_QUEUE.stop()
        await DB_POOL.close()
        await PAYLOAD_TRACKER_PRODUCER.stop()
        await REMEDIATIONS_PRODUCER.stop()


def main():
    """Application entrypoint"""
    init_logging()

    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, MAIN_LOOP)
    MAIN_LOOP.run_until_complete(status_site.start())
    MAIN_LOOP.run_until_complete(a_ensure_minimal_schema_version())

    MAIN_LOOP.run_until_complete(setup_db_pool())
    MAIN_LOOP.run_until_complete(db_init_caches())
    LOGGER.info('Starting advisor listener.')
    MAIN_LOOP.run_until_complete(run())


if __name__ == '__main__':
    main()
