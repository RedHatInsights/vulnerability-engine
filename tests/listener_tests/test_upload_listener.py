# -*- coding: utf-8 -*-
# pylint: disable=no-self-use
"""
Unit tests for listener module
"""

# pylint: disable=too-many-public-methods

import asyncio
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy
import logging

import pytest

import listener.upload_listener
from listener.upload_listener import format_vmaas_request, db_import_system, process_upload, LOGGER, \
    terminate, LISTENER_QUEUE, EVALUATOR_QUEUE, on_thread_done, format_repo_path, \
    db_delete_system, process_delete, ImportStatus, db_import_repos, db_import_system_repos, db_init_repo_cache, \
    db_delete_other_system_repos, db_update_system, process_update, main, process_message, CFG, ListenerCtx
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.mqueue import MQWriter
from common import utils

# We should refactor upload_listener to be a class where db and other things
# are initialized in __init__() method.  This would allow us to override
# those things in unit tests here.

PACKAGE_LIST = ['package-a', 'package-b']
REPO_LIST = ['repo-c', 'repo-d']
REPO_PATHS = ["/content/dist/rhel8/rhui/8/x86_64/baseos/os", "/content/dist/rhel8/rhui/8.4/x86_64/appstream/os"]
MODULES_LIST = ['module-e', 'module-f']
BASEARCH = "x86_64"
RESULT_JSON = '{"package_list": ["package-a", "package-b"], ' \
              '"repository_list": ["repo-c", "repo-d"], ' \
              '"repository_paths": ["/content/dist/rhel8/rhui/8/x86_64/baseos/os", ' \
              '"/content/dist/rhel8/rhui/8.4/x86_64/appstream/os"], ' \
              '"extended": true, '\
              '"modules_list": ["module-e", "module-f"], '\
              '"basearch": "x86_64"}'

A_SYSTEM = {'vmaas-json': 'NEW-JSON', 'host': {'id': '8e08be35-8419-4472-8fda-9fbb706e6d71',
                                               'account': 'NEW-ACCT',
                                               'org_id': 'NEW-ORG',
                                               'display_name': 'example.com',
                                               'stale_timestamp': '2020-01-09T10:17:33.881280+00:00',
                                               'stale_warning_timestamp': '2020-01-16T10:17:33.881280+00:00',
                                               'culled_timestamp': '2020-01-23T10:17:33.881280+00:00'},
            'platform_metadata': {'url': 'NEW-URL'}}
A_SYSTEM_DELETE = {'id': '8e08be35-8419-4472-8fda-9fbb706e6d71', 'account': 'NEW-ACCT', 'org_id': 'NEW-ORG'}
A_SYSTEM_UPDATE = {'host': {'id': '8e08be35-8419-4472-8fda-9fbb706e6d71', 'display_name': 'new.example.com'}}
TESTING_SYSTEM = "42e1f448-11da-4cc3-9f67-fc8c04830c44"

TEST_TOPIC = "test_topic"


def empty_fun(*_, **__):
    """Empty function for mocking functions"""
    return True


class KafkaMsgMock:
    """Mock for kafka MSG"""

    def __init__(self, topic, value):
        self.topic = topic
        self.value = value


class FutureMock:
    """Mock for future"""

    def add_done_callback(self, *__, **___):
        """Do not add callback"""


class ExecutorMock:
    """Mock for threaded executor
       Mock function submit checks if the function is equal
       to the correct_function field in constructor
    """

    def __init__(self, correct_function):
        self.correct_function = correct_function

    def submit(self, chosen_function, *_, **__):
        """Method mocks the submit function of executor
           to check if the function is correct type (used in tests)"""
        assert chosen_function == self.correct_function
        return FutureMock()


class TestUploadListener:
    """Unit tests for vulnerability-listener."""

    @pytest.mark.asyncio
    async def test_terminate(self):
        """test_terminate"""
        loop = asyncio.new_event_loop()
        await terminate(0, loop)
        assert not loop.is_running()
        assert not LISTENER_QUEUE.connected
        assert not EVALUATOR_QUEUE.connected

    def test_on_thread_done(self, caplog):
        "test_on_thread_done"
        with caplog.at_level(logging.ERROR):
            with ThreadPoolExecutor(max_workers=1) as executor:
                future_ok = executor.submit(lambda: True)
                future_ok.add_done_callback(on_thread_done)
        assert not caplog.records  # empty log
        caplog.clear()

        with caplog.at_level(logging.ERROR):
            with ThreadPoolExecutor(max_workers=1) as executor:
                future_fail = executor.submit(lambda: 1 / 0)
                future_fail.add_done_callback(on_thread_done)
        assert caplog.records  # not empty log
        caplog.clear()

    def test_format_vmaas_request(self):
        """test_format_vmaas_request"""
        result = format_vmaas_request(PACKAGE_LIST, REPO_LIST, BASEARCH, modules_list=MODULES_LIST, repo_paths=REPO_PATHS)
        assert result
        assert result == RESULT_JSON

    def test_format_repo_path(self):
        """Test format into repo path out of mirrorlist"""

        assert format_repo_path(None) is None
        assert format_repo_path("") is None
        assert format_repo_path("https://rhui.redhat.com[") is None
        assert format_repo_path("https://rhui.redhat.com/") is None

        assert format_repo_path(
            "https://rhui.redhat.com/pulp/mirror/content/dist/rhel8/rhui/8.4/x86_64/baseos/os",
        ) == "/content/dist/rhel8/rhui/8.4/x86_64/baseos/os"

        assert format_repo_path(
            "https://rhui.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/baseos/os",
            basearch="x86_64", releasever="8.4"
        ) == "/content/dist/rhel8/rhui/8.4/x86_64/baseos/os"

        assert format_repo_path(
            "https://rhui.redhat.com/pulp/content/eus/rhel8/rhui/8.4/x86_64/baseos/os",
        ) == "/content/eus/rhel8/rhui/8.4/x86_64/baseos/os"

    @staticmethod
    def _mark_evaluated(inventory_id: str):
        with DatabasePoolConnection() as conn:
            with conn.cursor() as cur:
                cur.execute("""update system_platform set last_evaluation = CURRENT_TIMESTAMP
                               where inventory_id = %s""", (inventory_id,))
            conn.commit()

    def test_import_system(self, pg_db_conn, caplog):  # pylint: disable=unused-argument
        """Test importing a system not-in-the-db, followed by same so it's-already-there"""
        # new system
        with DatabasePool(1):
            rtrn = db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])
            assert ImportStatus.INSERTED | ImportStatus.CHANGED == rtrn

            self._mark_evaluated(A_SYSTEM['host']['id'])

            # And now it's an rtrn['updated'], but same json
            rtrn = db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])
            assert ImportStatus.UPDATED == rtrn

            # And now it's another rtrn['updated'], same json
            rtrn = db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])
            assert ImportStatus.UPDATED == rtrn

            # And now it's an rtrn['updated'], with diff json
            rtrn = db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'] + '-1', [])
            assert ImportStatus.UPDATED | ImportStatus.CHANGED == rtrn

            # And try to import system with invalid inventory id
            with caplog.at_level(logging.ERROR):
                system_copy = deepcopy(A_SYSTEM)
                system_copy['host']['id'] = None
                db_import_system(system_copy, A_SYSTEM['vmaas-json'], [])
            assert caplog.records[0].msg.startswith("Error importing system:")
            caplog.clear()

    def test_process_upload(self, pg_db_conn, monkeypatch, caplog):  # pylint: disable=unused-argument
        """Test to see that upload only sends eval-msgs on new systems and ones with new vmaas_json"""
        upld_data = {'host': {'id': '38025376-77c3-4848-989c-d0d417456f9c', 'account': 'AN-ACCOUNT', 'org_id': 'AN-ORG',
                              'system_profile': {'installed_packages': ['kernel']}},
                     'platform_metadata': {'url': 'A-URL', 'request_id': 'some-request-id'},
                     'timestamp': '2021-11-24 12:00:00-04'}
        monkeypatch.setattr(MQWriter, 'send', lambda self, msg, loop: LOGGER.info('SENT'))

        with DatabasePool(1):
            # first-upload - should send
            caplog.clear()
            with caplog.at_level(logging.INFO):
                sent = process_upload(upld_data, None)
            assert caplog.records[0].msg == 'SENT'
            assert sent is True

            self._mark_evaluated(upld_data['host']['id'])

            # re-upload - should not send
            caplog.clear()
            with caplog.at_level(logging.INFO):
                sent = process_upload(upld_data, None)
            assert sent is False

            # same-id, diff pkg profile - should send
            upld_data['host']['system_profile']['installed_packages'].append('glibc')
            caplog.clear()
            with caplog.at_level(logging.INFO):
                sent = process_upload(upld_data, None)
            assert caplog.records[0].msg == 'SENT'
            assert sent is True

    def test_update_system(self, pg_db_conn, caplog):  # pylint: disable=unused-argument
        """Test updating a system."""
        with DatabasePool(1):
            # make sure system is in DB
            db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])

            # now update the system
            rtrn = db_update_system(A_SYSTEM_UPDATE)
            assert rtrn['updated']
            assert not rtrn['failed']

            # try to update system with non-existing id
            rtrn = db_update_system({'host': {'id': '11111111-1111-1111-1111-111111111111', 'display_name': 'new.example.com'}})
            assert not rtrn['updated']
            assert not rtrn['failed']
            caplog.clear()

    def test_process_update(self, pg_db_conn, caplog):  # pylint: disable=unused-argument
        """Test updating a system."""

        with DatabasePool(1):
            # make sure system is in DB
            db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])

            # update the system
            with caplog.at_level(logging.INFO):
                process_update(A_SYSTEM_UPDATE)
            assert caplog.records[0].msg.startswith("Updated system with inventory_id:")
            caplog.clear()

            # try to update system with non-existing id
            with caplog.at_level(logging.INFO):
                process_update({'host': {'id': '11111111-1111-1111-1111-111111111111', 'display_name': 'new.example.com'}})
            assert caplog.records[0].msg.startswith('Unable to update system, inventory_id not found: ')
            caplog.clear()

    def test_delete_system(self, pg_db_conn, caplog):  # pylint: disable=unused-argument
        """Test deleting a system."""
        with DatabasePool(1):
            # make sure system is in DB
            db_import_system(A_SYSTEM, A_SYSTEM['vmaas-json'], [])

            # now delete the system
            rtrn = db_delete_system(A_SYSTEM_DELETE)
            assert rtrn['deleted']
            assert not rtrn['failed']

            # and again, same result, because the record in system_platform still exists
            rtrn = db_delete_system(A_SYSTEM_DELETE)
            assert rtrn['deleted']
            assert not rtrn['failed']

            # try to delete system with non-existing id
            rtrn = db_delete_system({'id': '11111111-1111-1111-1111-111111111111', 'account': 'NEW-ACCT', 'org_id': 'NEW-ORG'})
            assert not rtrn['deleted']
            assert not rtrn['failed']
            caplog.clear()

    def test_process_delete(self, pg_db_conn, caplog):  # pylint: disable=unused-argument
        """Test deleting a system."""

        msg_dict = {'id': '0c32021e-8af8-4186-afb4-0255a71ece96', 'account': A_SYSTEM_DELETE['account'], 'org_id': A_SYSTEM_DELETE['org_id']}

        with DatabasePool(1):
            # make sure system is in DB
            system_copy = deepcopy(A_SYSTEM)
            system_copy['host']['id'] = msg_dict['id']
            db_import_system(system_copy, system_copy['vmaas-json'], [])

            # delete the system
            with caplog.at_level(logging.INFO):
                process_delete(msg_dict)
            assert caplog.records[0].msg.startswith("Deleted system with inventory_id:")
            caplog.clear()

            with caplog.at_level(logging.WARNING):
                db_import_system(system_copy, system_copy['vmaas-json'], [])
            assert caplog.records[0].msg.startswith('Received recently deleted inventory id:')
            caplog.clear()

    @staticmethod
    def _create_testing_system(cur, inventory_id: str):
        cur.execute("""insert into system_platform
                       (inventory_id, rh_account_id, s3_url, vmaas_json, json_checksum)
                       values (%s, 0, 'url', '{}', '0') returning id""", (inventory_id,))
        return cur.fetchone()[0]

    @staticmethod
    def _clean_tmp_db_items(cur, inventory_ids: tuple):
        cur.execute("""delete from system_repo""")
        cur.execute("""delete from repo""")
        cur.execute("""delete from system_platform where inventory_id in %s""", (inventory_ids,))
        listener.upload_listener.REPO_ID_CACHE = {}

    def test_import_repos(self, pg_db_conn):
        """Test import repos to db. Ensure unique repos."""

        cur = pg_db_conn.cursor()
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))
        inserted = db_import_repos(cur, ["repo1", "repo2", "repo1"])
        assert len(inserted) == 2
        assert set(inserted) == {"repo1", "repo2"}

        inserted = db_import_repos(cur, ["repo2", "repo3", "repo4"])
        assert len(inserted) == 2
        assert set(inserted) == {"repo3", "repo4"}

        cur.execute("""select name from repo""")
        rows = cur.fetchall()
        assert len(rows) == 4
        assert {"repo1", "repo2", "repo3", "repo4"} == {repo_name for repo_name, *_ in rows}
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))

    def test_init_repo_cache(self, pg_db_conn):
        """Test initializing repo cache."""

        cur = pg_db_conn.cursor()
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))
        inserted = db_import_repos(cur, ["repo1", "repo2", "repo1"])
        assert len(inserted) == 2
        assert set(inserted) == {"repo1", "repo2"}
        assert len(listener.upload_listener.REPO_ID_CACHE) == 2
        listener.upload_listener.REPO_ID_CACHE = {}
        with DatabasePool(1):
            db_init_repo_cache()
        assert len(listener.upload_listener.REPO_ID_CACHE) == 2
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))

    def test_import_system_repos(self, pg_db_conn):
        """Test import system_repo items to db."""

        cur = pg_db_conn.cursor()
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))
        repos = ["repo1", "repo2"]
        db_import_repos(cur, repos)
        system_id = self._create_testing_system(cur, TESTING_SYSTEM)
        inserted = db_import_system_repos(cur, repos, system_id)
        assert len(inserted) == 2
        cur.execute("select system_id, repo.name from system_repo inner join repo on repo.id = repo_id")
        rows = cur.fetchall()
        assert len(rows) == 2
        assert set(rows) == {(system_id, "repo1"), (system_id, "repo2")}
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))

    def test_delete_other_system_repos(self, pg_db_conn):
        """Test delete not actual system_repo items."""

        cur = pg_db_conn.cursor()
        self._clean_tmp_db_items(cur, (TESTING_SYSTEM,))
        repos = ["repo1", "repo2", "repo3"]
        inserted_repos = db_import_repos(cur, repos)
        assert len(inserted_repos) == 3
        system_id = self._create_testing_system(cur, TESTING_SYSTEM)
        inserted = db_import_system_repos(cur, ["repo1", "repo2", "repo3"], system_id)
        assert len(inserted) == 3
        deleted_system_repos = db_delete_other_system_repos(cur, ["repo3"], system_id)
        assert len(deleted_system_repos) == 2

    def test_main(self, monkeypatch, caplog):
        """Test main"""

        class LoopMock:
            """Event loop mock"""
            loop = asyncio.get_event_loop()

            def run_until_complete(self, *args, **kwargs):
                """run_until_complete mock"""
                self.loop.run_until_complete(*args, **kwargs)

            def add_signal_handler(self, *args, **kwargs):
                """add_signal_handler mock"""
                self.loop.add_signal_handler(*args, **kwargs)

            def run_forever(self, *_, **__):
                """Dont run forever, just mock for tests"""

        def get_event_loop_mock_single():
            if get_event_loop_mock_single.popped:
                return LoopMock.loop
            get_event_loop_mock_single.popped = True
            return LoopMock()

        get_event_loop_mock_single.popped = False
        CFG.minimal_schema = 0
        monkeypatch.setattr(asyncio, "get_event_loop", get_event_loop_mock_single)
        listener.upload_listener.LISTENER_QUEUE.listen = lambda _: None

        with caplog.at_level(logging.INFO):
            main()
        assert "Starting upload listener" in caplog.messages[1]
        assert "Shutting down" in caplog.messages[2]

    def test_process_msg_wrong_json(self, caplog):
        """Test invalid json in message"""
        ListenerCtx.set_listener_ctx()
        msg = KafkaMsgMock(TEST_TOPIC, "invalid_json".encode("utf-8"))

        with caplog.at_level(logging.INFO):
            process_message(msg)

        assert "Unable to parse message" in caplog.messages[0]

    def test_process_msg_wrong_topic(self, caplog):
        """Test wrong topic in msg"""
        ListenerCtx.set_listener_ctx()
        CFG.events_topic = TEST_TOPIC
        wrong_topic = "wrong_topic"
        msg = KafkaMsgMock(wrong_topic, "{}".encode("utf-8"))

        with caplog.at_level(logging.INFO):
            process_message(msg)

        assert f"Received message on unsupported topic: {wrong_topic}" in caplog.messages[-1]

    def test_process_msg_wrong_event(self, caplog):
        """Test wrong event in msg"""
        ListenerCtx.set_listener_ctx()
        CFG.events_topic = TEST_TOPIC
        wrong_type = "wrong"
        msg = KafkaMsgMock(TEST_TOPIC, f"{{ \"type\": \"{wrong_type}\" }}".encode("utf-8"))

        with caplog.at_level(logging.INFO):
            process_message(msg)

        assert f"Received unknown event type: {wrong_type}" in caplog.messages[0]

    def test_process_msg_delete(self, caplog):
        """Test delete type msg"""
        ListenerCtx.set_listener_ctx()
        executor = ExecutorMock(process_delete)
        ListenerCtx.executor = executor

        CFG.events_topic = TEST_TOPIC
        msg = KafkaMsgMock(TEST_TOPIC, """{
            "type": "delete",
            "id": 123,
            "account": 0,
            "org_id": 0,
            "insights_id": 123
        }""".encode("utf-8"))

        with caplog.at_level(logging.INFO):
            process_message(msg)

        assert "Received delete msg, inventory_id: 123" in caplog.messages[0]

    def test_process_msg_created(self, caplog, monkeypatch):
        """Test created type msg"""
        ListenerCtx.set_listener_ctx()
        executor = ExecutorMock(process_upload)
        ListenerCtx.executor = executor

        CFG.events_topic = TEST_TOPIC
        msg = KafkaMsgMock(TEST_TOPIC, """{
                "type": "created",
                "host": {
                    "id": "00000000-0000-0000-0000-000000000001",
                    "account": 123,
                    "org_id": 123,
                    "system_profile": {},
                    "display_name": 123,
                    "insights_id": "11111111-0000-0000-0000-000000000000"
                },
                "timestamp": "2021-07-29T16:01:54.265289387Z",
                "platform_metadata": {
                    "b64_identity": 123,
                    "url": 123
                }
            }""".encode("utf-8"))

        monkeypatch.setattr(utils, "send_msg_to_payload_tracker", empty_fun)
        monkeypatch.setattr(listener.upload_listener, "get_identity", empty_fun)
        monkeypatch.setattr(listener.upload_listener, "is_entitled_insights", empty_fun)

        with caplog.at_level(logging.INFO):
            with DatabasePool(1):
                process_message(msg)

        assert "Received created/updated msg" in caplog.messages[0]

    def test_process_msg_updated(self, caplog):
        """Test updated type msg"""
        ListenerCtx.set_listener_ctx()
        executor = ExecutorMock(process_update)
        ListenerCtx.executor = executor

        CFG.events_topic = TEST_TOPIC
        msg = KafkaMsgMock(TEST_TOPIC, """{
            "type": "updated",
            "host": {
                "id": "00000000-0000-0000-0000-000000000001",
                "account": 123,
                "org_id": 123,
                "system_profile": {},
                "display_name": 123,
                "insights_id": "11111111-0000-0000-0000-000000000000"
            },
            "timestamp": "2021-07-29T16:01:54.265289387Z"
        }""".encode("utf-8"))

        with caplog.at_level(logging.INFO):
            with DatabasePool(1):
                process_message(msg)

        assert "Received update event msg" in caplog.messages[0]
