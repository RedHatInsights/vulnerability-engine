"""
Inventory queue item processor
"""
import hashlib
import json
from urllib.parse import urlparse
from dateutil.parser import isoparse

from asyncpg.pool import Pool

from common.mqueue import MQWriter
from common.logging import get_logger
from .common import (CFG, REPO_PATH_PATTERN, REPO_BASEARCH_PLACEHOLDER,
                     REPO_RELEASEVER_PLACEHOLDER, NEW_RH_ACCOUNT,
                     RHUI_PATH_PART, UPLOAD_NO_RPMDB, NEW_REPO, NEW_SYSTEM_REPO,
                     DELETED_SYSTEM_REPO, DELETED_UPLOADED, DATABASE_ERROR,
                     UPDATE_SYSTEM, NEW_SYSTEM,
                     ImportStatus, InventoryMsgType, QueueItem)

LOGGER = get_logger(__name__)


class ListenerItemProcessor:
    # pylint: disable=no-self-use,broad-except
    """Listener Item Processor, processed items for ListenerQueue"""

    def __init__(self, db_pool: Pool, payload_tracker: MQWriter):
        """Constructor"""
        self.db_pool = db_pool
        self.repo_id_cache = {}
        self.payload_tracker = payload_tracker
        self.evaluator = MQWriter(CFG.evaluator_upload_topic)

    async def init(self):
        """Async constructor"""
        await self._load_repo_cache()

    async def stop(self):
        """Stops processing"""
        await self.evaluator.stop()

    async def _load_repo_cache(self):
        """Load the repository cache"""
        LOGGER.info("Populating repo cache")
        async with self.db_pool.acquire() as conn:
            try:
                for repo in await conn.fetch("SELECT id, name FROM repo"):
                    self.repo_id_cache[repo["name"]] = repo["id"]
            except Exception as exc:
                LOGGER.exception("Cannot cache repos: %s", exc)

    async def _db_account_lookup(self, conn, acc_num: str, org_id: str) -> int:
        """Lookup internal DB id for account number or org id)"""
        res = await conn.fetchrow("SELECT id FROM rh_account WHERE org_id = $1", org_id)
        if res:
            return res["id"]

        result = None
        if acc_num:
            result = await conn.fetchrow("""
                INSERT INTO rh_account (account_number, org_id) VALUES ($1, $2) ON CONFLICT (account_number) DO UPDATE SET
                org_id = $3 RETURNING (xmax = 0) AS inserted, id
            """, acc_num, org_id, org_id)
        else:
            result = await conn.fetchrow("""
                INSERT INTO rh_account (org_id) VALUES ($1) ON CONFLICT (org_id) DO UPDATE SET
                org_id = $2 RETURNING (xmax = 0) AS inserted, id
            """, org_id, org_id)
        if result["inserted"]:
            NEW_RH_ACCOUNT.inc()
        return result["id"]

    def _format_repo_placeholders(self, path, basearch=None, releasever=None) -> str:
        """Format repository path by replacing placeholders"""
        repo_path = path
        if basearch:
            repo_path = repo_path.replace(REPO_BASEARCH_PLACEHOLDER, basearch)
        if releasever:
            repo_path = repo_path.replace(REPO_RELEASEVER_PLACEHOLDER, releasever)
        return repo_path

    def _format_repo_path(self, repo_url, basearch=None, releasever=None) -> str:
        """Format known repository path out of base URL or mirrolist (RHUI only)"""
        if not repo_url or not repo_url.strip():
            return None

        try:
            parsed = urlparse(repo_url)
        except ValueError as err:
            LOGGER.warning("Base URL/mirrorlist parse error: %s (value: %s)", err, repo_url)
            return None

        path = parsed.path
        path_match = REPO_PATH_PATTERN.search(path)
        if RHUI_PATH_PART in path and path_match:
            return self._format_repo_placeholders(path_match.group(1),
                                                  basearch=basearch, releasever=releasever)
        return None

    def _format_repo_list(self, yum_repos=(), basearch=None, releasever=None) -> (list, list):
        """Get repo list and repo paths out of yum repos using basearch and releasever"""
        repo_list = []
        repo_paths = set()
        for repo in yum_repos:
            if repo.get("enabled", True) and repo.get("id", "").strip():
                repo_list.append(repo["id"].strip())
                repo_url = repo.get("mirrorlist") or repo.get("base_url")
                repo_path = self._format_repo_path(repo_url, basearch=basearch, releasever=releasever)
                if repo_path:
                    repo_paths.add(repo_path)
        return repo_list, list(repo_paths)

    def _format_vmaas_req(self, package_list: [], repo_list: [], basearch: str, modules_list=None, releasever=None, repo_paths=None) -> str:
        """Wrap package and repo list into the vmaas request format"""
        vmaas_request = {"package_list": package_list,
                         "repository_list": repo_list,
                         "repository_paths": repo_paths,
                         "extended": True}
        if modules_list:
            vmaas_request["modules_list"] = modules_list
        if basearch:
            vmaas_request["basearch"] = basearch
        if releasever:
            vmaas_request["releasever"] = releasever
        return json.dumps(vmaas_request)

    def _parse_inventory_system_data(self, item: QueueItem) -> (str, list):
        """Parse needed system data from inventory message"""
        inv_msg = item.inventory_msg

        vmaas_request = None
        repo_list = []
        system_profile = inv_msg["host"]["system_profile"]
        installed_packages = system_profile.get("installed_packages")
        if installed_packages:
            basearch = system_profile.get("basearch") or system_profile.get("arch")
            rhsm_ver = system_profile.get("rhsm", {}).get("version")
            releasever = system_profile.get("releasever")

            repo_list, repo_paths = self._format_repo_list(system_profile.get("yum_repos", ()),
                                                           basearch=basearch,
                                                           releasever=(rhsm_ver or releasever))
            modules_list = [{"module_name": m["name"], "module_stream": m["stream"]} for m in system_profile.get("dnf_modules", [])]
            vmaas_request = self._format_vmaas_req(installed_packages, repo_list, basearch,
                                                   modules_list=modules_list,
                                                   releasever=rhsm_ver,
                                                   repo_paths=repo_paths)
        return vmaas_request, repo_list

    async def _db_import_repos(self, conn, repo_list: list) -> [str]:
        """Import not yet imported repos to db"""
        repo_names = []
        to_insert = sorted([repo for repo in repo_list if repo not in self.repo_id_cache])
        if to_insert:
            for row in await conn.fetch("""INSERT INTO repo (name) (SELECT * FROM unnest($1::varchar[]))
                                           RETURNING id, name, (xmax = 0) AS inserted""", to_insert):
                if row["inserted"]:
                    NEW_REPO.inc()
                    repo_names.append(row["name"])
                self.repo_id_cache[row["name"]] = row["id"]
        return repo_names

    async def _db_import_system_repos(self, conn, repo_list: list, system_id: int) -> [int]:
        """Link system with given repositories"""
        repo_ids = []
        if repo_list:
            to_insert_system_ids = []
            to_insert_repo_ids = []
            for repo in repo_list:
                to_insert_system_ids.append(system_id)
                to_insert_repo_ids.append(self.repo_id_cache[repo])

            for row in await conn.fetch("""
                INSERT INTO system_repo (system_id, repo_id) (SELECT * FROM unnest($1::int[], $2::int[]))
                ON CONFLICT (system_id, repo_id) DO NOTHING RETURNING repo_id
            """, to_insert_system_ids, to_insert_repo_ids):
                repo_ids.append(row["repo_id"])
            if repo_ids:
                NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids

    async def _db_unlink_system_repos(self, conn, repo_list: list, system_id: int) -> [int]:
        """Unlink repos which are not linked to system anymore"""
        deleted_repo_ids = []
        if repo_list:
            to_delete_repos_ids = [self.repo_id_cache[repo] for repo in repo_list]
            for row in await conn.fetch("""
                DELETE FROM system_repo AS sr
                WHERE sr.system_id = $1 AND sr.repo_id NOT IN (SELECT * FROM unnest($2::int[]))
                RETURNING sr.repo_id
            """, system_id, to_delete_repos_ids):
                deleted_repo_ids.append(row["repo_id"])
        else:
            for row in await conn.fetch("""DELETE FROM system_repo WHERE system_id = $1 RETURNING repo_id"""):
                deleted_repo_ids.append(row["repo_id"])
        if deleted_repo_ids:
            DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
        return deleted_repo_ids

    def _prepare_db_insert(self, fields: dict) -> (str, str, str, []):
        """Dynamically prepare insert fields for system_platform table,
           based off the previously prepared fields dict by messages"""
        values = []
        columns = []
        formats = []
        upserts = []
        index = 1
        for column, value in fields.items():
            formats.append(f"${index}")
            columns.append(column)
            values.append(value)
            upserts.append(f"{column} = EXCLUDED.{column}")
            index += 1
        return ",".join(columns), ",".join(formats), ",".join(upserts), values

    async def _db_import_system(self, conn, fields: dict, org_id: str, acc_num: str) -> (ImportStatus, int):
        """Import system to system_platform table, update if exists"""
        rh_account_id = await self._db_account_lookup(conn, acc_num, org_id)

        fields["rh_account_id"] = rh_account_id
        update_columns, update_formats, upsert_formats, values = self._prepare_db_insert(fields)
        insert_query = f"""
            INSERT INTO system_platform
            ({update_columns},last_upload,stale)
            VALUES ({update_formats},CURRENT_TIMESTAMP,'F')
            ON CONFLICT (inventory_id) DO UPDATE SET
            {upsert_formats},last_upload = EXCLUDED.last_upload,stale = EXCLUDED.stale
            RETURNING (xmax = 0) AS inserted, unchanged_since, last_evaluation, id, when_deleted, advisor_unchanged_since, advisor_evaluated
            """

        res = await conn.fetchrow(insert_query, *values)
        if res["when_deleted"]:
            LOGGER.warning("Received recently deleted inventory_id: %s", fields["inventory_id"])
            DELETED_UPLOADED.inc()
            return None, None
        result = None
        if res["inserted"]:
            result = ImportStatus.INSERTED
            NEW_SYSTEM.inc()
        else:
            result = ImportStatus.UPDATED
            UPDATE_SYSTEM.inc()
        if res["inserted"] or (not res["last_evaluation"] or (res["unchanged_since"] > res["last_evaluation"])):
            result |= ImportStatus.CHANGED
        elif res["inserted"] or (not res["advisor_evaluated"] or (res["advisor_unchanged_since"] > res["advisor_evaluated"])):
            result |= ImportStatus.CHANGED
        return result, res["id"]

    def _parse_inventory_db_fields(self, fields: dict, item: QueueItem, vmaas_json: dict):
        """Parse DB fields for system_platform table insertion query
           from inventory message"""
        if not item.inventory_msg:
            return fields

        msg = item.inventory_msg
        host = item.inventory_msg["host"]

        fields["inventory_id"] = host["id"]
        fields["s3_url"] = msg["platform_metadata"]["url"]
        fields["vmaas_json"] = vmaas_json
        fields["json_checksum"] = hashlib.sha256(vmaas_json.encode("utf-8")).hexdigest()

        if host.get("display_name"):
            fields["display_name"] = host["display_name"]
        if host.get("stale_timestamp"):
            fields["stale_timestamp"] = isoparse(host["stale_timestamp"])
        if host.get("stale_warning_timestamp"):
            fields["stale_warning_timestamp"] = isoparse(host["stale_warning_timestamp"])
        if host.get("culled_timestamp"):
            fields["culled_timestamp"] = isoparse(host["culled_timestamp"])
        if host.get("system_profile", {}).get("host_type"):
            fields["host_type"] = host["system_profile"]["host_type"]
        return fields

    def _parse_hits(self, reports: dict, rule_results: dict):
        """Parse rule hits from advisor message"""
        for report in reports:
            if "cves" in report["details"]:
                rule = report["rule_id"]
                for cve in report["details"]["cves"]:
                    if not report["details"]["cves"][cve]:
                        rule_results[cve] = {
                            "rule_id": rule,
                            "details": json.dumps(report["details"]),
                        }
                    elif report["details"]["cves"][cve]:
                        rule_results[cve] = {
                            "rule_id": rule,
                            "mitigation_reason": report["details"]["cves"][cve]
                        }

    def _parse_passes(self, passes: dict, rule_results: dict):
        """Parse rule passes from advisor message"""
        for pass_ in passes:
            if "cves" in pass_["details"]:
                rule_only = pass_["pass_id"].split("|")[0]
                for cve in pass_["details"]["cves"]:
                    rule_results[cve] = {
                        "rule_id": rule_only,
                        "mitigation_reason": pass_["details"]["cves"][cve]
                    }

    def _parse_advisor_db_fields(self, fields: dict, item: QueueItem) -> dict:
        """Prepare DB fields for system_platform table insertion query
           from advisor message"""
        if not item.advisor_msg:
            return fields

        msg = item.advisor_msg
        host = item.advisor_msg["input"]["host"]

        fields["inventory_id"] = host["id"]
        fields["display_name"] = host["display_name"]

        rule_passes = {}
        passes = msg["results"].get("pass", [])
        self._parse_passes(passes, rule_passes)

        rule_hits = {}
        hits = msg["results"].get("reports", [])
        self._parse_hits(hits, rule_hits)

        results_raw = json.dumps({"rule_hits": rule_hits,
                                  "rule_passes": rule_passes}, sort_keys=True)

        fields["advisor_checksum"] = hashlib.sha256(results_raw.encode("utf-8")).hexdigest()
        fields["rule_results"] = results_raw

        if host.get("stale_timestamp"):
            fields["stale_timestamp"] = isoparse(host["stale_timestamp"])
        if host.get("stale_warning_timestamp"):
            fields["stale_warning_timestamp"] = isoparse(host["stale_warning_timestamp"])
        if host.get("culled_timestamp"):
            fields["culled_timestamp"] = isoparse(host["culled_timestamp"])
        return fields

    def _parse_org_id(self, item: QueueItem) -> (str, str):
        """Extract org id or account id from queue item"""
        if item.inventory_msg is not None and item.inventory_msg_type is InventoryMsgType.UPLOAD:
            return item.inventory_msg["host"]["org_id"], item.inventory_msg["host"].get("account")
        if item.inventory_msg is not None and item.inventory_msg_type is InventoryMsgType.DELETE:
            return item.inventory_msg["org_id"], item.inventory_msg.get("account")
        return item.advisor_msg["input"]["host"]["org_id"], item.advisor_msg["input"]["host"].get("account")

    def _send_for_evaluation(self, org_id: str, acc_num: str, inventory_id: str, item: QueueItem):
        """Send message to evaluator to evaluate"""
        msg = {
            "type": "upload",
            "host": {
                "inventory_id": inventory_id,
                "org_id": org_id,
                "account": acc_num,
            },
        }
        if item.advisor_msg is not None:
            msg["timestamp"] = item.advisor_msg["input"]["timestamp"]
        if item.inventory_msg is not None and item.inventory_msg_type is not InventoryMsgType.UNKNOWN:
            msg["timestamp"] = item.inventory_msg["timestamp"]
        # TODO: send to evaluator

    async def _process_upload(self, item: QueueItem):
        """Process upload message defined by QueueItem"""
        repo_list = []
        insert_fields = {}

        import_status = ImportStatus.FAILED

        org_id, acc_num = self._parse_org_id(item)
        if item.advisor_msg is not None:
            insert_fields = self._parse_advisor_db_fields(insert_fields, item)
        if item.inventory_msg is not None and item.inventory_msg_type is not InventoryMsgType.UNKNOWN:
            vmaas_json, repo_list = self._parse_inventory_system_data(item)
            if not vmaas_json and not item.advisor_msg:
                LOGGER.info("Skipping upload for system: %s, org_id: %s, due to empty repo list and non-recieved advisor msg",
                            item.inventory_msg["host"]["id"], org_id)
                UPLOAD_NO_RPMDB.inc()
                return

            if vmaas_json:
                insert_fields = self._parse_inventory_db_fields(insert_fields, item, vmaas_json)
            else:
                LOGGER.info("Skipping adding inventory data for system: %s, org_id: %s, due to empty repo list",
                            item.inventory_msg["host"]["id"], org_id)
                UPLOAD_NO_RPMDB.inc()

        async with self.db_pool.acquire() as conn:
            try:
                async with conn.transaction():
                    import_status, system_id = await self._db_import_system(conn, insert_fields, org_id, acc_num)
                    if repo_list:
                        await self._db_import_repos(conn, repo_list)
                        await self._db_import_system_repos(conn, repo_list, system_id)
                        await self._db_unlink_system_repos(conn, repo_list, system_id)
                    import_status -= ImportStatus.FAILED
            except Exception as exc:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: %s", exc)

        if ImportStatus.CHANGED in import_status or CFG.disable_optimisation:
            self._send_for_evaluation(org_id, acc_num, insert_fields["inventory_id"], item)

    async def _process_inventory_delete(self, item: QueueItem):
        """Process inventory delete message"""
        org_id = item.inventory_msg["org_id"]
        acc_num = item.inventory_msg.get("account")
        async with self.db_pool.acquire() as conn:
            try:
                async with conn.transaction():
                    acc_id = await self._db_account_lookup(conn, acc_num, org_id)
                    res = await conn.fetchrow("""
                        INSERT INTO system_platform
                        (inventory_id, rh_account_id, opt_out, stale, when_deleted)
                        VALUES ($1, $2, true, true, now())
                        ON CONFLICT (inventory_id) DO UPDATE SET
                        opt_out = EXCLUDED.opt_out, stale = EXCLUDED.stale, when_deleted = EXCLUDED.when_deleted
                        RETURNING (xmax = 0) AS inserted
                    """, item.inventory_msg["id"], acc_id)
            except Exception as exc:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: %s", exc)
            if not res["inserted"]:
                LOGGER.info("Deleted system: %s for org_id: %s", item.inventory_msg["id"], org_id)
            else:
                LOGGER.info("Attempted to delete non-existing system: %s with org_id: %s", item.inventory_msg["id"], org_id)

    async def process_item(self, item: QueueItem):
        """Process single queue item composed of inventory and advisor msg"""
        if item.inventory_msg_type is InventoryMsgType.DELETE:
            await self._process_inventory_delete(item)
        await self._process_upload(item)
