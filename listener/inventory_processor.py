"""
Inventory message processor
"""
import hashlib
import json
from urllib.parse import urlparse

from psycopg.rows import dict_row

from common.logging import get_logger
from common.utils import send_msg_to_payload_tracker
from common.mqueue import MQWriter, Partitioners
from .common import (CFG, REPO_PATH_PATTERN, REPO_BASEARCH_PLACEHOLDER,
                     REPO_RELEASEVER_PLACEHOLDER, RHUI_PATH_PART, UPLOAD_NO_RPMDB,
                     NEW_REPO, NEW_SYSTEM_REPO, DELETED_SYSTEM_REPO, DELETED_UPLOADED,
                     DATABASE_ERROR, UPDATE_SYSTEM, NEW_SYSTEM,
                     ImportStatus, InventoryMsgType, InventoryMsg, executemany_fetchall)
from .processor import BaseProcessor

LOGGER = get_logger(__name__)


class InventoryMsgProcessor(BaseProcessor):
    # pylint: disable=no-self-use,broad-except
    """Inventory msg processor, processes messages from inventory"""

    def __init__(self, *args, **kwargs):
        """Constructor"""
        super().__init__(*args, **kwargs)
        self.evaluator = MQWriter(CFG.evaluator_inventory_topic, partitioner=Partitioners.org_id_partitioner)
        self.repo_id_cache = {}

    async def init(self):
        """Async constructor"""
        await self._load_repo_cache()

    async def start(self):
        """Start inventory processor"""
        await self.evaluator.start()

    async def stop(self):
        """Stop inventory processor"""
        await self.evaluator.stop()

    async def _load_repo_cache(self):
        """Load the repository cache"""
        LOGGER.info("Populating repo cache")
        async with self.db_pool.connection() as conn:
            async with conn.cursor(row_factory=dict_row) as cur:
                try:
                    await cur.execute("""SELECT id, name FROM repo""")
                    for repo in await cur.fetchall():
                        self.repo_id_cache[repo["name"]] = repo["id"]
                except Exception as exc:
                    LOGGER.exception("Cannot cache repos: %s", exc)

    def _format_repo_placeholders(self, path, basearch=None, releasever=None) -> str:
        """Format repository path by replacing placeholders"""
        repo_path = path
        if basearch:
            repo_path = repo_path.replace(REPO_BASEARCH_PLACEHOLDER, basearch)
        if releasever:
            repo_path = repo_path.replace(REPO_RELEASEVER_PLACEHOLDER, releasever)
        return repo_path

    def _format_repo_path(self, repo_url, basearch=None, releasever=None) -> str:
        """Format known repository path out of base URL or mirrolist (RHUI only)"""
        if not repo_url or not repo_url.strip():
            return None

        try:
            parsed = urlparse(repo_url)
        except ValueError as err:
            LOGGER.warning("Base URL/mirrorlist parse error: %s (value: %s)", err, repo_url)
            return None

        path = parsed.path
        path_match = REPO_PATH_PATTERN.search(path)
        if RHUI_PATH_PART in path and path_match:
            return self._format_repo_placeholders(path_match.group(1),
                                                  basearch=basearch, releasever=releasever)
        return None

    def _format_repo_list(self, yum_repos=(), basearch=None, releasever=None) -> (list, list):
        """Get repo list and repo paths out of yum repos using basearch and releasever"""
        repo_list = []
        repo_paths = set()
        for repo in yum_repos:
            if repo.get("enabled", True) and repo.get("id", "").strip():
                repo_list.append(repo["id"].strip())
                repo_url = repo.get("mirrorlist") or repo.get("base_url")
                repo_path = self._format_repo_path(repo_url, basearch=basearch, releasever=releasever)
                if repo_path:
                    repo_paths.add(repo_path)
        return repo_list, list(repo_paths)

    def _format_vmaas_req(self, package_list: [], repo_list: [], basearch: str, modules_list=None, releasever=None, repo_paths=None) -> str:
        """Wrap package and repo list into the vmaas request format"""
        vmaas_request = {"package_list": package_list,
                         "repository_list": repo_list,
                         "repository_paths": repo_paths,
                         "extended": True}
        if modules_list:
            vmaas_request["modules_list"] = modules_list
        if basearch:
            vmaas_request["basearch"] = basearch
        if releasever:
            vmaas_request["releasever"] = releasever
        return json.dumps(vmaas_request)

    def _parse_system_data(self, msg: InventoryMsg) -> (str, list):
        """Parse needed system data from inventory message"""
        vmaas_request = None
        repo_list = []
        system_profile = msg.msg["host"]["system_profile"]
        installed_packages = system_profile.get("installed_packages")
        if installed_packages:
            basearch = system_profile.get("basearch") or system_profile.get("arch")
            rhsm_ver = system_profile.get("rhsm", {}).get("version")
            releasever = system_profile.get("releasever")

            repo_list, repo_paths = self._format_repo_list(system_profile.get("yum_repos", ()),
                                                           basearch=basearch,
                                                           releasever=(rhsm_ver or releasever))
            modules_list = [{"module_name": m["name"], "module_stream": m["stream"]} for m in system_profile.get("dnf_modules", [])]
            vmaas_request = self._format_vmaas_req(installed_packages, repo_list, basearch,
                                                   modules_list=modules_list,
                                                   releasever=rhsm_ver,
                                                   repo_paths=repo_paths)
        return vmaas_request, repo_list

    async def _db_import_repos(self, conn, repo_list: list) -> [str]:
        """Import not yet imported repos to db"""
        repo_names = []
        to_insert = sorted([(repo,) for repo in repo_list if repo not in self.repo_id_cache])
        if to_insert:
            async with conn.cursor(row_factory=dict_row) as cur:
                await cur.executemany("""
                    INSERT INTO repo (name)
                    VALUES (%s)
                    RETURNING id, name, (xmax = 0) AS inserted
                """, to_insert, returning=True)
                for row in await executemany_fetchall(cur):
                    if row["inserted"]:
                        NEW_REPO.inc()
                        repo_names.append(row["name"])
                    self.repo_id_cache[row["name"]] = row["id"]
        return repo_names

    async def _db_import_system_repos(self, conn, repo_list: list, system_id: int) -> [int]:
        """Link system with given repositories"""
        repo_ids = []
        if repo_list:
            to_insert = []
            for repo in repo_list:
                to_insert.append((system_id, self.repo_id_cache[repo]))

            async with conn.cursor(row_factory=dict_row) as cur:
                await cur.executemany("""
                    INSERT INTO system_repo (system_id, repo_id)
                    VALUES (%s, %s)
                    ON CONFLICT (system_id, repo_id)
                    DO NOTHING RETURNING repo_id
                """, to_insert, returning=True)
                for row in await executemany_fetchall(cur):
                    repo_ids.append(row["repo_id"])
                if repo_ids:
                    NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids

    async def _db_unlink_system_repos(self, conn, repo_list: list, system_id: int) -> [int]:
        """Unlink repos which are not linked to system anymore"""
        deleted_repo_ids = []
        async with conn.cursor(row_factory=dict_row) as cur:
            if repo_list:
                to_delete_repos_ids = [self.repo_id_cache[repo] for repo in repo_list]
                await cur.execute("""
                    DELETE FROM system_repo AS sr
                    WHERE sr.system_id = %s AND NOT(sr.repo_id = ANY(%s))
                    RETURNING sr.repo_id
                """, (system_id, to_delete_repos_ids,))
                for row in await cur.fetchall():
                    deleted_repo_ids.append(row["repo_id"])
            else:
                await cur.execute("""DELETE FROM system_repo WHERE system_id = %s RETURNING repo_id""")
                for row in await cur.fetchall:
                    deleted_repo_ids.append(row["repo_id"])
        if deleted_repo_ids:
            DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
        return deleted_repo_ids

    async def _db_import_system(self, conn, fields: dict, org_id: str, acc_num: str) -> (ImportStatus, int):
        """Import system to system_platform table, update if exists"""
        rh_account_id = await self._db_account_lookup(conn, acc_num, org_id)
        fields["rh_account_id"] = rh_account_id

        insert_query = """
            INSERT INTO system_platform
            (rh_account_id, inventory_id, s3_url, vmaas_json, json_checksum, display_name,
             stale_timestamp, stale_warning_timestamp, culled_timestamp,
             host_type, last_upload, stale)
            VALUES (%(rh_account_id)s, %(inventory_id)s, %(s3_url)s, %(vmaas_json)s, %(json_checksum)s, %(display_name)s,
                    %(stale_timestamp)s, %(stale_warning_timestamp)s, %(culled_timestamp)s,
                    %(host_type)s, CURRENT_TIMESTAMP, 'F')
            ON CONFLICT (inventory_id) DO UPDATE SET
            rh_account_id = EXCLUDED.rh_account_id, inventory_id = EXCLUDED.inventory_id, s3_url = EXCLUDED.s3_url, vmaas_json = EXCLUDED.vmaas_json,
            json_checksum = EXCLUDED.json_checksum, display_name = EXCLUDED.display_name, stale_timestamp = EXCLUDED.stale_timestamp,
            stale_warning_timestamp = EXCLUDED.stale_warning_timestamp, culled_timestamp = EXCLUDED.culled_timestamp,
            host_type = EXCLUDED.host_type, last_upload = EXCLUDED.last_upload, stale = EXCLUDED.stale
            RETURNING (xmax = 0) AS inserted, unchanged_since, last_evaluation, id, when_deleted
        """
        async with conn.cursor(row_factory=dict_row) as cur:
            await cur.execute(insert_query, fields)
            result = await cur.fetchone()
            if result["when_deleted"]:
                LOGGER.warning("Received recently deleted inventory_id: %s", fields["inventory_id"])
                DELETED_UPLOADED.inc()
                return ImportStatus.FAILED, None
            status = None
            if result["inserted"]:
                status = ImportStatus.INSERTED | ImportStatus.CHANGED
                NEW_SYSTEM.inc()
            else:
                status = ImportStatus.UPDATED
                UPDATE_SYSTEM.inc()

            if not result["last_evaluation"] or (result["unchanged_since"] > result["last_evaluation"]):
                status |= ImportStatus.CHANGED

        return status, result["id"]

    def _parse_db_fields(self, msg: InventoryMsg, vmaas_json: dict):
        """Parse DB fields for system_platform table insertion query
           from inventory message"""
        fields = {}

        host = msg.msg["host"]

        fields["inventory_id"] = host["id"]
        fields["s3_url"] = msg.msg["platform_metadata"]["url"]
        fields["vmaas_json"] = vmaas_json
        fields["json_checksum"] = hashlib.sha256(vmaas_json.encode("utf-8")).hexdigest()

        fields["display_name"] = host.get("display_name")
        fields["stale_timestamp"] = host.get("stale_timestamp")
        fields["stale_warning_timestamp"] = host.get("stale_warning_timestamp")
        fields["culled_timestamp"] = host.get("culled_timestamp")
        fields["host_type"] = host.get("system_profile", {}).get("host_type")
        return fields

    def _parse_org_id(self, msg: InventoryMsg) -> (str, str, str):
        """Extract org id, account id and inventory_id from inventory message"""
        if msg.msg_type in [InventoryMsgType.CREATED, InventoryMsgType.UPDATED]:
            return msg.msg["host"]["org_id"], msg.msg["host"].get("account"), msg.msg["host"]["id"]
        return msg.msg["org_id"], msg.msg.get("account"), msg.msg["id"]

    def _send_for_evaluation(self, org_id: str, acc_num: str, inventory_id: str, msg: InventoryMsg, import_status: ImportStatus):
        """Send message to evaluator to evaluate"""
        request_id = msg.msg["platform_metadata"].get("request_id")
        timestamp = msg.msg["timestamp"]

        msg = {
            "type": "inventory_upload",
            "host": {
                "id": inventory_id,
                "changed": ImportStatus.CHANGED in import_status,
                "account": acc_num,
                "org_id": org_id,
            },
            "platform_metadata": {
                "request_id": request_id,
            },
            "timestamp": timestamp
        }
        self.evaluator.send(msg, loop=self.loop, key=org_id)

    def _send_to_payload_tracker(self, status: str, msg: InventoryMsg, message=None):
        """Send payload tracker message"""
        send_msg_to_payload_tracker(self.payload_tracker, msg.msg, status, status_msg=message, loop=self.loop)

    async def _process_upload(self, msg: InventoryMsg):
        """Process upload message defined by QueueItem"""
        repo_list = []
        insert_fields = {}

        org_id, acc_num, inventory_id = self._parse_org_id(msg)

        vmaas_json, repo_list = self._parse_system_data(msg)
        if not vmaas_json:
            LOGGER.info("Skipping adding inventory data for system: %s, org_id: %s, due to empty repo list",
                        inventory_id, org_id)
            UPLOAD_NO_RPMDB.inc()
            return

        insert_fields = self._parse_db_fields(msg, vmaas_json)

        async with self.db_pool.connection() as conn:
            try:
                async with conn.transaction():
                    import_status, system_id = await self._db_import_system(conn, insert_fields, org_id, acc_num)
                    if ImportStatus.FAILED in import_status:
                        return
                    if repo_list:
                        await self._db_import_repos(conn, repo_list)
                        await self._db_import_system_repos(conn, repo_list, system_id)
                        await self._db_unlink_system_repos(conn, repo_list, system_id)
            except Exception as exc:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: %s", exc)
                return

        LOGGER.info("inventory data inserted, system: %s, org_id: %s", inventory_id, org_id)
        self._send_for_evaluation(org_id, acc_num, inventory_id, msg, import_status)
        self._send_to_payload_tracker("processing_success", msg, message="system successfully uploaded, sending for evaluation")

    async def _process_delete(self, msg: InventoryMsg):
        """Process inventory delete message"""
        org_id, acc_num, inventory_id = self._parse_org_id(msg)

        async with self.db_pool.connection() as conn:
            try:
                async with conn.transaction():
                    async with conn.cursor(row_factory=dict_row) as cur:
                        acc_id = await self._db_account_lookup(conn, acc_num, org_id)
                        await cur.execute("""
                            INSERT INTO system_platform
                            (inventory_id, rh_account_id, opt_out, stale, when_deleted)
                            VALUES (%s, %s, true, true, now())
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            opt_out = EXCLUDED.opt_out, stale = EXCLUDED.stale, when_deleted = EXCLUDED.when_deleted
                            RETURNING (xmax = 0) AS inserted
                        """, (inventory_id, acc_id,))
                        res = await cur.fetchone()
                        if not res["inserted"]:
                            LOGGER.info("Deleted system: %s for org_id: %s", inventory_id, org_id)
                        else:
                            LOGGER.info("Attempted to delete non-existing system: %s with org_id: %s", inventory_id, org_id)
            except Exception as exc:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: %s", exc)

    async def _process_update(self, msg: InventoryMsg):
        """Process display_name update, when the platform_metadata is empty"""
        org_id, acc_num, inventory_id = self._parse_org_id(msg)
        display_name = msg.msg["host"]["display_name"]

        async with self.db_pool.connection() as conn:
            try:
                async with conn.transaction():
                    async with conn.cursor(row_factory=dict_row) as cur:
                        rh_account_id = await self._db_account_lookup(conn, acc_num, org_id)
                        await cur.execute("""
                            UPDATE system_platform
                            SET display_name = %s
                            WHERE inventory_id = %s
                            AND rh_account_id = %s
                            RETURNING id AS updated
                        """, (display_name, inventory_id, rh_account_id,))
                        res = await cur.fetchone()
                        if res and "updated" in res:
                            LOGGER.info("Updated system: %s for org_id: %s", inventory_id, org_id)
                        else:
                            LOGGER.info("Attempted to update non-existing system: %s for org_id: %s", inventory_id, org_id)
            except Exception as exc:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error updating system: %s", exc)

    async def process_msg(self, msg: InventoryMsg):
        """Process single queue item composed of inventory and advisor msg"""
        if msg.msg_type is InventoryMsgType.DELETE:
            await self._process_delete(msg)
        elif (msg.msg_type in [InventoryMsgType.CREATED, InventoryMsgType.UPDATED] and
              not msg.msg.get("platform_metadata")):
            await self._process_update(msg)
        else:
            await self._process_upload(msg)
