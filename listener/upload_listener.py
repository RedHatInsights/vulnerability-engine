"""Module implementing kafka listener."""

import asyncio
from datetime import datetime, timedelta
import hashlib
import json
import os
import signal
import tempfile
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import flags

from psycopg2 import DatabaseError
from psycopg2.extras import execute_values
from prometheus_client import Counter, start_http_server
import requests
import pytz

from common import mqueue
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.logging import init_logging, get_logger
from .archive_parser import ArchiveParser

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# How many times are we willing to try to grab an uploaded archive before moving on?
MAX_ARCHIVE_RETRIES = int(os.getenv('MAX_ARCHIVE_RETRIES', '2'))
# number of worker threads
WORKER_THREADS = int(os.getenv('WORKER_THREADS', '30'))
MAX_QUEUE_SIZE = int(os.getenv('MAX_QUEUE_SIZE', '30'))
SYSTEM_DELETION_THRESHOLD = int(os.getenv('SYSTEM_DELETION_THRESHOLD', '24'))  # 24 hours

DISABLE_OPTIMISATION = strtobool(os.getenv('DISABLE_OPTIMISATION', 'False'))
# prometheus metrics
NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
UNCHANGED_SYSTEM = Counter('ve_listener_upl_unchanged_system', '# of system-updates with same vmaas info')
DELETED_SYSTEM = Counter('ve_listener_deleted_system', '# of systems deleted')
DELETED_SYSTEM_NOT_FOUND = Counter('ve_listener_deleted_system_nf', '# of systems to delete but not found')
DELETED_UPLOADED = Counter('ve_listener_deleted_uploaded', '# of systems uploaded after being deleted')
PROCESS_MESSAGES = Counter('ve_listener_messages_processed', '# of messages processed')
UNKNOWN_EVENT_TYPE = Counter('ve_listener_unknown_event_type', '# of msgs with unknown event type')
UNKNOWN_TOPIC = Counter('ve_listener_unknown_topic', '# of msgs received on unsupported topic')
DATABASE_ERROR = Counter('ve_listener_database_error', '# of database errors')
MISSING_ID = Counter('ve_listener_upl_missing_inventory_id', '# of upload-msgs missing inventory_id')
ARCHIVE_PARSE_FAILURE = Counter('ve_listener_upl_archive_exceptions', '# of exceptions during archive-processing')
ARCHIVE_RETRIEVE_ATTEMPT = Counter('ve_listener_upl_archive_tgz_attempt', '# of times retried archive-retrieval')
ARCHIVE_RETRIEVE_FAILURE = Counter('ve_listener_upl_archive_tgz_failure', '# of times gave up on archive retrieval')
ARCHIVE_RETRIEVE_INVALID_HTTP = Counter('ve_listener_upl_archive_tgz_invalid_http',
                                        '# archive downloaded with invalid http code')
ARCHIVE_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')
MESSAGE_PARSE_ERROR = Counter('ve_listener_message_parse_error', '# of message parse errors')
NEW_REPO = Counter('ve_listener_upl_new_repo', '# of new repos inserted')
NEW_SYSTEM_REPO = Counter('ve_listener_upl_new_system_repo', '# of new system_repo pairs inserted')
DELETED_REPO = Counter('ve_listener_upl_repo_deleted', '# deleted repos')
DELETED_SYSTEM_REPO = Counter('ve_listener_upl_system_repo_deleted', '# deleted system_repo pairs')

# kafka clients
LISTENER_QUEUE = mqueue.MQReader([mqueue.UPLOAD_TOPIC, mqueue.EVENTS_TOPIC])
EVALUATOR_QUEUE = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await LISTENER_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    loop.stop()


def on_thread_done(future):
    """Callback to call after ThreadPoolExecutor worker finishes."""
    try:
        future.result()
    except Exception:  # pylint: disable=broad-except
        LOGGER.exception("Future %s hit exception: ", future)


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(inventory_id: str, rh_account: str, s3_url: str, vmaas_json: str, satellite_managed: bool,
                     repo_list: list):
    """Import initial system record to the DB, report back on what we did."""

    status = ImportStatus.FAILED

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                import_status = db_import_system_platform(cur, inventory_id, rh_account, s3_url, vmaas_json,
                                                          satellite_managed)
                if import_status is None:
                    return status
                status |= import_status

                db_import_repos(cur, repo_list)
                db_import_system_repos(cur, repo_list, inventory_id)

                db_delete_other_system_repos(cur, repo_list, inventory_id)
                db_delete_unused_repos(cur)

                conn.commit()

                status -= ImportStatus.FAILED
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                conn.rollback()
            return status


def db_import_system_platform(cur, inventory_id: str, rh_account: str, s3_url: str, vmaas_json: str,
                              satellite_managed: bool):
    """Import system_platform item to db table."""

    curr_time = datetime.now(tz=pytz.utc)
    cur.execute("""select inventory_id from deleted_systems where inventory_id = %s and when_deleted > %s""",
                (inventory_id, curr_time - timedelta(hours=SYSTEM_DELETION_THRESHOLD, )))
    if cur.fetchone() is not None:
        LOGGER.warning('Received recently deleted inventory id: %s', inventory_id)
        DELETED_UPLOADED.inc()
        return None

    json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
    # xmax is PG system column used to find out if row was inserted or updated
    cur.execute("""INSERT INTO system_platform
                (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (inventory_id) DO UPDATE SET
                rh_account = %s, s3_url = %s, vmaas_json = %s, json_checksum = %s, satellite_managed = %s
                RETURNING (xmax = 0) AS inserted, unchanged_since""",
                (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,
                 rh_account, s3_url, vmaas_json, json_checksum, satellite_managed))
    inserted, unchanged_since = cur.fetchone()
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or (unchanged_since > curr_time):
        import_status |= ImportStatus.CHANGED
    return import_status


def db_import_system_repos(cur, repos: list, inventory_id: str):
    """Import items to system_repo table."""

    if repos:
        cur.execute("""select id from repo where name in %s""", (tuple(repos), ))
        to_insert = [(inventory_id, repo_id) for repo_id, *_ in cur.fetchall()]
        if to_insert:
            execute_values(cur, """insert into system_repo (inventory_id, repo_id) values %s
                                   on conflict (inventory_id, repo_id) do nothing returning repo_id""",
                           to_insert, page_size=len(to_insert))
        repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
        if repo_ids:
            NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids
    return None


def db_delete_other_system_repos(cur, repos: list, inventory_id: str):
    """Delete all system_repo items not including input repos."""

    if repos:
        cur.execute("""delete from system_repo sr
                    where sr.inventory_id = %s and sr.repo_id not in
                    (select repo.id from repo where repo.name in %s)
                    returning sr.repo_id""", (inventory_id, tuple(repos)))
    else:
        cur.execute("""delete from system_repo where inventory_id = %s returning repo_id""",
                    (inventory_id, ))
    deleted_repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
    if deleted_repo_ids:
        DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
    return deleted_repo_ids


def db_import_repos(cur, repos: list):
    """Ensure input repos to be in repo db table."""

    if repos:
        to_insert = [(repo, ) for repo in repos]
        execute_values(cur, """insert into repo (name) values %s on conflict (name) do nothing
                               returning name""", to_insert, page_size=len(to_insert))
        repo_names = [repo_name for repo_name, *_ in cur.fetchall()]
        if repo_names:
            NEW_REPO.inc(len(repo_names))
        return repo_names
    return None


def db_delete_unused_repos(cur) -> list:
    """Delete repos not related with any system."""

    cur.execute("""delete from repo
                   using repo ljr left join system_repo sr on sr.repo_id = ljr.id
                   where ljr.id = repo.id and sr.inventory_id is null
                   returning repo.name""")
    delete_repo_names = [repo_name for repo_name, *_ in cur.fetchall()]
    if delete_repo_names:
        DELETED_REPO.inc(len(delete_repo_names))
    return delete_repo_names


def db_delete_system(inventory_id):
    """Delete system with inventory ID."""

    rtrn = {'deleted': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                curr_time = datetime.now(tz=pytz.utc)
                cur.execute("""INSERT INTO deleted_systems
                            (inventory_id, when_deleted) VALUES (%s, %s)
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            when_deleted = EXCLUDED.when_deleted
                            """, (inventory_id, curr_time,))
                cur.execute("""DELETE FROM deleted_systems WHERE when_deleted < %s
                            """, (curr_time - timedelta(hours=SYSTEM_DELETION_THRESHOLD),))
                cur.execute("""SELECT rh_account, satellite_managed FROM system_platform
                            WHERE inventory_id = %s
                            FOR UPDATE
                            """, (inventory_id,))
                system_platform = cur.fetchone()
                if system_platform is not None:
                    # first opt-out system to run update count cache trigger
                    cur.execute("""UPDATE system_platform SET
                                    opt_out = true
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    cur.execute("""DELETE FROM system_vulnerabilities
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    cur.execute("""DELETE FROM system_repo
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    cur.execute("""DELETE FROM system_platform
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    rtrn['deleted'] = True
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: ")
                conn.rollback()
            return rtrn


def download_archive(url, tmp_file):
    """Download archive from url to tmp file."""
    success = False
    LOGGER.debug("Downloading %s to %s", url, tmp_file.name)
    # Grab the uploaded archive and write it locally so we can dig Important THings out of it
    # If the read-attempt fails, retry up to MAX_ARCHIVE_RETRIES times before logging the failure and moving on
    for tries in range(MAX_ARCHIVE_RETRIES):
        try:
            response = requests.get(url, stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            if response.status_code == 200:
                LOGGER.debug("Downloading %s finished", url)
                success = True
            else:
                ARCHIVE_RETRIEVE_INVALID_HTTP.inc()
                LOGGER.error("Invalid HTTP status while downloading %s: %s", url, response.status_code)
            break
        except Exception:  # pylint: disable=broad-except
            # on last try, log and fail this upload. Else, log try again
            if (tries+1) == MAX_ARCHIVE_RETRIES:
                ARCHIVE_RETRIEVE_FAILURE.inc()
                LOGGER.exception("Unable to retrieve archive: ")
            else:
                ARCHIVE_RETRIEVE_ATTEMPT.inc()
                LOGGER.exception("Unable to retrieve archive, retrying: ")

    return success


def parse_archive(upload_data):
    """Parse archive after it's downloaded."""
    vmaas_request, repo_list = None, None
    with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
        if download_archive(upload_data["url"], tmp_file):
            parser = ArchiveParser(tmp_file.name)
            try:
                parser.parse()
                if parser.package_list:
                    vmaas_request = format_vmaas_request(parser.package_list,
                                                         repo_list=parser.repo_list,
                                                         modules_list=parser.modules_list)
                    repo_list = parser.repo_list
                else:
                    ARCHIVE_NO_RPMDB.inc()
                    LOGGER.error("Unable to store system, empty package list.")
            except Exception:  # pylint: disable=broad-except
                ARCHIVE_PARSE_FAILURE.inc()
                LOGGER.exception("Unable to parse archive: ")
    return vmaas_request, repo_list


def process_upload(upload_data, loop=None):
    """Parse archive and store it, ASSUMING vmaas-json has changed."""
    vmaas_request, repo_list = parse_archive(upload_data)
    sent = False
    if vmaas_request:
        satellite_managed = upload_data.get("satellite_managed", False)
        # received satellite_managed value is None sometimes
        if not isinstance(satellite_managed, bool):
            satellite_managed = False
        import_status = db_import_system(upload_data["id"], upload_data["account"], upload_data["url"],
                                         vmaas_request, satellite_managed, repo_list)
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if ImportStatus.CHANGED in import_status or DISABLE_OPTIMISATION:
            new_upload_msg = {"type": "upload_new_file", "system_id": upload_data["id"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
        else:
            UNCHANGED_SYSTEM.inc()
    return sent


def process_delete(msg_dict, **_):
    """Delete system."""
    rtrn = db_delete_system(msg_dict["id"])
    if not rtrn["failed"]:
        if rtrn["deleted"]:
            DELETED_SYSTEM.inc()
            LOGGER.info("Deleted system with inventory_id: %s", msg_dict["id"])
        else:
            DELETED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to delete system, inventory_id not found: %s", msg_dict["id"])


def main():
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    LOGGER.info("Starting upload listener.")

    loop = asyncio.get_event_loop()
    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = BoundedExecutor(MAX_QUEUE_SIZE, max_workers=WORKER_THREADS)

    def process_message(msg):
        """Message processing logic"""
        PROCESS_MESSAGES.inc()
        LOGGER.info('Received message from topic %s: %s', msg.topic, msg.value)

        try:
            msg_dict = json.loads(msg.value.decode("utf8"))
        except json.decoder.JSONDecodeError:
            MESSAGE_PARSE_ERROR.inc()
            LOGGER.exception("Unable to parse message: ")
            return

        if msg.topic == mqueue.UPLOAD_TOPIC:
            process_func = process_upload
        elif msg.topic == mqueue.EVENTS_TOPIC:
            if msg_dict['type'] == 'delete':
                process_func = process_delete
            else:
                UNKNOWN_EVENT_TYPE.inc()
                LOGGER.error("Received unknown event type: %s", msg_dict['type'])
                return
        else:
            UNKNOWN_TOPIC.inc()
            LOGGER.error("Received message on unsupported topic: %s", msg.topic)
            return

        if 'id' not in msg_dict or msg_dict["id"] is None:
            MISSING_ID.inc()
            LOGGER.warning("Unable to process message, inventory ID is missing.")
            return

        future = executor.submit(process_func, msg_dict, loop=loop)
        future.add_done_callback(on_thread_done)

    with DatabasePool(WORKER_THREADS):
        LISTENER_QUEUE.listen(process_message)

        # wait until loop is stopped from terminate callback
        loop.run_forever()

        LOGGER.info("Shutting down.")
        executor.shutdown()


if __name__ == '__main__':
    main()
