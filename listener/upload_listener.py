"""Module implementing kafka listener."""

import hashlib
import json
import os
import tempfile

from psycopg2 import DatabaseError
from prometheus_client import Counter, start_http_server
import requests

from common import mqueue
from common.database_handler import DatabaseHandler, init_db
from common.logging import init_logging, get_logger
from .archive_parser import ArchiveParser

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# How many times are we willing to try to grab an uploaded archive before moving on?
MAX_ARCHIVE_RETRIES = int(os.getenv('MAX_ARCHIVE_RETRIES', '2'))

NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
PROCESS_UPLOAD = Counter('ve_listener_upl_uploads_processed', '# of uploaded archives processed')
MISSING_ID = Counter('ve_listener_upl_missing_inventory_id', '# of upload-msgs missing inventory_id')
ARCHIVE_PARSE_FAILURE = Counter('ve_listener_upl_archive_exceptions', '# of exceptions during archive-processing')
ARCHIVE_RETRIEVE_ATTEMPT = Counter('ve_listener_upl_archive_tgz_attempt', '# of times retried archive-retrieval')
ARCHIVE_RETRIEVE_FAILURE = Counter('ve_listener_upl_archive_tgz_failure', '# of times gave up on archive retrieval')
ARCHIVE_RETRIEVE_INVALID_HTTP = Counter('ve_listener_upl_archive_tgz_invalid_http',
                                        '# archive downloaded with invalid http code')
ARCHIVE_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(conn, inventory_id, rh_account, s3_url, vmaas_json, satellite_managed):
    """Import initial system record to the DB."""
    # TODO: json_checksum
    inserted = False
    updated = False
    json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
    with conn.cursor() as cur:
        try:
            # xmax is PG system column used to find out if row was inserted or updated
            cur.execute("""INSERT INTO system_platform
                        (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        ON CONFLICT (inventory_id) DO UPDATE SET
                        rh_account = %s, s3_url = %s, vmaas_json = %s, json_checksum = %s, satellite_managed = %s
                        RETURNING (xmax = 0) AS inserted
                        """,
                        (inventory_id, rh_account, s3_url, vmaas_json, '0', satellite_managed,
                         rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,))
            inserted, = cur.fetchone()
            conn.commit()
            if inserted:
                NEW_SYSTEM.inc()
            else:
                updated = True
                UPDATE_SYSTEM.inc()
        except DatabaseError:
            LOGGER.exception("Error importing system: ")
            conn.rollback()
        return (inserted, updated)


def download_archive(url, tmp_file, session):
    """Download archive from url to tmp file."""
    success = False
    LOGGER.debug("Downloading %s to %s", url, tmp_file.name)
    # Grab the uploaded archive and write it locally so we can dig Important THings out of it
    # If the read-attempt fails, retry up to MAX_ARCHIVE_RETRIES times before logging the failure and moving on
    for tries in range(MAX_ARCHIVE_RETRIES):
        try:
            response = session.get(url, stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            if response.status_code == 200:
                LOGGER.debug("Downloading %s finished", url)
                success = True
            else:
                ARCHIVE_RETRIEVE_INVALID_HTTP.inc()
                LOGGER.error("Invalid HTTP status while downloading %s: %s", url, response.status_code)
            break
        except Exception: # pylint: disable=broad-except
            # on last try, log and fail this upload. Else, log try again
            if (tries+1) == MAX_ARCHIVE_RETRIES:
                ARCHIVE_RETRIEVE_FAILURE.inc()
                LOGGER.exception("Unable to retrieve archive: ")
            else:
                ARCHIVE_RETRIEVE_ATTEMPT.inc()
                LOGGER.exception("Unable to retrieve archive, retrying: ")

    return success


def parse_archive(upload_data, session):
    """Parse archive after it's downloaded."""
    vmaas_request = None
    with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
        if download_archive(upload_data["url"], tmp_file, session):
            parser = ArchiveParser(tmp_file.name)
            try:
                parser.parse()
                if parser.package_list:
                    vmaas_request = format_vmaas_request(parser.package_list,
                                                         repo_list=parser.repo_list,
                                                         modules_list=parser.modules_list)
                else:
                    ARCHIVE_NO_RPMDB.inc()
                    LOGGER.error("Unable to store system, empty package list.")
            except Exception: # pylint: disable=broad-except
                ARCHIVE_PARSE_FAILURE.inc()
                LOGGER.exception("Unable to parse archive: ")
    return vmaas_request


def main():
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    init_db()
    LOGGER.info("Starting upload listener.")
    # get DB connection
    conn = DatabaseHandler.get_connection()
    upload_queue = mqueue.MQReader(mqueue.UPLOAD_TOPIC)
    vuln_queue = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)

    session = requests.Session()

    def process_upload(msg):
        """Message processing logic"""
        PROCESS_UPLOAD.inc()
        LOGGER.info('Received message from topic %s: %s', msg.topic, msg.value)

        upload_data = json.loads(msg.value.decode("utf8"))

        # Inventory ID is missing
        if 'id' not in upload_data or upload_data["id"] is None:
            MISSING_ID.inc()
            LOGGER.warning("Unable to store system, inventory ID is missing.")
            return

        # Download archive an parse
        vmaas_request = parse_archive(upload_data, session)
        if vmaas_request:
            db_import_system(conn, upload_data["id"], upload_data["rh_account"], upload_data["url"],
                             vmaas_request, upload_data.get("satellite_managed", False))
            new_upload_msg = {"type": "upload_new_file", "system_id": upload_data["id"]}
            vuln_queue.send(new_upload_msg)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))

    upload_queue.listen(process_upload)
    session.close()

if __name__ == '__main__':
    main()
