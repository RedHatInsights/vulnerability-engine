"""Module implementing kafka listener."""

import asyncio
from datetime import datetime
import hashlib
import json
import os
import signal
import tempfile

from psycopg2 import DatabaseError
from prometheus_client import Counter, start_http_server
import requests
import pytz

from common import mqueue
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.logging import init_logging, get_logger
from .archive_parser import ArchiveParser

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# How many times are we willing to try to grab an uploaded archive before moving on?
MAX_ARCHIVE_RETRIES = int(os.getenv('MAX_ARCHIVE_RETRIES', '2'))
# number of worker threads
WORKER_THREADS = int(os.getenv('WORKER_THREADS', '30'))
MAX_QUEUE_SIZE = int(os.getenv('MAX_QUEUE_SIZE', '30'))

# prometheus metrics
NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
UNCHANGED_SYSTEM = Counter('ve_listener_upl_unchanged_system', '# of system-updates with same vmaas info')
DELETED_SYSTEM = Counter('ve_listener_deleted_system', '# of systems deleted')
DELETED_SYSTEM_NOT_FOUND = Counter('ve_listener_deleted_system_nf', '# of systems to delete but not found')
PROCESS_MESSAGES = Counter('ve_listener_messages_processed', '# of messages processed')
UNKNOWN_EVENT_TYPE = Counter('ve_listener_unknown_event_type', '# of msgs with unknown event type')
UNKNOWN_TOPIC = Counter('ve_listener_unknown_topic', '# of msgs received on unsupported topic')
DATABASE_ERROR = Counter('ve_listener_database_error', '# of database errors')
MISSING_ID = Counter('ve_listener_upl_missing_inventory_id', '# of upload-msgs missing inventory_id')
ARCHIVE_PARSE_FAILURE = Counter('ve_listener_upl_archive_exceptions', '# of exceptions during archive-processing')
ARCHIVE_RETRIEVE_ATTEMPT = Counter('ve_listener_upl_archive_tgz_attempt', '# of times retried archive-retrieval')
ARCHIVE_RETRIEVE_FAILURE = Counter('ve_listener_upl_archive_tgz_failure', '# of times gave up on archive retrieval')
ARCHIVE_RETRIEVE_INVALID_HTTP = Counter('ve_listener_upl_archive_tgz_invalid_http',
                                        '# archive downloaded with invalid http code')
ARCHIVE_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')
MESSAGE_PARSE_ERROR = Counter('ve_listener_message_parse_error', '# of message parse errors')

# kafka clients
LISTENER_QUEUE = mqueue.MQReader([mqueue.UPLOAD_TOPIC, mqueue.EVENTS_TOPIC])
EVALUATOR_QUEUE = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await LISTENER_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    loop.stop()


def on_thread_done(future):
    """Callback to call after ThreadPoolExecutor worker finishes."""
    try:
        future.result()
    except Exception:  # pylint: disable=broad-except
        LOGGER.exception("Future %s hit exception: ", future)


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(inventory_id, rh_account, s3_url, vmaas_json, satellite_managed):
    """Import initial system record to the DB, report back on what we did."""

    rtrn = {'inserted': False, 'updated': False, 'changed': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                unchanged_since = None
                json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
                curr_time = datetime.now(tz=pytz.utc)
                # xmax is PG system column used to find out if row was inserted or updated
                cur.execute("""INSERT INTO system_platform
                            (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed)
                            VALUES (%s, %s, %s, %s, %s, %s)
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            rh_account = %s, s3_url = %s, vmaas_json = %s, json_checksum = %s, satellite_managed = %s
                            RETURNING (xmax = 0) AS inserted, unchanged_since
                            """,
                            (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,
                             rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,))
                rtrn['inserted'], unchanged_since = cur.fetchone()
                conn.commit()

                # If inserting, or if unchanged_since is newer-than 'now', we want to evaluate this upload
                rtrn['changed'] = rtrn['inserted'] or (unchanged_since > curr_time)

                if rtrn['inserted']:
                    NEW_SYSTEM.inc()
                else:
                    rtrn['updated'] = True
                    UPDATE_SYSTEM.inc()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                conn.rollback()
            return rtrn


def db_delete_system(inventory_id):
    """Delete system with inventory ID."""

    rtrn = {'deleted': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""SELECT rh_account, satellite_managed FROM system_platform
                            WHERE inventory_id = %s
                            FOR UPDATE
                            """, (inventory_id,))
                system_platform = cur.fetchone()
                if system_platform is not None:
                    # first opt-out system to run update count cache trigger
                    cur.execute("""UPDATE system_platform SET
                                    opt_out = true
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    cur.execute("""DELETE FROM system_vulnerabilities
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    cur.execute("""DELETE FROM system_platform
                                WHERE inventory_id = %s
                                """, (inventory_id,))
                    rtrn['deleted'] = True
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: ")
                conn.rollback()
            return rtrn


def download_archive(url, tmp_file):
    """Download archive from url to tmp file."""
    success = False
    LOGGER.debug("Downloading %s to %s", url, tmp_file.name)
    # Grab the uploaded archive and write it locally so we can dig Important THings out of it
    # If the read-attempt fails, retry up to MAX_ARCHIVE_RETRIES times before logging the failure and moving on
    for tries in range(MAX_ARCHIVE_RETRIES):
        try:
            response = requests.get(url, stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            if response.status_code == 200:
                LOGGER.debug("Downloading %s finished", url)
                success = True
            else:
                ARCHIVE_RETRIEVE_INVALID_HTTP.inc()
                LOGGER.error("Invalid HTTP status while downloading %s: %s", url, response.status_code)
            break
        except Exception:  # pylint: disable=broad-except
            # on last try, log and fail this upload. Else, log try again
            if (tries+1) == MAX_ARCHIVE_RETRIES:
                ARCHIVE_RETRIEVE_FAILURE.inc()
                LOGGER.exception("Unable to retrieve archive: ")
            else:
                ARCHIVE_RETRIEVE_ATTEMPT.inc()
                LOGGER.exception("Unable to retrieve archive, retrying: ")

    return success


def parse_archive(upload_data):
    """Parse archive after it's downloaded."""
    vmaas_request = None
    with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
        if download_archive(upload_data["url"], tmp_file):
            parser = ArchiveParser(tmp_file.name)
            try:
                parser.parse()
                if parser.package_list:
                    vmaas_request = format_vmaas_request(parser.package_list,
                                                         repo_list=parser.repo_list,
                                                         modules_list=parser.modules_list)
                else:
                    ARCHIVE_NO_RPMDB.inc()
                    LOGGER.error("Unable to store system, empty package list.")
            except Exception:  # pylint: disable=broad-except
                ARCHIVE_PARSE_FAILURE.inc()
                LOGGER.exception("Unable to parse archive: ")
    return vmaas_request


def process_upload(upload_data, loop=None):
    """Parse archive and store it, ASSUMING vmaas-json has changed."""
    vmaas_request = parse_archive(upload_data)
    sent = False
    if vmaas_request:
        satellite_managed = upload_data.get("satellite_managed", False)
        # received satellite_managed value is None sometimes
        if not isinstance(satellite_managed, bool):
            satellite_managed = False
        rtrn = db_import_system(upload_data["id"], upload_data["account"], upload_data["url"],
                                vmaas_request, satellite_managed)
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if rtrn['changed']:
            new_upload_msg = {"type": "upload_new_file", "system_id": upload_data["id"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
        else:
            UNCHANGED_SYSTEM.inc()
    return sent


def process_delete(msg_dict, **_):
    """Delete system."""
    rtrn = db_delete_system(msg_dict["id"])
    if not rtrn["failed"]:
        if rtrn["deleted"]:
            DELETED_SYSTEM.inc()
            LOGGER.info("Deleted system with inventory_id: %s", msg_dict["id"])
        else:
            DELETED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to delete system, inventory_id not found: %s", msg_dict["id"])


def main():
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    LOGGER.info("Starting upload listener.")

    loop = asyncio.get_event_loop()
    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = BoundedExecutor(MAX_QUEUE_SIZE, max_workers=WORKER_THREADS)

    def process_message(msg):
        """Message processing logic"""
        PROCESS_MESSAGES.inc()
        LOGGER.info('Received message from topic %s: %s', msg.topic, msg.value)

        try:
            msg_dict = json.loads(msg.value.decode("utf8"))
        except json.decoder.JSONDecodeError:
            MESSAGE_PARSE_ERROR.inc()
            LOGGER.exception("Unable to parse message: ")
            return

        if msg.topic == mqueue.UPLOAD_TOPIC:
            process_func = process_upload
        elif msg.topic == mqueue.EVENTS_TOPIC:
            if msg_dict['type'] == 'delete':
                process_func = process_delete
            else:
                UNKNOWN_EVENT_TYPE.inc()
                LOGGER.error("Received unknown event type: %s", msg_dict['type'])
                return
        else:
            UNKNOWN_TOPIC.inc()
            LOGGER.error("Received message on unsupported topic: %s", msg.topic)
            return

        if 'id' not in msg_dict or msg_dict["id"] is None:
            MISSING_ID.inc()
            LOGGER.warning("Unable to process message, inventory ID is missing.")
            return

        future = executor.submit(process_func, msg_dict, loop=loop)
        future.add_done_callback(on_thread_done)

    with DatabasePool(WORKER_THREADS):
        LISTENER_QUEUE.listen(process_message)

        # wait until loop is stopped from terminate callback
        loop.run_forever()

        LOGGER.info("Shutting down.")
        executor.shutdown()


if __name__ == '__main__':
    main()
