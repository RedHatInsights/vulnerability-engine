"""Module implementing kafka listener."""

import asyncio
import hashlib
import json
import os
import signal
from typing import Optional
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import flags

from psycopg2 import DatabaseError
from psycopg2.extras import execute_values
from prometheus_client import Counter, start_http_server

from common import mqueue
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.identity import get_identity, is_entitled_insights
from common.logging import init_logging, get_logger
from common.failed_cache import FailedCache
from common.utils import on_thread_done, send_msg_to_payload_tracker, ensure_minimal_schema_version

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# number of worker threads
WORKER_THREADS = int(os.getenv('WORKER_THREADS', '30'))
MAX_QUEUE_SIZE = int(os.getenv('MAX_QUEUE_SIZE', '30'))
DISABLE_OPTIMISATION = strtobool(os.getenv('DISABLE_OPTIMISATION', 'False'))

# prometheus metrics
NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
UNCHANGED_SYSTEM = Counter('ve_listener_upl_unchanged_system', '# of system-updates with same vmaas info')
DELETED_SYSTEM = Counter('ve_listener_deleted_system', '# of systems deleted')
DELETED_SYSTEM_NOT_FOUND = Counter('ve_listener_deleted_system_nf', '# of systems to delete but not found')
UPDATED_SYSTEM = Counter('ve_listener_updated_system', '# of systems updated')
UPDATED_SYSTEM_NOT_FOUND = Counter('ve_listener_updated_system_nf', '# of systems to update but not found')
DELETED_UPLOADED = Counter('ve_listener_deleted_uploaded', '# of systems uploaded after being deleted')
PROCESS_MESSAGES = Counter('ve_listener_messages_processed', '# of messages processed')
SKIPPED_MESSAGES = Counter('ve_listener_messages_skipped', '# of messages skipped')
UNKNOWN_EVENT_TYPE = Counter('ve_listener_unknown_event_type', '# of msgs with unknown event type')
UNKNOWN_TOPIC = Counter('ve_listener_unknown_topic', '# of msgs received on unsupported topic')
DATABASE_ERROR = Counter('ve_listener_database_error', '# of database errors')
UPLOAD_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')
MESSAGE_PARSE_ERROR = Counter('ve_listener_message_parse_error', '# of message parse errors')
NEW_REPO = Counter('ve_listener_upl_new_repo', '# of new repos inserted')
NEW_RH_ACCOUNT = Counter('ve_listener_upl_new_rh_account', '# of new rh accounts inserted')
NEW_SYSTEM_REPO = Counter('ve_listener_upl_new_system_repo', '# of new system_repo pairs inserted')
DELETED_SYSTEM_REPO = Counter('ve_listener_upl_system_repo_deleted', '# deleted system_repo pairs')
INVALID_IDENTITY = Counter('ve_listener_upl_invalid_identity',
                           '# of skipped uploads because of invalid identity')
MISSING_INSIGHTS_ENTITLEMENT = Counter('ve_listener_upl_non_insights_entitlement',
                                       '# of skipped uploads because of entitlement check')

# kafka clients
LISTENER_QUEUE = mqueue.MQReader([mqueue.EVENTS_TOPIC])
EVALUATOR_QUEUE = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(mqueue.PAYLOAD_TRACKER_TOPIC)

# caching repo names to id
REPO_ID_CACHE = {}

REQUIRED_CREATED_UPDATED_MESSAGE_FIELDS = {
    "host": ["id", "account", "system_profile", "display_name"],
    "timestamp": [],
    "type": []
}
REQUIRED_UPLOAD_MESSAGE_FIELDS = {
    "platform_metadata": ["b64_identity", "url"]
}
REQUIRED_DELETE_MESSAGE_FIELDS = {
    "id": [],
    "account": [],
    "type": []
}


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await LISTENER_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    await PAYLOAD_TRACKER_PRODUCER.stop()
    loop.stop()


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(upload_data, vmaas_json: str, repo_list: list):
    """Import initial system record to the DB, report back on what we did."""

    status = ImportStatus.FAILED

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                host = upload_data["host"]
                import_status, system_id = db_import_system_platform(cur, host['id'], host['account'], upload_data['platform_metadata']['url'],
                                                                     host.get("display_name"), host.get('stale_timestamp'),
                                                                     host.get('stale_warning_timestamp'), host.get('culled_timestamp'), vmaas_json)
                if import_status is None:
                    conn.rollback()
                    return status
                status |= import_status

                db_import_repos(cur, repo_list)
                db_import_system_repos(cur, repo_list, system_id)

                db_delete_other_system_repos(cur, repo_list, system_id)

                conn.commit()

                status -= ImportStatus.FAILED
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                FailedCache.push(FailedCache.upload_cache, upload_data)
                LOGGER.info("Remembered upload %s", str(upload_data))
                conn.rollback()
    return status


def db_account_lookup(cur, account_name):
    """Make sure account is in DB and return row id."""
    cur.execute("""INSERT INTO rh_account (name) VALUES(%s) ON CONFLICT (name) DO NOTHING
                RETURNING (xmax = 0) AS inserted""", (account_name,))
    inserted = cur.fetchone()
    if inserted:
        NEW_RH_ACCOUNT.inc()

    cur.execute("""SELECT id FROM rh_account WHERE name = %s""", (account_name,))
    account_id = cur.fetchone()[0]
    return account_id


def db_import_system_platform(cur, inventory_id: str, rh_account: str, s3_url: str, display_name: str,
                              stale_timestamp: str, stale_warning_timestamp: str, culled_timestamp: str, vmaas_json: str):
    """Import system_platform item to db table."""
    rh_account_id = db_account_lookup(cur, rh_account)

    json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
    # xmax is PG system column used to find out if row was inserted or updated
    cur.execute("""INSERT INTO system_platform
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, last_upload,
                stale_timestamp, stale_warning_timestamp, culled_timestamp, stale)
                VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, %s, %s, %s, 'F')
                ON CONFLICT (inventory_id) DO UPDATE SET
                s3_url = %s, display_name = %s, vmaas_json = %s, json_checksum = %s, last_upload = CURRENT_TIMESTAMP,
                stale_timestamp = %s, stale_warning_timestamp = %s, culled_timestamp = %s, stale = 'F'
                RETURNING (xmax = 0) AS inserted, unchanged_since, last_evaluation, id, when_deleted""",
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp,
                 s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp))
    inserted, unchanged_since, last_evaluation, system_id, when_deleted = cur.fetchone()
    if when_deleted:
        LOGGER.warning('Received recently deleted inventory id: %s', inventory_id)
        DELETED_UPLOADED.inc()
        return None, None
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or not last_evaluation or (unchanged_since > last_evaluation):
        import_status |= ImportStatus.CHANGED
    return import_status, system_id


def db_import_system_repos(cur, repos: list, system_id: int):
    """Import items to system_repo table."""

    if repos:
        to_insert = [(system_id, REPO_ID_CACHE[repo]) for repo in repos]
        execute_values(cur, """insert into system_repo (system_id, repo_id) values %s
                               on conflict (system_id, repo_id) do nothing returning repo_id""",
                       to_insert, page_size=len(to_insert))
        repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
        if repo_ids:
            NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids
    return None


def db_delete_other_system_repos(cur, repos: list, system_id: int):
    """Delete all system_repo items not including input repos."""

    if repos:
        repos_ids = [REPO_ID_CACHE[repo] for repo in repos]
        cur.execute("""delete from system_repo sr
                    where sr.system_id = %s and sr.repo_id not in %s
                    returning sr.repo_id""", (system_id, tuple(repos_ids)))
    else:
        cur.execute("""delete from system_repo where system_id = %s returning repo_id""",
                    (system_id, ))
    deleted_repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
    if deleted_repo_ids:
        DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
    return deleted_repo_ids


def db_import_repos(cur, repos: list):
    """Ensure input repos to be in repo db table."""

    to_insert = sorted({(repo, ) for repo in repos if repo not in REPO_ID_CACHE})
    if to_insert:
        repo_names = []
        # make sure we do an update on conflict to fetch already existing ID imported by someone else concurrently
        execute_values(cur, """insert into repo (name) values %s on conflict (name) do update set name = EXCLUDED.name
                               returning id, name, (xmax = 0) AS inserted""", to_insert, page_size=len(to_insert))
        for repo_id, repo_name, inserted in cur.fetchall():
            if inserted:
                NEW_REPO.inc()
                repo_names.append(repo_name)
            REPO_ID_CACHE[repo_name] = repo_id
        return repo_names
    return None


def db_delete_system(msg_dict):
    """Delete system with inventory ID."""

    rtrn = {'deleted': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                rh_account_id = db_account_lookup(cur, msg_dict['account'])
                cur.execute("""INSERT INTO system_platform
                            (inventory_id, rh_account_id, opt_out, stale, when_deleted)
                            VALUES (%s, %s, true, true, now())
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            opt_out = EXCLUDED.opt_out, stale = EXCLUDED.stale, when_deleted = EXCLUDED.when_deleted
                            RETURNING (xmax = 0) AS inserted""",
                            (msg_dict['id'], rh_account_id,))

                inserted, = cur.fetchone()
                rtrn['deleted'] = not inserted
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: ")
                FailedCache.push(FailedCache.delete_cache, msg_dict)
                LOGGER.info("Remembered deleting %s", str(msg_dict))
                conn.rollback()
    return rtrn


def db_update_system(msg_dict):
    """Update system with inventory ID."""

    rtrn = {'updated': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""UPDATE system_platform
                            SET display_name = %s
                            WHERE inventory_id = %s
                            RETURNING id AS updated""",
                            (msg_dict['host']['display_name'], msg_dict['host']['id'],))
                updated = cur.fetchone()
                rtrn['updated'] = bool(updated)
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error updating system: ")
                conn.rollback()
    return rtrn


def db_init_repo_cache():
    """Populate initial repo cache"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""SELECT id, name FROM repo""")
                for repo_id, repo_name in cur.fetchall():
                    REPO_ID_CACHE[repo_name] = repo_id
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error caching repos: ")
                conn.rollback()


def parse_inventory_data(upload_data: dict) -> (Optional[str], list):
    """Parse inventory data from upload message."""
    vmaas_request = None
    repo_list = []
    system_profile = upload_data["host"]["system_profile"]
    installed_packages = system_profile.get("installed_packages")
    if installed_packages:
        repo_list = [r["id"].strip() for r in system_profile.get("yum_repos", []) if r.get("enabled", True) and r.get("id", "").strip()]
        modules_list = [{"module_name": m["name"], "module_stream": m["stream"]} for m in system_profile.get("dnf_modules", [])]
        vmaas_request = format_vmaas_request(installed_packages, repo_list=repo_list, modules_list=modules_list)
    else:
        UPLOAD_NO_RPMDB.inc()
        LOGGER.error("Skipping inventory_id because of empty package list: %s", upload_data["host"]["id"])
    return vmaas_request, repo_list


def process_upload(upload_data, loop=None):
    """Parse system upload msg and store it, ASSUMING vmaas-json has changed."""
    vmaas_request, repo_list = parse_inventory_data(upload_data)
    sent = False
    if vmaas_request:
        import_status = db_import_system(upload_data, vmaas_request, repo_list)
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if ImportStatus.CHANGED in import_status or DISABLE_OPTIMISATION:
            new_upload_msg = {
                "type": "upload_new_file",
                "host": {
                    "id": upload_data["host"]["id"],
                    "account": upload_data["host"]["account"]
                },
                "platform_metadata": {
                    "request_id": upload_data['platform_metadata'].get("request_id")
                },
                "timestamp": upload_data["timestamp"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
        elif ImportStatus.FAILED not in import_status:
            UNCHANGED_SYSTEM.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, upload_data, 'success',
                                        status_msg='unchanged system and not evaluated', loop=loop)
    return sent


def process_delete(msg_dict, **_):
    """Delete system."""
    rtrn = db_delete_system(msg_dict)
    if not rtrn["failed"]:
        if rtrn["deleted"]:
            DELETED_SYSTEM.inc()
            LOGGER.info("Deleted system with inventory_id: %s", msg_dict["id"])
        else:
            DELETED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to delete system, inventory_id not found: %s", msg_dict["id"])


def process_update(msg_dict, **_):
    """Update system."""
    rtrn = db_update_system(msg_dict)
    if not rtrn["failed"]:
        if rtrn["updated"]:
            UPDATED_SYSTEM.inc()
            LOGGER.info("Updated system with inventory_id: %s", msg_dict["host"]["id"])
        else:
            UPDATED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to update system, inventory_id not found: %s", msg_dict["host"]["id"])


def validate_msg(msg_dict, required_fields):
    """Check if all required fields are in msg_dict."""
    for key, required_inner_keys in required_fields.items():
        if key not in msg_dict:
            SKIPPED_MESSAGES.inc()
            LOGGER.warning("Missing key '%s' in msg_dict, skipping %s.", key, msg_dict["type"])
            return False
        inner_msg_dict = msg_dict[key]
        for inner_key in required_inner_keys:
            if inner_msg_dict is None or inner_key not in inner_msg_dict:
                SKIPPED_MESSAGES.inc()
                LOGGER.warning("Missing key '%s.%s' in msg_dict, skipping %s.", key, inner_key, msg_dict["type"])
                return False
    return True


def main():  # pylint: disable=too-many-statements
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    ensure_minimal_schema_version()
    LOGGER.info("Starting upload listener.")

    loop = asyncio.get_event_loop()
    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = BoundedExecutor(MAX_QUEUE_SIZE, max_workers=WORKER_THREADS)

    def process_message(msg):  # pylint: disable=too-many-return-statements,too-many-branches
        """Message processing logic"""
        PROCESS_MESSAGES.inc()
        LOGGER.debug('Received message from topic %s: %s', msg.topic, msg.value)

        try:
            msg_dict = json.loads(msg.value.decode("utf8"))
        except json.decoder.JSONDecodeError:
            MESSAGE_PARSE_ERROR.inc()
            LOGGER.exception("Unable to parse message: ")
            return
        FailedCache.process_failed_cache(FailedCache.upload_cache, executor, process_upload, loop)
        FailedCache.process_failed_cache(FailedCache.delete_cache, executor, process_delete, loop)

        if msg.topic == mqueue.EVENTS_TOPIC:
            if msg_dict.get("type", "") in ["created", "updated"]:
                if not validate_msg(msg_dict, REQUIRED_CREATED_UPDATED_MESSAGE_FIELDS):
                    return
                if msg_dict.get("platform_metadata"):
                    if not validate_msg(msg_dict, REQUIRED_UPLOAD_MESSAGE_FIELDS):
                        return
                    LOGGER.info("Received created/updated msg, inventory_id: %s, type: %s", msg_dict["host"]["id"], msg_dict["type"])
                    # send message to payload tracker
                    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'received', loop=loop)
                    # process only system uploads from insights entitled accounts
                    identity = get_identity(msg_dict["platform_metadata"]["b64_identity"])
                    if identity is None:
                        INVALID_IDENTITY.inc()
                        error_msg = "Skipped upload due to invalid identity header."
                        LOGGER.warning(error_msg)
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=loop)
                        return
                    if not is_entitled_insights(identity, allow_missing_section=True):
                        MISSING_INSIGHTS_ENTITLEMENT.inc()
                        error_msg = "Skipped upload due to missing insights entitlement."
                        LOGGER.debug(error_msg)
                        send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=loop)
                        return
                    process_func = process_upload
                else:
                    # display name change message doesn't have platform_metadata section, cannot validate identity and track payload,
                    # support only display name change
                    LOGGER.info("Received update event msg, inventory_id: %s", msg_dict["host"]["id"])
                    process_func = process_update
            elif msg_dict.get("type", "") == "delete":
                if not validate_msg(msg_dict, REQUIRED_DELETE_MESSAGE_FIELDS):
                    return
                LOGGER.info("Received delete msg, inventory_id: %s", msg_dict["id"])
                process_func = process_delete
            else:
                UNKNOWN_EVENT_TYPE.inc()
                LOGGER.error("Received unknown event type: %s", msg_dict['type'])
                return
        else:
            UNKNOWN_TOPIC.inc()
            LOGGER.error("Received message on unsupported topic: %s", msg.topic)
            return

        future = executor.submit(process_func, msg_dict, loop=loop)
        future.add_done_callback(on_thread_done)

    with DatabasePool(WORKER_THREADS):
        # prepare repo name to id cache
        db_init_repo_cache()
        LISTENER_QUEUE.listen(process_message)

        # wait until loop is stopped from terminate callback
        loop.run_forever()

        LOGGER.info("Shutting down.")
        executor.shutdown()


if __name__ == '__main__':
    main()
