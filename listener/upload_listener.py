"""Module implementing kafka listener."""

import re
import asyncio
import hashlib
import json
import signal
from urllib.parse import urlparse
from typing import Optional, Tuple
import flags

from psycopg2 import DatabaseError
from psycopg2.extras import execute_values

from common import mqueue
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.identity import get_identity, is_entitled_insights
from common.logging import init_logging, get_logger
from common.utils import on_thread_done, send_msg_to_payload_tracker, a_ensure_minimal_schema_version, validate_kafka_msg, validate_system_inventory
from common.status_app import create_status_app, create_status_runner
from .common import (CFG, PROMETHEUS_PORT, NEW_SYSTEM, UPDATE_SYSTEM, UNCHANGED_SYSTEM, DELETED_SYSTEM,
                     DELETED_SYSTEM_NOT_FOUND, UPDATED_SYSTEM, UPDATED_SYSTEM_NOT_FOUND, DELETED_UPLOADED,
                     PROCESS_MESSAGES, SKIPPED_MESSAGES, UNKNOWN_EVENT_TYPE, UNKNOWN_TOPIC, DATABASE_ERROR,
                     UPLOAD_NO_RPMDB, MESSAGE_PARSE_ERROR, NEW_REPO, NEW_RH_ACCOUNT, NEW_SYSTEM_REPO,
                     DELETED_SYSTEM_REPO, INVALID_IDENTITY, MISSING_INSIGHTS_ENTITLEMENT)

LOGGER = get_logger(__name__)

# kafka clients
LISTENER_QUEUE = mqueue.MQReader([CFG.events_topic])
EVALUATOR_QUEUE = mqueue.MQWriter(CFG.evaluator_upload_topic)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(CFG.payload_tracker_topic)

RHUI_PATH_PART = '/rhui/'
REPO_PATH_PATTERN = re.compile("(/content/.*)")
REPO_BASEARCH_PLACEHOLDER = "$basearch"
REPO_RELEASEVER_PLACEHOLDER = "$releasever"

WORKER_THREADS = CFG.worker_threads or 30

# caching repo names to id
REPO_ID_CACHE = {}

REQUIRED_CREATED_UPDATED_MESSAGE_FIELDS = {
    "host": ["id", "org_id", "system_profile", "display_name", ("insights_id", False)],
    "timestamp": [],
    "type": []
}
REQUIRED_UPLOAD_MESSAGE_FIELDS = {
    "platform_metadata": ["b64_identity", "url"]
}
REQUIRED_DELETE_MESSAGE_FIELDS = {
    "id": [],
    "org_id": [],
    "type": [],
    "insights_id": []
}


class ListenerCtx:
    """Class represents context of Upload Listener"""
    loop = None
    executor = None

    @staticmethod
    def set_listener_ctx():
        """Setup upload listener context"""
        ListenerCtx.loop = asyncio.get_event_loop()
        ListenerCtx.executor = BoundedExecutor(CFG.max_queue_size, max_workers=WORKER_THREADS)


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await LISTENER_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    await PAYLOAD_TRACKER_PRODUCER.stop()
    loop.stop()


def format_vmaas_request(package_list, repo_list, basearch, modules_list=None, releasever=None, repo_paths=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list,
                     "repository_list": repo_list,
                     "repository_paths": repo_paths,
                     "extended": True}
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    if basearch:
        vmaas_request["basearch"] = basearch
    if releasever:
        vmaas_request["releasever"] = releasever
    return json.dumps(vmaas_request)


def format_repo_path(repo_url, basearch=None, releasever=None):
    """Format known repository path out of base URL or mirrolist (RHUI only)"""
    if not repo_url or not repo_url.strip():
        return None

    try:
        parsed = urlparse(repo_url)
    except ValueError as err:
        LOGGER.warning("Base URL/mirrorlist parse error: %s (value: %s)", err, repo_url)
        return None

    path = parsed.path
    path_match = REPO_PATH_PATTERN.search(path)
    if RHUI_PATH_PART in path and path_match:
        return _format_repo_placeholders(path_match.group(1),
                                         basearch=basearch, releasever=releasever)

    return None


def _format_repo_placeholders(path, basearch=None, releasever=None):
    """Format repository path by replacing placeholders"""
    repo_path = path
    if basearch:
        repo_path = repo_path.replace(REPO_BASEARCH_PLACEHOLDER, basearch)
    if releasever:
        repo_path = repo_path.replace(REPO_RELEASEVER_PLACEHOLDER, releasever)

    return repo_path


def db_import_system(upload_data, vmaas_json: str, repo_list: list):
    """Import initial system record to the DB, report back on what we did."""

    status = ImportStatus.FAILED

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                host = upload_data["host"]
                import_status, system_id = db_import_system_platform(cur, host['id'], host.get('account'), host['org_id'],
                                                                     upload_data['platform_metadata']['url'],
                                                                     host.get("display_name"), host.get('stale_timestamp'),
                                                                     host.get('stale_warning_timestamp'), host.get('culled_timestamp'),
                                                                     host.get('system_profile', {}).get('host_type'), vmaas_json)
                if import_status is None:
                    conn.rollback()
                    return status
                status |= import_status

                db_import_repos(cur, repo_list)
                db_import_system_repos(cur, repo_list, system_id)

                db_delete_other_system_repos(cur, repo_list, system_id)

                conn.commit()

                status -= ImportStatus.FAILED
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                conn.rollback()
    return status


def db_account_lookup(cur, account_number, org_id):
    """Make sure account is in DB and return row id."""
    cur.execute("""SELECT id FROM rh_account WHERE org_id = %s""", (org_id,))
    row = cur.fetchone()
    if row:
        return row[0]

    if account_number:
        # Associate account_number with org_id if there are still any accounts with only account_number
        # TODO: delete later
        cur.execute("""INSERT INTO rh_account (account_number, org_id) VALUES(%s, %s) ON CONFLICT (account_number) DO UPDATE SET
                       org_id = %s RETURNING (xmax = 0) AS inserted, id""", (account_number, org_id, org_id,))
    else:
        # Set only org_id if there is no account_number provided
        cur.execute("""INSERT INTO rh_account (org_id) VALUES(%s) ON CONFLICT (org_id) DO UPDATE SET
                       org_id = %s RETURNING (xmax = 0) AS inserted, id""", (org_id, org_id,))
    inserted, account_id = cur.fetchone()
    if inserted:
        NEW_RH_ACCOUNT.inc()
    return account_id


def db_import_system_platform(cur, inventory_id: str, account_number: str, org_id: str, s3_url: str, display_name: str,
                              stale_timestamp: str, stale_warning_timestamp: str, culled_timestamp: str, host_type: str, vmaas_json: str):
    """Import system_platform item to db table."""
    rh_account_id = db_account_lookup(cur, account_number, org_id)

    json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
    # xmax is PG system column used to find out if row was inserted or updated
    cur.execute("""INSERT INTO system_platform
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, last_upload,
                stale_timestamp, stale_warning_timestamp, culled_timestamp, stale, host_type)
                VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, %s, %s, %s, 'F', %s)
                ON CONFLICT (inventory_id) DO UPDATE SET
                s3_url = %s, display_name = %s, vmaas_json = %s, json_checksum = %s, last_upload = CURRENT_TIMESTAMP,
                stale_timestamp = %s, stale_warning_timestamp = %s, culled_timestamp = %s, stale = 'F', host_type = %s
                RETURNING (xmax = 0) AS inserted, unchanged_since, last_evaluation, id, when_deleted""",
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp,
                 host_type, s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp, host_type))
    inserted, unchanged_since, last_evaluation, system_id, when_deleted = cur.fetchone()
    if when_deleted:
        LOGGER.warning('Received recently deleted inventory id: %s', inventory_id)
        DELETED_UPLOADED.inc()
        return None, None
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or not last_evaluation or (unchanged_since > last_evaluation):
        import_status |= ImportStatus.CHANGED
    return import_status, system_id


def db_import_system_repos(cur, repos: list, system_id: int):
    """Import items to system_repo table."""

    if repos:
        to_insert = [(system_id, REPO_ID_CACHE[repo]) for repo in repos]
        execute_values(cur, """insert into system_repo (system_id, repo_id) values %s
                               on conflict (system_id, repo_id) do nothing returning repo_id""",
                       to_insert, page_size=len(to_insert))
        repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
        if repo_ids:
            NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids
    return None


def db_delete_other_system_repos(cur, repos: list, system_id: int):
    """Delete all system_repo items not including input repos."""

    if repos:
        repos_ids = [REPO_ID_CACHE[repo] for repo in repos]
        cur.execute("""delete from system_repo sr
                    where sr.system_id = %s and sr.repo_id not in %s
                    returning sr.repo_id""", (system_id, tuple(repos_ids)))
    else:
        cur.execute("""delete from system_repo where system_id = %s returning repo_id""",
                    (system_id, ))
    deleted_repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
    if deleted_repo_ids:
        DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
    return deleted_repo_ids


def db_import_repos(cur, repos: list):
    """Ensure input repos to be in repo db table."""

    to_insert = sorted({(repo, ) for repo in repos if repo not in REPO_ID_CACHE})
    if to_insert:
        repo_names = []
        # make sure we do an update on conflict to fetch already existing ID imported by someone else concurrently
        execute_values(cur, """insert into repo (name) values %s on conflict (name) do update set name = EXCLUDED.name
                               returning id, name, (xmax = 0) AS inserted""", to_insert, page_size=len(to_insert))
        for repo_id, repo_name, inserted in cur.fetchall():
            if inserted:
                NEW_REPO.inc()
                repo_names.append(repo_name)
            REPO_ID_CACHE[repo_name] = repo_id
        return repo_names
    return None


def db_delete_system(msg_dict):
    """Delete system with inventory ID."""

    rtrn = {'deleted': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                rh_account_id = db_account_lookup(cur, msg_dict.get('account'), msg_dict['org_id'])
                cur.execute("""INSERT INTO system_platform
                            (inventory_id, rh_account_id, opt_out, stale, when_deleted)
                            VALUES (%s, %s, true, true, now())
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            opt_out = EXCLUDED.opt_out, stale = EXCLUDED.stale, when_deleted = EXCLUDED.when_deleted
                            RETURNING (xmax = 0) AS inserted""",
                            (msg_dict['id'], rh_account_id,))

                inserted, = cur.fetchone()
                rtrn['deleted'] = not inserted
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: ")
                conn.rollback()
    return rtrn


def db_update_system(msg_dict):
    """Update system with inventory ID."""

    rtrn = {'updated': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""UPDATE system_platform
                            SET display_name = %s
                            WHERE inventory_id = %s
                            RETURNING id AS updated""",
                            (msg_dict['host']['display_name'], msg_dict['host']['id'],))
                updated = cur.fetchone()
                rtrn['updated'] = bool(updated)
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error updating system: ")
                conn.rollback()
    return rtrn


def db_init_repo_cache():
    """Populate initial repo cache"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""SELECT id, name FROM repo""")
                for repo_id, repo_name in cur.fetchall():
                    REPO_ID_CACHE[repo_name] = repo_id
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error caching repos: ")
                conn.rollback()


def parse_inventory_data(upload_data: dict) -> Tuple[Optional[str], list]:
    """Parse inventory data from upload message."""
    vmaas_request = None
    repo_list = []
    system_profile = upload_data["host"]["system_profile"]
    installed_packages = system_profile.get("installed_packages")
    if installed_packages:
        basearch = system_profile.get("basearch", None) or system_profile.get("arch", None)
        rhsm_version = system_profile.get("rhsm", {}).get("version", None)
        releasever = system_profile.get("releasever", None)

        repo_list, repo_paths = _get_repo_list(system_profile.get("yum_repos", ()),
                                               basearch=basearch,
                                               releasever=(rhsm_version or releasever))
        modules_list = [{"module_name": m["name"], "module_stream": m["stream"]} for m in system_profile.get("dnf_modules", [])]

        vmaas_request = format_vmaas_request(installed_packages, repo_list, basearch, modules_list=modules_list,
                                             releasever=rhsm_version, repo_paths=repo_paths)
    else:
        UPLOAD_NO_RPMDB.inc()
        LOGGER.error("Skipping inventory_id because of empty package list: %s", upload_data["host"]["id"])
    return vmaas_request, repo_list


def _get_repo_list(yum_repos=(), basearch=None, releasever=None):
    """Get repo list and repo paths out of yum repos using basearch and releasever"""
    repo_list = []
    repo_paths = set()
    for repo in yum_repos:
        if repo.get("enabled", True) and repo.get("id", "").strip():
            repo_list.append(repo["id"].strip())
            repo_url = repo.get("mirrorlist") or repo.get("base_url")
            repo_path = format_repo_path(repo_url, basearch=basearch, releasever=releasever)
            if repo_path:
                repo_paths.add(repo_path)

    return repo_list, list(repo_paths)


def process_upload(upload_data, loop=None):
    """Parse system upload msg and store it, ASSUMING vmaas-json has changed."""
    sent = False
    if not validate_system_inventory(upload_data["host"]["id"], upload_data["timestamp"]):
        LOGGER.info("Skipping upload, due to system not in inventory anymore, inventory_id: %s", upload_data["host"]["id"])
        DELETED_UPLOADED.inc()
        return sent
    vmaas_request, repo_list = parse_inventory_data(upload_data)
    if vmaas_request:
        import_status = db_import_system(upload_data, vmaas_request, repo_list)
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if ImportStatus.CHANGED in import_status or CFG.disable_optimisation:
            new_upload_msg = {
                "type": "upload_new_file",
                "host": {
                    "id": upload_data["host"]["id"],
                    "account": upload_data["host"].get("account"),
                    "org_id": upload_data["host"]["org_id"]
                },
                "platform_metadata": {
                    "request_id": upload_data['platform_metadata'].get("request_id")
                },
                "timestamp": upload_data["timestamp"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', CFG.evaluator_upload_topic,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, upload_data, "processing_success",
                                        status_msg="system successfully uploaded, sending for evaluation", loop=loop)
        elif ImportStatus.FAILED not in import_status:
            UNCHANGED_SYSTEM.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, upload_data, 'success',
                                        status_msg='unchanged system and not evaluated', loop=loop)
    return sent


def process_delete(msg_dict, **_):
    """Delete system."""
    rtrn = db_delete_system(msg_dict)
    if not rtrn["failed"]:
        if rtrn["deleted"]:
            DELETED_SYSTEM.inc()
            LOGGER.info("Deleted system with inventory_id: %s", msg_dict["id"])
        else:
            DELETED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to delete system, inventory_id not found: %s", msg_dict["id"])


def process_update(msg_dict, **_):
    """Update system."""
    rtrn = db_update_system(msg_dict)
    if not rtrn["failed"]:
        if rtrn["updated"]:
            UPDATED_SYSTEM.inc()
            LOGGER.info("Updated system with inventory_id: %s", msg_dict["host"]["id"])
        else:
            UPDATED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to update system, inventory_id not found: %s", msg_dict["host"]["id"])


def process_message(msg):  # pylint: disable=too-many-return-statements,too-many-branches
    """Message processing logic"""
    PROCESS_MESSAGES.inc()
    LOGGER.debug('Received message from topic %s: %s', msg.topic, msg.value)

    try:
        msg_dict = json.loads(msg.value.decode("utf8"))
    except json.decoder.JSONDecodeError:
        MESSAGE_PARSE_ERROR.inc()
        LOGGER.exception("Unable to parse message: ")
        return

    if msg.topic == CFG.events_topic:
        if msg_dict.get("type", "") in ["created", "updated"]:
            if not validate_kafka_msg(msg_dict, REQUIRED_CREATED_UPDATED_MESSAGE_FIELDS):
                SKIPPED_MESSAGES.inc()
                return
            if msg_dict.get("platform_metadata"):
                if not validate_kafka_msg(msg_dict, REQUIRED_UPLOAD_MESSAGE_FIELDS):
                    SKIPPED_MESSAGES.inc()
                    return
                LOGGER.info("Received created/updated msg, inventory_id: %s, type: %s", msg_dict["host"]["id"], msg_dict["type"])
                # send message to payload tracker
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'received', loop=ListenerCtx.loop)
                # process only system uploads from insights entitled accounts
                identity = get_identity(msg_dict["platform_metadata"]["b64_identity"])
                if identity is None:
                    INVALID_IDENTITY.inc()
                    error_msg = "Skipped upload due to invalid identity header."
                    LOGGER.warning(error_msg)
                    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=ListenerCtx.loop)
                    return
                if not is_entitled_insights(identity, allow_missing_section=True):
                    MISSING_INSIGHTS_ENTITLEMENT.inc()
                    error_msg = "Skipped upload due to missing insights entitlement."
                    LOGGER.debug(error_msg)
                    send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=ListenerCtx.loop)
                    return
                process_func = process_upload
            else:
                # display name change message doesn't have platform_metadata section, cannot validate identity and track payload,
                # support only display name change
                LOGGER.info("Received update event msg, inventory_id: %s", msg_dict["host"]["id"])
                process_func = process_update
        elif msg_dict.get("type", "") == "delete":
            if not validate_kafka_msg(msg_dict, REQUIRED_DELETE_MESSAGE_FIELDS):
                SKIPPED_MESSAGES.inc()
                return
            LOGGER.info("Received delete msg, inventory_id: %s", msg_dict["id"])
            process_func = process_delete
        else:
            UNKNOWN_EVENT_TYPE.inc()
            LOGGER.error("Received unknown event type: %s", msg_dict.get('type', 'missing event type'))
            return
    else:
        UNKNOWN_TOPIC.inc()
        LOGGER.error("Received message on unsupported topic: %s", msg.topic)
        return

    future = ListenerCtx.executor.submit(process_func, msg_dict, loop=ListenerCtx.loop)
    future.add_done_callback(on_thread_done)


def main():  # pylint: disable=too-many-statements
    """Main kafka listener entrypoint."""
    init_logging()

    loop = asyncio.get_event_loop()
    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, loop)
    loop.run_until_complete(status_site.start())

    loop.run_until_complete(a_ensure_minimal_schema_version())
    LOGGER.info("Starting upload listener.")

    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))

    ListenerCtx.set_listener_ctx()

    with DatabasePool(WORKER_THREADS):
        # prepare repo name to id cache
        db_init_repo_cache()
        LISTENER_QUEUE.listen(process_message)

        # wait until loop is stopped from terminate callback
        loop.run_forever()

        LOGGER.info("Shutting down.")
        ListenerCtx.executor.shutdown()


if __name__ == '__main__':
    main()
