"""Module implementing kafka listener."""

import json
import tempfile

import requests

from archive_parser import ArchiveParser
from common.logging import init_logging, get_logger
from database.database_handler import DatabaseHandler, init_db
import mqueue
from utils import VERSION, format_vmaas_request

LOGGER = get_logger(__name__)


def db_import_system(conn, insights_id, inventory_id, rh_account, hostname, s3_url, vmaas_json, satellite_managed):
    """Import initial system record to the DB."""
    # TODO: json_checksum
    cur = conn.cursor()
    cur.execute("select insights_id from system_platform where insights_id = %s", (insights_id,))
    system_record = cur.fetchone()
    if system_record is None:
        cur.execute("""insert into system_platform (insights_id, inventory_id, rh_account, display_name, s3_url,
                                                    vmaas_json, json_checksum, satellite_managed)
                        values (%s, %s, %s, %s, %s, %s, %s, %s)""",
                    (insights_id, inventory_id, rh_account, hostname, s3_url, vmaas_json, '0', satellite_managed,))
    else:
        cur.execute("""update system_platform set inventory_id = %s, rh_account = %s, display_name = %s, s3_url = %s,
                                                  vmaas_json = %s, json_checksum = %s, satellite_managed = %s
                       where insights_id = %s""",
                    (inventory_id, rh_account, hostname, s3_url, vmaas_json, '0', satellite_managed, insights_id,))
    cur.close()
    conn.commit()


def main():
    """Main kafka listener entrypoint."""
    init_logging()
    init_db()
    LOGGER.info("Starting upload listener (version %s).", VERSION)
    # get DB connection
    conn = DatabaseHandler.get_connection()
    upload_queue = mqueue.QReader(mqueue.UPLOAD_TOPIC, mqueue.GROUP_ID)
    vuln_queue = mqueue.QWriter(mqueue.EVALUATOR_TOPIC)

    for msg in upload_queue:
        LOGGER.info('Received message from topic %s: %s', msg.topic, msg.value)
        upload_data = json.loads(msg.value.decode("utf8"))

        # Inventory ID is missing
        if upload_data["id"] is None:
            LOGGER.warning("Unable to store system, inventory ID is missing.")
            continue

        # Download archive an parse
        with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
            LOGGER.debug("Writing temp file to %s", tmp_file.name)
            response = requests.get(upload_data["url"], stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            parser = ArchiveParser(tmp_file.name)
            if not parser.parse():
                continue

        db_import_system(conn, parser.system_id, upload_data["id"], upload_data["rh_account"], parser.hostname,
                         upload_data["url"], format_vmaas_request(parser.package_list, parser.repo_list),
                         parser.satellite_managed)
        new_upload_msg = {"type": "upload_new_file",
                          "version": 1,
                          "system_id": parser.system_id,
                          "principal": upload_data["principal"],
                          "rh_account": upload_data["rh_account"],
                          "url": upload_data["url"],
                          "payload_id": upload_data["payload_id"]
                         }
        vuln_queue.send(new_upload_msg)
        LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC, json.dumps(new_upload_msg).encode("utf8"))

if __name__ == '__main__':
    main()
