"""Module implementing kafka listener."""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import hashlib
import json
import os
import signal
import tempfile

from psycopg2 import DatabaseError
from prometheus_client import Counter, start_http_server
import requests
import pytz

from common import mqueue
from common.database_handler import DatabaseHandler, init_db
from common.logging import init_logging, get_logger
from .archive_parser import ArchiveParser

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# How many times are we willing to try to grab an uploaded archive before moving on?
MAX_ARCHIVE_RETRIES = int(os.getenv('MAX_ARCHIVE_RETRIES', '2'))
# number of worker threads
WORKER_THREADS = int(os.getenv('WORKER_THREADS', '30'))

# prometheus metrics
NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
UNCHANGED_SYSTEM = Counter('ve_listener_upl_unchanged_system', '# of system-updates with same vmaas info')
PROCESS_UPLOAD = Counter('ve_listener_upl_uploads_processed', '# of uploaded archives processed')
MISSING_ID = Counter('ve_listener_upl_missing_inventory_id', '# of upload-msgs missing inventory_id')
ARCHIVE_PARSE_FAILURE = Counter('ve_listener_upl_archive_exceptions', '# of exceptions during archive-processing')
ARCHIVE_RETRIEVE_ATTEMPT = Counter('ve_listener_upl_archive_tgz_attempt', '# of times retried archive-retrieval')
ARCHIVE_RETRIEVE_FAILURE = Counter('ve_listener_upl_archive_tgz_failure', '# of times gave up on archive retrieval')
ARCHIVE_RETRIEVE_INVALID_HTTP = Counter('ve_listener_upl_archive_tgz_invalid_http',
                                        '# archive downloaded with invalid http code')
ARCHIVE_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')

# kafka clients
UPLOAD_QUEUE = mqueue.MQReader(mqueue.UPLOAD_TOPIC)
EVALUATOR_QUEUE = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await UPLOAD_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    loop.stop()


def on_thread_done(future):
    """Callback to call after ThreadPoolExecutor worker finishes."""
    try:
        future.result()
    except Exception: # pylint: disable=broad-except
        LOGGER.exception("Future %s hit exception: ", future)


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(conn, inventory_id, rh_account, s3_url, vmaas_json, satellite_managed):
    """Import initial system record to the DB, report back on what we did."""

    rtrn = {'inserted': False, 'updated': False, 'changed': False, 'failed': True}

    with conn.cursor() as cur:
        try:
            unchanged_since = None
            json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
            curr_time = datetime.now(tz=pytz.utc)
            # xmax is PG system column used to find out if row was inserted or updated
            cur.execute("""INSERT INTO system_platform
                        (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        ON CONFLICT (inventory_id) DO UPDATE SET
                        rh_account = %s, s3_url = %s, vmaas_json = %s, json_checksum = %s, satellite_managed = %s
                        RETURNING (xmax = 0) AS inserted, unchanged_since
                        """,
                        (inventory_id, rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,
                         rh_account, s3_url, vmaas_json, json_checksum, satellite_managed,))
            rtrn['inserted'], unchanged_since = cur.fetchone()
            conn.commit()

            # If inserting, or if unchanged_since is newer-than 'now', we want to evaluate this upload
            rtrn['changed'] = rtrn['inserted'] or (unchanged_since > curr_time)

            if rtrn['inserted']:
                NEW_SYSTEM.inc()
            else:
                rtrn['updated'] = True
                UPDATE_SYSTEM.inc()
            rtrn['failed'] = False
        except DatabaseError:
            LOGGER.exception("Error importing system: ")
            conn.rollback()
        return rtrn


def download_archive(url, tmp_file, session):
    """Download archive from url to tmp file."""
    success = False
    LOGGER.debug("Downloading %s to %s", url, tmp_file.name)
    # Grab the uploaded archive and write it locally so we can dig Important THings out of it
    # If the read-attempt fails, retry up to MAX_ARCHIVE_RETRIES times before logging the failure and moving on
    for tries in range(MAX_ARCHIVE_RETRIES):
        try:
            response = session.get(url, stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            if response.status_code == 200:
                LOGGER.debug("Downloading %s finished", url)
                success = True
            else:
                ARCHIVE_RETRIEVE_INVALID_HTTP.inc()
                LOGGER.error("Invalid HTTP status while downloading %s: %s", url, response.status_code)
            break
        except Exception: # pylint: disable=broad-except
            # on last try, log and fail this upload. Else, log try again
            if (tries+1) == MAX_ARCHIVE_RETRIES:
                ARCHIVE_RETRIEVE_FAILURE.inc()
                LOGGER.exception("Unable to retrieve archive: ")
            else:
                ARCHIVE_RETRIEVE_ATTEMPT.inc()
                LOGGER.exception("Unable to retrieve archive, retrying: ")

    return success


def parse_archive(upload_data, session):
    """Parse archive after it's downloaded."""
    vmaas_request = None
    with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
        if download_archive(upload_data["url"], tmp_file, session):
            parser = ArchiveParser(tmp_file.name)
            try:
                parser.parse()
                if parser.package_list:
                    vmaas_request = format_vmaas_request(parser.package_list,
                                                         repo_list=parser.repo_list,
                                                         modules_list=parser.modules_list)
                else:
                    ARCHIVE_NO_RPMDB.inc()
                    LOGGER.error("Unable to store system, empty package list.")
            except Exception: # pylint: disable=broad-except
                ARCHIVE_PARSE_FAILURE.inc()
                LOGGER.exception("Unable to parse archive: ")
    return vmaas_request


def process_upload(upload_data, session, conn, loop=None):
    """Parse archive and store it, ASSUMING vmaas-json has changed."""
    vmaas_request = parse_archive(upload_data, session)
    sent = False
    if vmaas_request:
        rtrn = db_import_system(conn, upload_data["id"], upload_data["rh_account"], upload_data["url"],
                                vmaas_request, upload_data.get("satellite_managed", False))
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if rtrn['changed']:
            new_upload_msg = {"type": "upload_new_file", "system_id": upload_data["id"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
        else:
            UNCHANGED_SYSTEM.inc()
    return sent

def main():
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    init_db()
    LOGGER.info("Starting upload listener.")
    # get DB connection
    conn = DatabaseHandler.get_connection()

    session = requests.Session()
    loop = asyncio.get_event_loop()
    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = ThreadPoolExecutor(WORKER_THREADS)

    def process_message(msg):
        """Message processing logic"""
        PROCESS_UPLOAD.inc()
        LOGGER.info('Received message from topic %s: %s', msg.topic, msg.value)

        upload_data = json.loads(msg.value.decode("utf8"))

        # Inventory ID is missing
        if 'id' not in upload_data or upload_data["id"] is None:
            MISSING_ID.inc()
            LOGGER.warning("Unable to store system, inventory ID is missing.")
            return

        future = executor.submit(process_upload, upload_data, session, conn, loop=loop)
        future.add_done_callback(on_thread_done)

    UPLOAD_QUEUE.listen(process_message)

    # wait until loop is stopped from terminate callback
    loop.run_forever()

    LOGGER.info("Shutting down.")
    executor.shutdown()
    session.close()

if __name__ == '__main__':
    main()
