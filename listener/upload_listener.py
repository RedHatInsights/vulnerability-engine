"""Module implementing kafka listener."""

import asyncio
from datetime import datetime, timedelta
import hashlib
import json
import os
import signal
import tempfile
from typing import Optional
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import flags

from psycopg2 import DatabaseError
from psycopg2.extras import execute_values
from prometheus_client import Counter, start_http_server
import requests
import pytz

from common import mqueue
from common.bounded_executor import BoundedExecutor
from common.database_handler import DatabasePool, DatabasePoolConnection
from common.identity import get_identity, is_entitled_insights
from common.logging import init_logging, get_logger
from common.failed_cache import FailedCache
from common.utils import on_thread_done, send_msg_to_payload_tracker
from .archive_parser import ArchiveParser

LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8086')
# How many times are we willing to try to grab an uploaded archive before moving on?
MAX_GET_RETRIES = int(os.getenv('MAX_GET_RETRIES', '3'))
# number of worker threads
WORKER_THREADS = int(os.getenv('WORKER_THREADS', '30'))
MAX_QUEUE_SIZE = int(os.getenv('MAX_QUEUE_SIZE', '30'))
SYSTEM_DELETION_THRESHOLD = int(os.getenv('SYSTEM_DELETION_THRESHOLD', '24'))  # 24 hours
DISABLE_OPTIMISATION = strtobool(os.getenv('DISABLE_OPTIMISATION', 'False'))
HOST_INVENTORY_PROFILE_URL = "%s%s" % (os.getenv('HOST_INVENTORY_HOST', 'http://platform_mock:8000'),
                                       os.getenv('HOST_INVENTORY_PROFILE_API', '/api/inventory/v1/hosts/%s/system_profile'))
DIRECT_INVENTORY_FETCH = strtobool(os.getenv('DIRECT_INVENTORY_FETCH', 'True'))

# prometheus metrics
NEW_SYSTEM = Counter('ve_listener_upl_new_system', '# of new systems inserted')
UPDATE_SYSTEM = Counter('ve_listener_upl_update_system', '# of systems updated')
UNCHANGED_SYSTEM = Counter('ve_listener_upl_unchanged_system', '# of system-updates with same vmaas info')
DELETED_SYSTEM = Counter('ve_listener_deleted_system', '# of systems deleted')
DELETED_SYSTEM_NOT_FOUND = Counter('ve_listener_deleted_system_nf', '# of systems to delete but not found')
DELETED_UPLOADED = Counter('ve_listener_deleted_uploaded', '# of systems uploaded after being deleted')
PROCESS_MESSAGES = Counter('ve_listener_messages_processed', '# of messages processed')
SKIPPED_MESSAGES = Counter('ve_listener_messages_skipped', '# of messages skipped')
UNKNOWN_EVENT_TYPE = Counter('ve_listener_unknown_event_type', '# of msgs with unknown event type')
UNKNOWN_TOPIC = Counter('ve_listener_unknown_topic', '# of msgs received on unsupported topic')
DATABASE_ERROR = Counter('ve_listener_database_error', '# of database errors')
ARCHIVE_PARSE_FAILURE = Counter('ve_listener_upl_archive_exceptions', '# of exceptions during archive-processing')
ARCHIVE_RETRIEVE_ATTEMPT = Counter('ve_listener_upl_archive_tgz_attempt', '# of times retried archive-retrieval')
ARCHIVE_RETRIEVE_FAILURE = Counter('ve_listener_upl_archive_tgz_failure', '# of times gave up on archive retrieval')
ARCHIVE_RETRIEVE_INVALID_HTTP = Counter('ve_listener_upl_archive_tgz_invalid_http',
                                        '# archive downloaded with invalid http code')
INVENTORY_RETRIEVE_ATTEMPT = Counter('ve_listener_inventory_attempt', '# of times retried inventory fetch')
INVENTORY_RETRIEVE_FAILURE = Counter('ve_listener_inventory_failure', '# of times gave up on inventory fetch')
INVENTORY_RETRIEVE_INVALID_HTTP = Counter('ve_listener_inventory_invalid_http', '# inventory fetch with invalid http code')
ARCHIVE_NO_RPMDB = Counter('ve_listener_upl_no_rpmdb', '# of systems ignored due to missing rpmdb')
MESSAGE_PARSE_ERROR = Counter('ve_listener_message_parse_error', '# of message parse errors')
NEW_REPO = Counter('ve_listener_upl_new_repo', '# of new repos inserted')
NEW_RH_ACCOUNT = Counter('ve_listener_upl_new_rh_account', '# of new rh accounts inserted')
NEW_SYSTEM_REPO = Counter('ve_listener_upl_new_system_repo', '# of new system_repo pairs inserted')
DELETED_SYSTEM_REPO = Counter('ve_listener_upl_system_repo_deleted', '# deleted system_repo pairs')
INVALID_IDENTITY = Counter('ve_listener_upl_invalid_identity',
                           '# of skipped uploads because of invalid identity')
MISSING_INSIGHTS_ENTITLEMENT = Counter('ve_listener_upl_non_insights_entitlement',
                                       '# of skipped uploads because of entitlement check')

# kafka clients
LISTENER_QUEUE = mqueue.MQReader([mqueue.UPLOAD_TOPIC, mqueue.EVENTS_TOPIC])
EVALUATOR_QUEUE = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)
PAYLOAD_TRACKER_PRODUCER = mqueue.MQWriter(mqueue.PAYLOAD_TRACKER_TOPIC)

# caching repo names to id
REPO_ID_CACHE = {}

REQUIRED_UPLOAD_MESSAGE_FIELDS = {
    "host": ["id", "account", "system_profile"],
    "platform_metadata": ["b64_identity", "url"],
    "timestamp": [],
    "type": []
}
REQUIRED_EVENT_MESSAGE_FIELDS = {
    "id": [],
    "type": []
}


class ImportStatus(flags.Flags):
    """Import to database status."""

    INSERTED = 1
    CHANGED = 2
    UPDATED = 4
    FAILED = 8


async def terminate(_, loop):
    """Trigger shutdown."""
    LOGGER.info("Signal received, stopping kafka consumers.")
    await LISTENER_QUEUE.stop()
    await EVALUATOR_QUEUE.stop()
    await PAYLOAD_TRACKER_PRODUCER.stop()
    loop.stop()


def format_vmaas_request(package_list, repo_list=None, modules_list=None):
    """Wrap package and repo list into the vmaas request format."""
    vmaas_request = {"package_list": package_list}
    if repo_list:
        vmaas_request["repository_list"] = repo_list
    if modules_list:
        vmaas_request["modules_list"] = modules_list
    return json.dumps(vmaas_request)


def db_import_system(upload_data, vmaas_json: str, repo_list: list):
    """Import initial system record to the DB, report back on what we did."""

    status = ImportStatus.FAILED

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                host = upload_data["host"]
                import_status, system_id = db_import_system_platform(cur, host['id'], host['account'], upload_data['platform_metadata']['url'],
                                                                     host.get("display_name"), host.get('stale_timestamp'),
                                                                     host.get('stale_warning_timestamp'), host.get('culled_timestamp'), vmaas_json)
                if import_status is None:
                    return status
                status |= import_status

                db_import_repos(cur, repo_list)
                db_import_system_repos(cur, repo_list, system_id)

                db_delete_other_system_repos(cur, repo_list, system_id)

                conn.commit()

                status -= ImportStatus.FAILED
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error importing system: ")
                FailedCache.push(FailedCache.upload_cache, upload_data)
                LOGGER.info("Remembered upload %s", str(upload_data))
                conn.rollback()
    return status


def db_import_system_platform(cur, inventory_id: str, rh_account: str, s3_url: str, display_name: str,
                              stale_timestamp: str, stale_warning_timestamp: str, culled_timestamp: str, vmaas_json: str):
    """Import system_platform item to db table."""

    curr_time = datetime.now(tz=pytz.utc)
    cur.execute("""select inventory_id from deleted_systems where inventory_id = %s and when_deleted > %s""",
                (inventory_id, curr_time - timedelta(hours=SYSTEM_DELETION_THRESHOLD, )))
    if cur.fetchone() is not None:
        LOGGER.warning('Received recently deleted inventory id: %s', inventory_id)
        DELETED_UPLOADED.inc()
        return None, None

    cur.execute("""INSERT INTO rh_account (name) VALUES(%s) ON CONFLICT (name) DO NOTHING
                RETURNING (xmax = 0) AS inserted""", (rh_account,))
    inserted = cur.fetchone()
    if inserted:
        NEW_RH_ACCOUNT.inc()

    cur.execute("""SELECT id FROM rh_account WHERE name = %s""", (rh_account,))
    rh_account_id = cur.fetchone()[0]

    json_checksum = hashlib.sha256(vmaas_json.encode('utf-8')).hexdigest()
    # xmax is PG system column used to find out if row was inserted or updated
    cur.execute("""INSERT INTO system_platform
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, last_upload,
                stale_timestamp, stale_warning_timestamp, culled_timestamp, stale)
                VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, %s, %s, %s, 'F')
                ON CONFLICT (inventory_id) DO UPDATE SET
                rh_account_id = %s, s3_url = %s, display_name = %s, vmaas_json = %s, json_checksum = %s, last_upload = CURRENT_TIMESTAMP,
                stale_timestamp = %s, stale_warning_timestamp = %s, culled_timestamp = %s, stale = 'F'
                RETURNING (xmax = 0) AS inserted, unchanged_since, last_evaluation, id""",
                (inventory_id, rh_account_id, s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp,
                 rh_account_id, s3_url, display_name, vmaas_json, json_checksum, stale_timestamp, stale_warning_timestamp, culled_timestamp))
    inserted, unchanged_since, last_evaluation, system_id = cur.fetchone()
    if inserted:
        import_status = ImportStatus.INSERTED
        NEW_SYSTEM.inc()
    else:
        import_status = ImportStatus.UPDATED
        UPDATE_SYSTEM.inc()

    if inserted or not last_evaluation or (unchanged_since > last_evaluation):
        import_status |= ImportStatus.CHANGED
    return import_status, system_id


def db_import_system_repos(cur, repos: list, system_id: int):
    """Import items to system_repo table."""

    if repos:
        to_insert = [(system_id, REPO_ID_CACHE[repo]) for repo in repos]
        execute_values(cur, """insert into system_repo (system_id, repo_id) values %s
                               on conflict (system_id, repo_id) do nothing returning repo_id""",
                       to_insert, page_size=len(to_insert))
        repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
        if repo_ids:
            NEW_SYSTEM_REPO.inc(len(repo_ids))
        return repo_ids
    return None


def db_delete_other_system_repos(cur, repos: list, system_id: int):
    """Delete all system_repo items not including input repos."""

    if repos:
        repos_ids = [REPO_ID_CACHE[repo] for repo in repos]
        cur.execute("""delete from system_repo sr
                    where sr.system_id = %s and sr.repo_id not in %s
                    returning sr.repo_id""", (system_id, tuple(repos_ids)))
    else:
        cur.execute("""delete from system_repo where system_id = %s returning repo_id""",
                    (system_id, ))
    deleted_repo_ids = [repo_id for repo_id, *_ in cur.fetchall()]
    if deleted_repo_ids:
        DELETED_SYSTEM_REPO.inc(len(deleted_repo_ids))
    return deleted_repo_ids


def db_import_repos(cur, repos: list):
    """Ensure input repos to be in repo db table."""

    to_insert = sorted({(repo, ) for repo in repos if repo not in REPO_ID_CACHE})
    if to_insert:
        repo_names = []
        # make sure we do an update on conflict to fetch already existing ID imported by someone else concurrently
        execute_values(cur, """insert into repo (name) values %s on conflict (name) do update set name = EXCLUDED.name
                               returning id, name, (xmax = 0) AS inserted""", to_insert, page_size=len(to_insert))
        for repo_id, repo_name, inserted in cur.fetchall():
            if inserted:
                NEW_REPO.inc()
                repo_names.append(repo_name)
            REPO_ID_CACHE[repo_name] = repo_id
        return repo_names
    return None


def db_delete_system(msg_dict):
    """Delete system with inventory ID."""

    rtrn = {'deleted': False, 'failed': True}

    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                curr_time = datetime.now(tz=pytz.utc)
                cur.execute("""INSERT INTO deleted_systems
                            (inventory_id, when_deleted) VALUES (%s, %s)
                            ON CONFLICT (inventory_id) DO UPDATE SET
                            when_deleted = EXCLUDED.when_deleted
                            """, (msg_dict['id'], curr_time,))
                cur.execute("""DELETE FROM deleted_systems WHERE when_deleted < %s
                            """, (curr_time - timedelta(hours=SYSTEM_DELETION_THRESHOLD),))
                cur.execute("""SELECT deleted_inventory_id FROM delete_system(%s)""", (msg_dict['id'],))
                system_platform = cur.fetchone()
                if system_platform is not None:
                    rtrn['deleted'] = True
                conn.commit()
                rtrn['failed'] = False
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error deleting system: ")
                FailedCache.push(FailedCache.delete_cache, msg_dict)
                LOGGER.info("Remembered deleting %s", str(msg_dict))
                conn.rollback()
    return rtrn


def db_init_repo_cache():
    """Populate initial repo cache"""
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            try:
                cur.execute("""SELECT id, name FROM repo""")
                for repo_id, repo_name in cur.fetchall():
                    REPO_ID_CACHE[repo_name] = repo_id
            except DatabaseError:
                DATABASE_ERROR.inc()
                LOGGER.exception("Error caching repos: ")
                conn.rollback()


def download_archive(url, tmp_file):
    """DEPRECATED. Download archive from url to tmp file."""
    success = False
    LOGGER.debug("Downloading %s to %s", url, tmp_file.name)
    # Grab the uploaded archive and write it locally so we can dig Important THings out of it
    # If the read-attempt fails, retry up to MAX_GET_RETRIES times before logging the failure and moving on
    for tries in range(MAX_GET_RETRIES):
        try:
            response = requests.get(url, stream=True, allow_redirects=True)
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    tmp_file.write(chunk)
            tmp_file.flush()
            if response.status_code == 200:
                LOGGER.debug("Downloading %s finished", url)
                success = True
            else:
                ARCHIVE_RETRIEVE_INVALID_HTTP.inc()
                LOGGER.error("Invalid HTTP status while downloading %s: %s", url, response.status_code)
            break
        except Exception:  # pylint: disable=broad-except
            # on last try, log and fail this upload. Else, log try again
            if (tries+1) == MAX_GET_RETRIES:
                ARCHIVE_RETRIEVE_FAILURE.inc()
                LOGGER.exception("Unable to retrieve archive: ")
            else:
                ARCHIVE_RETRIEVE_ATTEMPT.inc()
                LOGGER.exception("Unable to retrieve archive, retrying: ")

    return success


def parse_archive(upload_data: dict) -> (Optional[str], list):
    """DEPRECATED. Parse archive after it's downloaded."""
    vmaas_request = None
    repo_list = []
    with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
        if download_archive(upload_data['platform_metadata']["url"], tmp_file):
            parser = ArchiveParser(tmp_file.name)
            try:
                parser.parse()
                if parser.package_list:
                    vmaas_request = format_vmaas_request(parser.package_list,
                                                         repo_list=parser.repo_list,
                                                         modules_list=parser.modules_list)
                    repo_list = parser.repo_list
                else:
                    ARCHIVE_NO_RPMDB.inc()
                    LOGGER.error("Unable to store system, empty package list.")
            except Exception:  # pylint: disable=broad-except
                ARCHIVE_PARSE_FAILURE.inc()
                LOGGER.exception("Unable to parse archive: ")
    return vmaas_request, repo_list


def parse_inventory_data(upload_data: dict) -> (Optional[str], list):
    """Parse inventory data from upload message."""
    vmaas_request = None
    repo_list = []
    system_profile = upload_data["host"]["system_profile"]
    installed_packages = system_profile.get("installed_packages")
    if installed_packages:
        repo_list = [r["id"].strip() for r in system_profile.get("yum_repos", []) if r.get("enabled", True) and r["id"].strip()]
        modules_list = [{"module_name": m["name"], "module_stream": m["stream"]} for m in system_profile.get("dnf_modules", [])]
        vmaas_request = format_vmaas_request(installed_packages, repo_list=repo_list, modules_list=modules_list)
    else:
        ARCHIVE_NO_RPMDB.inc()
        LOGGER.error("Skipping inventory_id because of empty package list: %s", upload_data["host"]["id"])
    return vmaas_request, repo_list


def process_upload(upload_data, loop=None):
    """Parse archive and store it, ASSUMING vmaas-json has changed."""
    if DIRECT_INVENTORY_FETCH:
        vmaas_request, repo_list = parse_inventory_data(upload_data)
    else:  # fallback to old deprecated way
        vmaas_request, repo_list = parse_archive(upload_data)
    sent = False
    if vmaas_request:
        import_status = db_import_system(upload_data, vmaas_request, repo_list)
        # only give evaluator work if the system's vmaas-call has changed since the last time we did this
        if ImportStatus.CHANGED in import_status or DISABLE_OPTIMISATION:
            new_upload_msg = {
                "type": "upload_new_file",
                "host": {
                    "id": upload_data["host"]["id"],
                    "account": upload_data["host"]["account"]
                },
                "platform_metadata": {
                    "request_id": upload_data['platform_metadata'].get("request_id")
                },
                "timestamp": upload_data["timestamp"]}
            EVALUATOR_QUEUE.send(new_upload_msg, loop=loop)
            LOGGER.info('Sent message to topic %s: %s', mqueue.EVALUATOR_TOPIC,
                        json.dumps(new_upload_msg).encode("utf8"))
            sent = True
        else:
            UNCHANGED_SYSTEM.inc()
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, upload_data, 'success',
                                        status_msg='unchanged system and not evaluated', loop=loop)
    return sent


def process_delete(msg_dict, **_):
    """Delete system."""
    rtrn = db_delete_system(msg_dict)
    if not rtrn["failed"]:
        if rtrn["deleted"]:
            DELETED_SYSTEM.inc()
            LOGGER.info("Deleted system with inventory_id: %s", msg_dict["id"])
        else:
            DELETED_SYSTEM_NOT_FOUND.inc()
            LOGGER.info("Unable to delete system, inventory_id not found: %s", msg_dict["id"])


def validate_msg(msg_dict, msg_type, required_fields):
    """Check if all required fields are in msg_dict."""
    for key, required_inner_keys in required_fields.items():
        if key not in msg_dict:
            SKIPPED_MESSAGES.inc()
            LOGGER.warning("Missing key '%s' in msg_dict, skipping %s.", key, msg_type)
            return False
        inner_msg_dict = msg_dict[key]
        for inner_key in required_inner_keys:
            if inner_key not in inner_msg_dict:
                SKIPPED_MESSAGES.inc()
                LOGGER.warning("Missing key '%s.%s' in msg_dict, skipping %s.", key, inner_key, msg_type)
                return False
    return True


def main():  # pylint: disable=too-many-statements
    """Main kafka listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    LOGGER.info("Starting upload listener.")

    loop = asyncio.get_event_loop()
    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        loop.add_signal_handler(
            sig, lambda sig=sig: loop.create_task(terminate(sig, loop)))
    executor = BoundedExecutor(MAX_QUEUE_SIZE, max_workers=WORKER_THREADS)

    def process_message(msg):  # pylint: disable=too-many-return-statements,too-many-branches
        """Message processing logic"""
        PROCESS_MESSAGES.inc()
        LOGGER.debug('Received message from topic %s: %s', msg.topic, msg.value)

        try:
            msg_dict = json.loads(msg.value.decode("utf8"))
        except json.decoder.JSONDecodeError:
            MESSAGE_PARSE_ERROR.inc()
            LOGGER.exception("Unable to parse message: ")
            return
        FailedCache.process_failed_cache(FailedCache.upload_cache, executor, process_upload, loop)
        FailedCache.process_failed_cache(FailedCache.delete_cache, executor, process_delete, loop)

        if msg.topic == mqueue.UPLOAD_TOPIC:
            if not validate_msg(msg_dict, "upload", REQUIRED_UPLOAD_MESSAGE_FIELDS):
                return
            LOGGER.info("Received upload msg, inventory_id: %s, type: %s", msg_dict["host"]["id"], msg_dict["type"])
            # send message to payload tracker
            send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'received', loop=loop)
            # proces only archives from insights entitled accounts
            identity = get_identity(msg_dict["platform_metadata"]["b64_identity"])
            if identity is None:
                INVALID_IDENTITY.inc()
                error_msg = "Skipped upload due to invalid identity header."
                LOGGER.warning(error_msg)
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=loop)
                return
            if not is_entitled_insights(identity, allow_missing_section=True):
                MISSING_INSIGHTS_ENTITLEMENT.inc()
                error_msg = "Skipped upload due to missing insights entitlement."
                LOGGER.debug(error_msg)
                send_msg_to_payload_tracker(PAYLOAD_TRACKER_PRODUCER, msg_dict, 'error', status_msg=error_msg, loop=loop)
                return
            process_func = process_upload
        elif msg.topic == mqueue.EVENTS_TOPIC:
            if not validate_msg(msg_dict, "event", REQUIRED_EVENT_MESSAGE_FIELDS):
                return
            LOGGER.info("Received event msg, inventory_id: %s, type: %s", msg_dict["id"], msg_dict["type"])
            if msg_dict['type'] == 'delete':
                process_func = process_delete
            else:
                UNKNOWN_EVENT_TYPE.inc()
                LOGGER.error("Received unknown event type: %s", msg_dict['type'])
                return
        else:
            UNKNOWN_TOPIC.inc()
            LOGGER.error("Received message on unsupported topic: %s", msg.topic)
            return

        future = executor.submit(process_func, msg_dict, loop=loop)
        future.add_done_callback(on_thread_done)

    with DatabasePool(WORKER_THREADS):
        # prepare repo name to id cache
        db_init_repo_cache()
        LISTENER_QUEUE.listen(process_message)

        # wait until loop is stopped from terminate callback
        loop.run_forever()

        LOGGER.info("Shutting down.")
        executor.shutdown()


if __name__ == '__main__':
    main()
