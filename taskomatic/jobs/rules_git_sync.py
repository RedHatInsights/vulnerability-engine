#!/usr/bin/env python3
"""
Importer for insights rules from insights-content git
"""

from datetime import datetime
import importlib
import os
import sys
import tempfile

from git import Repo
from git.cmd import GitCommandError
import psycopg2
from psycopg2.extras import execute_values
import yaml

from common.database_handler import DB_NAME, DB_USER, DB_PASS, DB_HOST, DB_PORT
from common.logging import get_logger, init_logging

LOGGER = get_logger(__name__)

CONTENT_GIT_REPO = os.getenv("CONTENT_GIT_REPO", "")
PLAYBOOKS_GIT_REPO = os.getenv("PLAYBOOKS_GIT_REPO", "")
PRODSEC_GIT_REPO = os.getenv("PRODSEC_GIT_REPO", "")

GIT_TOKEN = os.getenv("GIT_TOKEN", "")

CONTENT_GIT_NAME = "content_git"
PLAYBOOKS_GIT_NAME = "playbooks_git"
PRODSEC_GIT_NAME = "prodsec_git"


def import_into_db(rules_dict):
    """Imports rules into database"""
    conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT)
    cur = conn.cursor()

    to_delete = []
    to_disable = []
    to_enable = []
    to_insert = []
    to_update = []
    to_update_names = []
    cur.execute("""SELECT id, name, active from insights_rule""")
    for rule_id, rule_name, active in cur.fetchall():
        if rule_name not in rules_dict:
            pass
            # to_delete.append(rule_id)
        else:
            to_update.append(rule_id)
            to_update_names.append(rule_name)
            if not active and rules_dict[rule_name]['active']:
                to_enable.append(rule_id)
            elif active and not rules_dict[rule_name]['active']:
                to_disable.append(rule_id)
    to_insert = rules_dict.keys() - to_update_names

    if to_insert:
        rule_insert(conn, cur, to_insert, rules_dict)
    if to_update:
        rule_update(conn, cur, to_update_names, rules_dict)
    if to_delete:
        rule_delete(conn, cur, to_delete)
    if to_enable:
        rule_enable(conn, cur, to_enable)
    if to_disable:
        rule_disable(conn, cur, to_disable)

    cur.close()
    conn.close()


def update_playbooks(cur, to_insert, to_delete):
    """Updates playbooks associated with rules as they have no unique identifiers, it's better to delete them and repopulate

    Args:
        cur: database cursor
        to_insert ([list(tuple)]): List of tuples containing playbook data
        to_delete ([tuple]): List of rules which should have their playbook data deleted and repoplated
    """
    if to_delete:
        execute_values(cur, """DELETE FROM playbook WHERE rule_id IN (SELECT id from insights_rule WHERE name IN (%s))""",
                       (to_delete,), page_size=len(to_delete))
    for record in to_insert:
        cur.execute("""INSERT INTO playbook (rule_id, play, version, description)
                       VALUES ((SELECT id FROM insights_rule where name = %s), %s, %s, %s)""", record)


def rule_delete(conn, cur, to_delete):
    """Delete rules"""
    rule_disable(conn, cur, to_delete)
    LOGGER.info("Deleting %s rules.", len(to_delete))
    cur.execute("""DELETE FROM playbook WHERE rule_id IN (%s)""", tuple(to_delete))
    cur.execute("""DELETE FROM insights_rule WHERE id IN (%s)""", tuple(to_delete))
    conn.commit()


def rule_disable(conn, cur, to_disable):
    """Disable rule"""
    LOGGER.info("Disabling %s rules.", len(to_disable))
    execute_values(cur, """UPDATE insights_rule AS ir SET active = 'F' FROM (VALUES %s) AS v(id) WHERE ir.id = v.id""",
                   [(x,) for x in to_disable], page_size=len(to_disable))
    conn.commit()


def rule_enable(conn, cur, to_enable):
    """Enable rule"""
    LOGGER.info("Enabling %s rules.", len(to_enable))
    execute_values(cur, """UPDATE insights_rule AS ir SET active = 'T' FROM (VALUES %s) AS v(id) WHERE ir.id = v.id""",
                   [(x,) for x in to_enable], page_size=len(to_enable))
    conn.commit()


def prep_playbooks(rules_dict):
    """Helper function to extract playbooks from rules data dictionary

    Args:
        rules_dict ([dict]): Rules data dictionary

    Returns:
        [list(tuple)]: List of tuples for the SQL command in update_playbooks function
    """
    playbooks = []
    for rule in rules_dict:
        playbooks.extend([(rule, playbook['play'], playbook['version'], 'Fix issues caused by %s.' % rule)
                          for playbook in rules_dict[rule]['playbooks']])
    return playbooks


def rule_insert(conn, cur, to_insert_names, rules_dict):
    """Insert existing rules"""

    to_insert = [(
        (rules_dict[rule]['id']), (rules_dict[rule]['description']), (rules_dict[rule]['summary']), (rules_dict[rule]['generic']),
        (rules_dict[rule]['reason']), (rules_dict[rule]['resolution']), (rules_dict[rule]['more_info']), (rules_dict[rule]['reboot_required']),
        (rules_dict[rule]['playbook_count']), (rules_dict[rule]['change_risk']), (rules_dict[rule]['kbase_node_id']), (rules_dict[rule]['active']),
        (rules_dict[rule]['rule_impact']), (rules_dict[rule]['publish_date']),
    ) for rule in to_insert_names]

    LOGGER.info("Inserting %s.", ", ".join([insert[0] for insert in to_insert]))

    execute_values(cur, """INSERT INTO insights_rule (name, description_text, summary_text, generic_text, reason_text, resolution_text, more_info_text,
                                                      reboot_required, playbook_count, change_risk,
                                                      kbase_node_id, active, rule_impact, publish_date) VALUES %s""",
                   to_insert, template='(%s, %s, %s, %s, %s, %s, %s, %s, %s::integer, %s::integer, %s::integer, %s, %s::integer, %s::timestamp)',
                   page_size=len(to_insert))

    conn.commit()

    cve_id_mapping = {}
    cur.execute("""SELECT id, cve FROM cve_metadata""")
    for cve_id, cve_name in cur.fetchall():
        cve_id_mapping[cve_name] = cve_id

    for rule_name in to_insert_names:
        rule = rules_dict[rule_name]
        for cve in rule['cves']:
            if cve not in cve_id_mapping:
                cur.execute("""INSERT INTO cve_metadata (cve, description, impact_id) VALUES %s ON CONFLICT (cve) DO UPDATE SET cve = %s
                RETURNING id AS inserted""", ((cve, 'unknown', 0,), cve))
                conn.commit()
                inserted = cur.fetchone()
                cve_id_mapping[cve] = inserted[0]
            cur.execute("""INSERT INTO cve_rule_mapping (cve_id, rule_id) VALUES (%s, (SELECT id from insights_rule WHERE name = %s))""",
                        (cve_id_mapping[cve], rule['id']))
    update_playbooks(cur, prep_playbooks(rules_dict), tuple({x[0] for x in to_insert}))
    conn.commit()


def rule_update(conn, cur, to_update_names, rules_dict):
    """Update existing rules"""

    to_update = [(
        (rules_dict[rule]['id']), (rules_dict[rule]['description']), (rules_dict[rule]['summary']), (rules_dict[rule]['generic']),
        (rules_dict[rule]['reason']), (rules_dict[rule]['resolution']), (rules_dict[rule]['more_info']), (rules_dict[rule]['reboot_required']),
        (rules_dict[rule]['playbook_count']), (rules_dict[rule]['change_risk']), (rules_dict[rule]['kbase_node_id']), (rules_dict[rule]['rule_impact']),
        (rules_dict[rule]['publish_date'])
    ) for rule in to_update_names]

    LOGGER.info("Updating %s.", ", ".join([update[0] for update in to_update]))

    execute_values(cur, """UPDATE insights_rule AS ir SET description_text = v.description, summary_text = v.summary, generic_text = v.generic,
                    reason_text = v.reason, resolution_text = v.resolution, more_info_text = v.more_info,
                    reboot_required = v.reboot_required, playbook_count = v.playbook_count, change_risk = v.change_risk,
                    kbase_node_id = v.kbase_node_id, rule_impact = v.rule_impact, publish_date = v.publish_date FROM (VALUES %s)
                    AS v(name, description, summary, generic, reason, resolution, more_info, reboot_required, playbook_count, change_risk, kbase_node_id,
                     rule_impact, publish_date)
                    WHERE v.name = ir.name""",
                   to_update, template='(%s, %s, %s, %s, %s, %s, %s, %s, %s::integer, %s::integer, %s::integer, %s::integer, %s::timestamp)',
                   page_size=len(to_update))
    update_playbooks(cur, prep_playbooks(rules_dict), tuple({x[0] for x in to_update}))
    conn.commit()


def resolution_risk_map(config_file):
    """Returns mapping of resolution risk to number"""
    with open(config_file) as file_desc:
        data = yaml.safe_load(file_desc)
    return data['resolution_risk']


def impact_string_map(config_file):
    """Returns mapping of impact string to number"""
    with open(config_file) as file_desc:
        data = yaml.safe_load(file_desc)
    return data['severity']


def sync(tmpdirname, playbooks_version):  # pylint: disable=too-many-branches
    """Sync from dirs function"""

    content_git = f"{tmpdirname}/{CONTENT_GIT_NAME}"
    playbooks_git = f"{tmpdirname}/{PLAYBOOKS_GIT_NAME}"
    prodsec_git = f"{tmpdirname}/{PRODSEC_GIT_NAME}"
    securiry_rules_dir = '{}/content'.format(content_git)
    playbooks_dir = '{}/playbooks/security'.format(playbooks_git)
    for git_dir in (securiry_rules_dir, playbooks_dir, prodsec_git):
        if not os.path.isdir(git_dir):
            LOGGER.error("%s directory does not exist", git_dir)
            return

    sys.path.append(prodsec_git)

    risk_map = resolution_risk_map(os.path.join(content_git, 'content/config.yaml'))
    impact_map = impact_string_map(os.path.join(content_git, 'content/config.yaml'))

    # pylint: disable=too-many-nested-blocks
    for file_name in os.listdir(securiry_rules_dir):
        dir_name = '{}/{}'.format(securiry_rules_dir, file_name)
        if os.path.isdir(dir_name):
            try:
                with open(os.path.join(dir_name, 'plugin.yaml'), 'r') as stream:
                    plugin_info = yaml.safe_load(stream)
                for nested_file in os.listdir(dir_name):
                    full_nested_path = os.path.join(dir_name, nested_file)
                    if os.path.isdir(full_nested_path):
                        with open(os.path.join(full_nested_path, 'metadata.yaml'), 'r') as stream:
                            metadata = yaml.safe_load(stream)
                        rule_playbook_dir = os.path.join(playbooks_dir, file_name, nested_file, 'rhel_host')
                        playbooks = []
                        if os.path.isdir(rule_playbook_dir):
                            for play in os.listdir(rule_playbook_dir):
                                if play.endswith('_fixit.yml'):
                                    with open(os.path.join(rule_playbook_dir, play)) as play_stream:
                                        playbooks.append({'play': play_stream.read(), 'version': playbooks_version})
                        rule_module_name = plugin_info.get('python_module')
                        rule_module = importlib.import_module(rule_module_name)
                        rule = {
                            'active': metadata.get('status') == 'active',
                            'change_risk': risk_map.get(metadata.get('resolution_risk'), 1),
                            'cves': rule_module.CVES.copy(),
                            'description': metadata.get('description').split(': ')[-1],
                            'id': '{}|{}'.format(file_name, nested_file),
                            'kbase_node_id': plugin_info.get('node_id') if plugin_info.get('node_id') else None,
                            'playbook_count': len(playbooks),
                            'playbooks': playbooks,
                            'publish_date': datetime.strptime(metadata['publish_date'], '%Y-%m-%d %H:%M:%S') if 'publish_date' in metadata else None,
                            'reboot_required': plugin_info.get('reboot_required'),
                            'rule_impact': impact_map.get(metadata.get('severity'), 1)
                        }

                        for info_file, attribute in [('summary.md', 'summary'), ('generic.md', 'generic'), ('reason.md', 'reason'),
                                                     ('resolution.md', 'resolution'), ('more_info.md', 'more_info')]:
                            if os.path.exists(os.path.join(full_nested_path, info_file)):
                                with open(os.path.join(full_nested_path, info_file)) as file_desc:
                                    rule[attribute] = file_desc.read()
                            else:
                                with open(os.path.join(dir_name, info_file)) as file_desc:
                                    rule[attribute] = file_desc.read()
                        import_into_db({rule['id']: rule})
            except Exception:  # pylint: disable=broad-except
                LOGGER.exception('Error during rule parsing: ')
    sys.path.pop()
    # Force Python to forget currently imported rule modules
    delete_rule_module_names = [module_name for module_name, module in sys.modules.items() if prodsec_git in getattr(module, "__file__", "")]
    for rule_module_name in delete_rule_module_names:
        del sys.modules[rule_module_name]


def clone_repo(git_repo, repo_dir_name):
    """Clone single git repo"""
    if not git_repo:
        LOGGER.error("Git repo for %s not set.", repo_dir_name)
        return False
    git_repo = git_repo.replace("github.com", "%s@github.com" % GIT_TOKEN) if GIT_TOKEN else git_repo
    try:
        repo = Repo.clone_from(git_repo, repo_dir_name, depth=1)
    except GitCommandError:
        LOGGER.exception("Error cloning repo: ")
        return False
    return repo.head.object.hexsha


def clone_repos(tmpdirname):
    """Clone git repos to temp dir"""
    results = (
        clone_repo(CONTENT_GIT_REPO, f"{tmpdirname}/{CONTENT_GIT_NAME}"),
        clone_repo(PLAYBOOKS_GIT_REPO, f"{tmpdirname}/{PLAYBOOKS_GIT_NAME}"),
        clone_repo(PRODSEC_GIT_REPO, f"{tmpdirname}/{PRODSEC_GIT_NAME}")
    )
    return all(results), results


def run():
    """Application entrypoint"""
    LOGGER.info('Started rules_git_sync job.')
    with tempfile.TemporaryDirectory() as tmpdirname:
        LOGGER.info("Cloning repos.")
        success, revisions = clone_repos(tmpdirname)
        if success:
            LOGGER.info("Cloning repos successful, starting sync.")
            sync(tmpdirname, revisions[1])
    LOGGER.info('Finished rules_git_sync job.')


if __name__ == '__main__':
    init_logging()
    run()
