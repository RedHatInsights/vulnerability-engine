"""VMaaS websocket listener module."""

import asyncio
import datetime as dt
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import os
import signal

from aiohttp import web, ClientSession, WSMsgType
from prometheus_client import Counter
from psycopg2.extras import execute_values, Json

from common.database_handler import DatabasePool, DatabasePoolConnection, NamedCursor
from common.logging import init_logging, get_logger
from common.paging import paging
from common.utils import a_ensure_minimal_schema_version, external_service_request
from common import mqueue, constants
from common.status_app import create_status_app, create_status_runner
from common.backoffice_proxy import create_cert, format_headers, BO_PROXY_ENDPOINT_EXPLOITS

WEBSOCKET_RECONNECT_INTERVAL = 60
LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8087')

REFRESH = Counter('ve_vmaas_sync_webscan_refreshes', '# of times VMaaS told us it had new data')
CNX_FAIL = Counter('ve_vmaas_sync_websocket_failures', '# of times VMaaS websocket closed on us')
CNX_RECONNECT = Counter('ve_vmaas_sync_websocket_recnx', '# of times attempted VMaaS websocket reconnect')

VMAAS_HOST = os.getenv('VMAAS_HOST', 'http://vmaas_webapp:8080')
VMAAS_CVES_ENDPOINT = "%s/api/v1/cves" % VMAAS_HOST
VMAAS_REPOS_ENDPOINT = "%s/api/v1/repos" % VMAAS_HOST
ENABLE_RE_EVALUATION = strtobool(os.getenv("ENABLE_RE_EVALUATION", "YES"))
ENABLE_REPO_BASED_RE_EVALUATION = strtobool(os.getenv("ENABLE_REPO_BASED_RE_EVALUATION", "NO"))
DEFAULT_PAGE_SIZE = int(os.getenv("DEFAULT_PAGE_SIZE", "5000"))

# how many systems to select in one batch and send to kafka
RE_EVALUATION_KAFKA_BATCH_SIZE = int(os.getenv("RE_EVALUATION_KAFKA_BATCH_SIZE", "10000"))
# how many batches to create at the same time
RE_EVALUATION_KAFKA_BATCHES = int(os.getenv("RE_EVALUATION_KAFKA_BATCHES", "10"))
RE_EVALUATION_KAFKA_BATCH_SEMAPHORE = asyncio.BoundedSemaphore(RE_EVALUATION_KAFKA_BATCHES)

SYNC_LOCK = asyncio.Lock()


class BaseView(web.View):
    """AIOHTTP web View class for endpoints with special helper functions."""

    @classmethod
    def select_repo_based_inventory_ids(cls, cur, repos: list):
        """Select inventory-ids connected with inserted repos, don't fetch it."""
        if repos:
            cur.execute("""select inventory_id from system_platform where
                           when_deleted is null and
                           id in (select distinct system_id from system_repo where repo_id in
                           (select id from repo where name in %s))""", (tuple(repos),))
        else:
            cur.execute("""select * from system_repo where (1=0)""")  # ensure empty result

    @classmethod
    def select_all_inventory_ids(cls, cur):
        """Select all inventory-ids, don't fetch it."""
        cur.execute("select inventory_id from system_platform where when_deleted is null")

    @classmethod
    def get_last_repobased_eval_tms(cls, cur):
        """Select last repo-based evaluation timestamp."""
        cur.execute("select value from timestamp_kv where name = %s", (constants.TIMESTAMP_LAST_REPO_BASED_EVAL,))
        ret = cur.fetchone()
        if ret:
            return ret[0]
        return None

    @classmethod
    def set_last_repobased_eval_tms(cls, cur, timestamp: dt.datetime):
        """Update last repo-based evaluation timestamp."""
        cur.execute("""insert into timestamp_kv (name, value) values (%s, %s)
                    on conflict (name) do update set value = %s returning value, (xmax = 0) as inserted""",
                    (constants.TIMESTAMP_LAST_REPO_BASED_EVAL, timestamp, timestamp))
        ret = cur.fetchone()
        return ret

    @classmethod
    def _vmaas_repos_modified_since(cls, modified_since: str) -> list:
        """Get list of modified repose since `modified since`"""
        repos_json = {"repository_list": [".*"], "page": 1, "page_size": DEFAULT_PAGE_SIZE,
                      "modified_since": modified_since}
        success, repos_pages = paging(VMAAS_REPOS_ENDPOINT, repos_json)
        if not success:
            return []
        repos = list(repos_pages["repository_list"])
        LOGGER.info("%d repos found updated since %s", len(repos), modified_since)
        return repos

    @staticmethod
    def construct_cve_row(cve, impact_id_map):
        """Helper function to construct CVE row to be insterted/updated in the DB"""
        description = cve['description']
        impact_id = impact_id_map[cve['impact']]
        public_date = cve['public_date'] or None
        modified_date = cve['modified_date'] or None
        cvss3_score = float(cve['cvss3_score']) if cve.get('cvss3_score') else None
        cvss3_metrics = cve.get('cvss3_metrics')
        cvss2_score = float(cve['cvss2_score']) if cve.get('cvss2_score') else None
        cvss2_metrics = cve.get('cvss2_metrics')
        redhat_url = cve.get('redhat_url', None)
        secondary_url = cve.get('secondary_url', None)
        advisories_list = Json(cve.get('errata_list', None))
        return (cve['synopsis'], description, impact_id, public_date, modified_date, cvss3_score, cvss3_metrics,
                cvss2_score, cvss2_metrics, redhat_url, secondary_url, advisories_list)

    @staticmethod
    def process_cve_list(cves, cves_in_db, impact_id_map):
        """Processes cve list returned from VMaaS"""
        to_insert = []
        to_update = []
        to_delete = []
        for cve in cves:
            row = SyncHandler.construct_cve_row(cves[cve], impact_id_map)
            if cve not in cves_in_db:
                to_insert.append(row)
            else:
                to_update.append(row)
        to_delete = [(cve,) for cve in cves_in_db if cve not in cves]
        return to_insert, to_update, to_delete

    @staticmethod
    def insert_cves(cur, to_insert):
        """Insert CVEs into DB"""
        if to_insert:
            execute_values(cur, """insert into cve_metadata
                                (cve, description, impact_id, public_date, modified_date,
                                cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                secondary_url, advisories_list)
                                values %s""", to_insert, page_size=len(to_insert))

    @staticmethod
    def update_cves(cur, to_update):
        """Update already present CVEs in DB"""
        if to_update:
            execute_values(cur, """update cve_metadata set description = data.description,
                                    impact_id = data.impact_id,
                                    public_date = cast(data.public_date as timestamp with time zone),
                                    modified_date = cast(data.modified_date as timestamp with time zone),
                                    cvss3_score = cast(data.cvss3_score as numeric),
                                    cvss3_metrics = data.cvss3_metrics,
                                    cvss2_score = cast(data.cvss2_score as numeric),
                                    cvss2_metrics = data.cvss2_metrics,
                                    redhat_url = data.redhat_url,
                                    secondary_url = data.secondary_url,
                                    advisories_list = cast(data.advisories_list as JSONB)
                                    from (values %s) as data
                                    (cve, description, impact_id, public_date, modified_date,
                                    cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                    secondary_url, advisories_list)
                                    where cve_metadata.cve = data.cve""",
                           to_update, page_size=len(to_update))


class HealthHandler(BaseView):
    """Handler class providing health status."""

    async def get(self):
        """Answer GET request."""
        return web.Response()


class SyncHandler(BaseView):
    """Convenient API performing sync on demand."""

    async def put(self):
        """Answer PUT request."""
        if SYNC_LOCK.locked():
            LOGGER.warning('Sync is still running.')
        else:
            asyncio.ensure_future(SyncHandler.a_sync_cve_md())
        return web.Response()

    @staticmethod
    def sync_cve_md():
        """Sync all CVE metadata from VMaaS"""
        LOGGER.info('Syncing CVE metadata')
        with DatabasePoolConnection() as conn:
            with conn.cursor() as cur:
                impact_id_map = {}
                cur.execute("select name, id from cve_impact")
                for impact_name, impact_id in cur.fetchall():
                    impact_id_map[impact_name] = impact_id
                cur.execute('select id, cve from cve_metadata')
                cves_in_db = {}
                for cve_tuple in cur.fetchall():
                    cves_in_db[cve_tuple[1]] = cve_tuple[0]
                cve_json = {'cve_list': [".*"], 'page': 1, 'page_size': DEFAULT_PAGE_SIZE, 'rh_only': True, 'errata_associated': True}
                success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
                if not success:
                    return success
                cves = cve_pages['cve_list']
                LOGGER.info("Importing CVE metadata")

                to_insert, to_update, to_delete = SyncHandler.process_cve_list(cves, cves_in_db, impact_id_map)

                SyncHandler.insert_cves(cur, to_insert)
                SyncHandler.update_cves(cur, to_update)

                if to_delete:
                    associated_cves = set()
                    LOGGER.info("Deleting %s unnecessary CVE metadata", len(to_delete))
                    cur.execute("""select distinct cve_id from system_vulnerabilities""")
                    for row in cur.fetchall():
                        associated_cves.add(row[0])
                    cur.execute("""select distinct cve_id from cve_rule_mapping""")
                    for row in cur.fetchall():
                        associated_cves.add(row[0])
                    safety_delete = []
                    unable_to_delete = []
                    for cve_to_delete in to_delete:
                        cve_id = cves_in_db[cve_to_delete[0]]
                        if cve_id in associated_cves:
                            unable_to_delete.append(cve_to_delete[0])
                        else:
                            safety_delete.append(cve_id)
                    if safety_delete:
                        execute_values(cur, """delete from cve_account_data
                                               where cve_id in (%s)""",
                                       list(zip(safety_delete)), page_size=len(safety_delete))
                        execute_values(cur, """delete from cve_metadata where id in (%s)""",
                                       list(zip(safety_delete)), page_size=len(safety_delete))
                        LOGGER.info('Finished deleting unnecessary CVE metadata')
                    if unable_to_delete:
                        LOGGER.warning(
                            'Unable to delete %s cves (still referenced from system_vulnerabilities table or have rules): %s',
                            len(unable_to_delete), str(unable_to_delete))
                        LOGGER.debug('Attempting to update information about %s', str(unable_to_delete))
                        cve_json = {'cve_list': unable_to_delete, 'page': 1, 'page_size': DEFAULT_PAGE_SIZE, 'rh_only': True}
                        success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
                        if not success:
                            return success
                        cves = cve_pages['cve_list']
                        _, to_update, _ = SyncHandler.process_cve_list(cves, cves_in_db, impact_id_map)
                        SyncHandler.update_cves(cur, to_update)

                conn.commit()
                LOGGER.info('Finished syncing CVE metadata')
                return success

    @staticmethod
    def sync_exploits():
        """Sync exploits data from ProdSec"""
        LOGGER.info('Syncing CVE Exploits metadata')

        headers = format_headers()

        cert = create_cert()
        data = external_service_request(BO_PROXY_ENDPOINT_EXPLOITS, method='GET', headers=headers, verify=cert.name)
        os.unlink(cert.name)

        cves_with_exploits = set()
        if data is not None:
            for item in data:
                cves = item.get('cves', [])
                cves_with_exploits.update(cves)
        else:
            LOGGER.warning('Cannot sync exploits data')
            return

        cves_wo_exploits = set()
        with DatabasePoolConnection() as conn:
            with conn.cursor() as cur:
                # Get all cve with exploits from our DB
                cur.execute("select cve from cve_metadata where exploits = true")
                for row in cur.fetchall():
                    cve = row[0]
                    # Check if CVE with exploits=true not present in ProdSec response
                    if cve not in cves_with_exploits:
                        # if false add it to list to change the status
                        cves_wo_exploits.add(cve)

                if cves_with_exploits:
                    execute_values(cur, "update cve_metadata set exploits = true where cve in (%s)", list(zip(cves_with_exploits)),
                                   page_size=len(cves_with_exploits))
                if cves_wo_exploits:
                    execute_values(cur, "update cve_metadata set exploits = false where cve in (%s)", list(zip(cves_wo_exploits)),
                                   page_size=len(cves_wo_exploits))
                conn.commit()
                LOGGER.info('Finished syncing CVE Exploits metadata')

    @staticmethod
    async def a_sync_cve_md():
        """async wrapper for sync_cve_md and sync_exploits"""
        with await SYNC_LOCK:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, SyncHandler.sync_cve_md)
            await loop.run_in_executor(None, SyncHandler.sync_exploits)


class ReEvaluateHandler(BaseView):
    """Convenient API to schedule re-evaluation for all systems."""

    async def put(self):
        """Answer PUT request."""
        asyncio.ensure_future(ReEvaluateHandler.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
        return web.Response()

    @classmethod
    async def re_evaluate_systems(cls, repo_based: bool):
        """Schedule re-evaluation for all systems in DB."""
        with DatabasePoolConnection() as conn:
            if repo_based:
                updated_repos = cls._get_updated_repos(conn)

            with NamedCursor(conn) as cur:
                if repo_based:
                    LOGGER.info("Re-evaluating in repo-based mode")
                    cls.select_repo_based_inventory_ids(cur, updated_repos)
                else:
                    LOGGER.info("Re-evaluating all systems")
                    cls.select_all_inventory_ids(cur)
                total_scheduled = 0
                while True:
                    await RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.acquire()
                    rows = cur.fetchmany(size=RE_EVALUATION_KAFKA_BATCH_SIZE)
                    if not rows:
                        RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release()
                        break
                    msgs = [{"type": "re-evaluate_system", "host": {"id": inventory_id}} for inventory_id, in rows]
                    total_scheduled += len(msgs)
                    future = VmaasSyncContext.evaluator_queue.send_list(msgs)
                    future.add_done_callback(lambda x: RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release())
                LOGGER.info("%s systems scheduled for re-evaluation", total_scheduled)
            conn.commit()

    @classmethod
    def _get_updated_repos(cls, conn) -> list:
        """Get repos updated since last repo-based evaluation"""
        with conn.cursor() as cur:
            modified_since_dt = cls.get_last_repobased_eval_tms(cur)
            # last modified timestamp
            if modified_since_dt is None:
                modified_since_dt = dt.datetime.utcfromtimestamp(0).replace(tzinfo=dt.timezone.utc)
                # modified time is current time
            repos = cls._vmaas_repos_modified_since(modified_since_dt.isoformat())
            # list of modified repos
            cls.set_last_repobased_eval_tms(cur, dt.datetime.now())
            return repos


class DeleteHandler(BaseView):
    """Convenient API to delete CVEs"""

    async def delete(self):
        """Answer DELETE request"""
        res, msg = self._delete_cve(self.request.query.get("cve_names", ""))
        return web.Response(status=(200 if res else 400),
                            body=(msg if res != 200 else None))

    def _delete_cve(self, cve_names: str) -> bool:  # pylint: disable=no-self-use
        """Delete CVEs"""
        if not cve_names:
            LOGGER.info("Need to specify CVE")
            return False, "Need to specify CVE"
        cve_list = cve_names.split(',')
        LOGGER.info('Deleting %s CVE metadata', len(cve_list))
        with DatabasePoolConnection() as conn:
            with conn.cursor() as cur:
                success = True
                execute_values(cur, """select id from cve_metadata where cve in (%s)""",
                               list(zip(cve_list)), page_size=len(cve_list))
                cve_ids_to_delete = cur.fetchall()
                if cve_ids_to_delete:
                    execute_values(cur, """delete from cve_account_data where cve_id in (%s)""",
                                   cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                    execute_values(cur, """delete from system_vulnerabilities where cve_id in (%s)""",
                                   cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                    execute_values(cur, """delete from cve_metadata where id in (%s)""",
                                   cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                conn.commit()
        LOGGER.info('Finished deleting CVE metadata')
        return success, None


class ExploitsHandler(BaseView):
    """Convenient API to sync exploits"""

    async def put(self):
        """Answer PUT request"""
        SyncHandler.sync_exploits()
        return web.Response()


class VmaasSyncContext:
    """Websocket client application."""

    vmaas_websocket = None
    reconnect_callback = None
    evaluator_queue = None
    vmaas_websocket_url = None
    instance = None
    session = None
    websocket_task = None

    def __init__(self):
        self.app = web.Application(logger=LOGGER)

        router = self.app.router
        router.add_view(r"/healthz", HealthHandler)
        router.add_view(r"/api/v1/sync", SyncHandler)
        router.add_view(r"/api/v1/re-evaluate", ReEvaluateHandler)
        router.add_view(r"/api/v1/cves", DeleteHandler)
        router.add_view(r"/api/v1/exploits", ExploitsHandler)

        self.app.on_startup.append(self.start)
        self.app.on_shutdown.append(self.stop)
        self.app.on_cleanup.append(self.stop)

        self.instance = asyncio.get_event_loop()
        self.vmaas_websocket_url = "ws://%s/" % os.getenv("VMAAS_WEBSOCKET_HOST", "vmaas_websocket:8082")
        self.session = ClientSession()

    async def start(self, _):
        """Start websocket server."""
        # Sync CVEs always when app starts
        self.evaluator_queue = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)
        SyncHandler.sync_cve_md()
        SyncHandler.sync_exploits()
        self.websocket_task = asyncio.ensure_future(self._websocket_loop())

    async def stop(self, _):
        """Stop websocket server."""
        await self.evaluator_queue.stop()

        if self.websocket_task is not None or not self.websocket_task.done():
            self.websocket_task.cancel()

        if self.vmaas_websocket is not None or not self.vmaas_websocket.closed:
            await self.vmaas_websocket.close()
            self.vmaas_websocket = None
            LOGGER.info("Websocket connection closed.")

        await self.session.close()
        self.instance.stop()

    async def _websocket_reconnect(self):
        """Reconnect websocket if its not connected.."""
        if self.vmaas_websocket is None or self.vmaas_websocket.closed:
            CNX_RECONNECT.inc()
            while True:
                try:
                    self.vmaas_websocket = await self.session.ws_connect(self.vmaas_websocket_url)
                    LOGGER.info("Connected to: %s", self.vmaas_websocket_url)
                    await self.vmaas_websocket.send_str("subscribe-listener")
                    break
                except Exception as exc:  # noqa: E722 pylint: disable=broad-except
                    LOGGER.warning("Unable to connect to: %s, traceback: %s", self.vmaas_websocket_url, exc)
                    CNX_FAIL.inc()
                    self.vmaas_websocket = None
                    await asyncio.sleep(2)

    async def _websocket_loop(self):
        """Websocket reciever and handler loop."""
        while True:
            await self._websocket_reconnect()

            async for msg in self.vmaas_websocket:
                if msg.type == WSMsgType.TEXT and msg.data == "webapps-refreshed":
                    REFRESH.inc()
                    LOGGER.info("VMaaS cache refreshed")
                    SyncHandler.sync_cve_md()
                    SyncHandler.sync_exploits()
                    if ENABLE_RE_EVALUATION:
                        asyncio.ensure_future(ReEvaluateHandler.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
                    else:
                        LOGGER.info("Re-evaluation is disabled, skipping")
                else:
                    CNX_FAIL.inc()
                    LOGGER.warning("Connection to %s closed: %s", self.vmaas_websocket_url, self.vmaas_websocket.close_code)
                    if not self.vmaas_websocket.closed:
                        self.vmaas_websocket.close()
                    self.vmaas_websocket = None


def main():
    """Main VMaaS listener entrypoint."""
    init_logging()

    loop = asyncio.get_event_loop()
    status_app = create_status_app(LOGGER)
    _, status_site = create_status_runner(status_app, int(PROMETHEUS_PORT), LOGGER, loop)
    loop.run_until_complete(status_site.start())

    loop.run_until_complete(a_ensure_minimal_schema_version())

    LOGGER.info("Starting VMaaS sync service.")
    with DatabasePool(1):
        app_cont = VmaasSyncContext()

        def terminate(*_):
            """Trigger shutdown."""
            LOGGER.info("Signal received, stopping application.")
            loop.add_callback_from_signal(app_cont.app.shutdown)

        signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
        for sig in signals:
            signal.signal(sig, terminate)

        web.run_app(app_cont.app, port=8000)

    LOGGER.info("Shutting down.")


if __name__ == '__main__':
    main()
