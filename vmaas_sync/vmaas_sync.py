"""VMaaS sync module."""
import asyncio
import datetime as dt
import json
import re
from datetime import datetime
from datetime import timezone
from typing import Any
from typing import Dict
from typing import Tuple

import requests
from dateutil import parser
from psycopg2.extras import execute_values

from common import constants
from common import mqueue
from common import utils
from common.config import Config
from common.constants import VMAAS_CVES_ENDPOINT
from common.constants import VMAAS_REPOS_ENDPOINT
from common.database_handler import DatabasePool
from common.database_handler import DatabasePoolConnection
from common.database_handler import NamedCursor
from common.logging import get_logger
from common.logging import init_logging
from common.utils import construct_cve_row
from common.utils import ensure_minimal_schema_version

CFG = Config()
LOGGER = get_logger(__name__)

EVALUATOR_QUEUE = mqueue.MQWriter(CFG.evaluator_recalc_topic)
BATCH_SEMAPHORE = asyncio.BoundedSemaphore(CFG.re_evaluation_kafka_batches)
PACKAGE_RE = re.compile(r"(([0-9]+):)?(?P<pn>[^:]+)-.*[^-:]+-.*")


def _get_last_repobased_eval_tms(cur):
    """Select last repo-based evaluation timestamp."""
    cur.execute("select value from timestamp_kv where name = %s", (constants.TIMESTAMP_LAST_REPO_BASED_EVAL,))
    ret = cur.fetchone()
    if ret:
        return ret[0]
    return None


def _set_last_repobased_eval_tms(cur, timestamp: dt.datetime):
    """Update last repo-based evaluation timestamp."""
    cur.execute(
        """insert into timestamp_kv (name, value) values (%s, %s)
                on conflict (name) do update set value = %s returning value, (xmax = 0) as inserted""",
        (constants.TIMESTAMP_LAST_REPO_BASED_EVAL, timestamp, timestamp),
    )
    ret = cur.fetchone()
    return ret


def _get_updated_repos(conn) -> dict[str, list[str]]:
    """
    Get repos updated since last repo-based evaluation
    { repo_name: [updated_package1, updated_package2, ...], repo_name2: ... }
    """
    with conn.cursor() as cur:
        modified_since_dt = _get_last_repobased_eval_tms(cur)
        # last modified timestamp
        if modified_since_dt is None:
            modified_since_dt = dt.datetime.utcfromtimestamp(0).replace(tzinfo=dt.timezone.utc)
            # modified time is current time
        repos = _fetch_vmaas_repos(modified_since_dt.isoformat())
        if not repos:
            return []
        # list of modified repos
        if repos["latest_repo_change"]:
            _set_last_repobased_eval_tms(cur, repos["latest_repo_change"])
        return repos["repos"]


def _select_repo_based_inventory_ids(cur, repos: list):
    """Select inventory-ids connected with inserted repos, don't fetch it."""
    if repos:
        cur.execute(
            """SELECT repo.name AS repo_name, sp.inventory_id, ra.org_id, vmaas_json
                FROM system_platform AS sp
                JOIN rh_account AS ra ON sp.rh_account_id = ra.id
                JOIN system_repo sr ON sp.id = sr.system_id
                JOIN repo ON repo.id = sr.repo_id
                WHERE sp.when_deleted IS NULL
                AND sp.id IN (
                    SELECT DISTINCT system_id
                    FROM system_repo
                    WHERE repo_id IN (
                        SELECT id
                        FROM repo
                        WHERE name IN %s
                    )
                )
                ORDER BY sp.rh_account_id;
                """,
            (tuple(repos),),
        )
    else:
        cur.execute("""select * from system_repo where (1=0)""")  # ensure empty result


def _select_all_inventory_ids(cur):
    """Select all inventory-ids, don't fetch it."""
    cur.execute(
        """SELECT NULL AS repo_name, sp.inventory_id, ra.org_id, NULL AS vmaas_json
            FROM system_platform AS sp
            JOIN rh_account AS ra ON sp.rh_account_id = ra.id
            WHERE sp.when_deleted IS NULL
            ORDER BY sp.rh_account_id"""
    )


def _process_cve_list(cves, cves_in_db, impact_id_map):
    """Processes cve list returned from VMaaS"""
    to_insert_update = [construct_cve_row(cves[cve], impact_id_map) for cve in sorted(cves)]
    to_delete = [(cve,) for cve in cves_in_db if cve not in sorted(cves)]
    return to_insert_update, to_delete


def _insert_update_cves(cur, to_insert_update):
    """Insert or update CVEs in the DB"""
    if to_insert_update:
        execute_values(
            cur,
            """insert into cve_metadata
                            (cve, description, impact_id, public_date, modified_date,
                            cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                            secondary_url, advisories_list)
                            values %s on conflict (cve) do update set
                                description = excluded.description,
                                impact_id = excluded.impact_id,
                                public_date = excluded.public_date,
                                modified_date = excluded.modified_date,
                                cvss3_score = excluded.cvss3_score,
                                cvss3_metrics = excluded.cvss3_metrics,
                                cvss2_score = excluded.cvss2_score,
                                cvss2_metrics = excluded.cvss2_metrics,
                                redhat_url = excluded.redhat_url,
                                secondary_url = excluded.secondary_url,
                                advisories_list = excluded.advisories_list""",
            to_insert_update,
            page_size=len(to_insert_update),
        )


def _fetch_vmaas_pages(endpoint: str, request_data: dict, method: str) -> Tuple[bool, Any]:
    """Fetches VMaaS paged endpoint, and returns all page responses in list determined by the settings
    sent in request_data. Expects keys in requests such as 'page_size', 'page'.
    And expects keys in response such as 'pages'."""
    session = requests.Session()
    result = []

    LOGGER.info("Fetching page 1 (size: %s) from %s", request_data["page_size"], endpoint)
    req_res = utils.external_service_request(endpoint, request_data, session, method, verify=CFG.tls_ca_path)
    if req_res is None:
        LOGGER.error("Fetching page 1 from %s failed", endpoint)
        return False, result

    result.append(req_res)

    max_pages = req_res.get("pages")
    if max_pages is None:
        LOGGER.error("Fetching page 1 (size: %s) from %s failed, 'pages' key not found", request_data["page_size"], endpoint)
        return False, result

    for page in range(1, max_pages):
        request_data["page"] += 1
        LOGGER.info("Fetching page %s/%s (size: %s) from %s", request_data["page"], max_pages, request_data["page_size"], endpoint)
        req_res = utils.external_service_request(endpoint, request_data, session, method, verify=CFG.tls_ca_path)
        if req_res is None:
            LOGGER.error(
                "Fetching page %s/%s (size: %s) from %s failed", request_data["page"], max_pages, request_data["page_size"], endpoint
            )
            return False, []
        result.append(req_res)
    return True, result


def _fetch_vmaas_cves() -> Dict[str, Any]:
    """Fetch every CVE from VMaaS"""
    req = {"cve_list": [".*"], "page": 1, "page_size": CFG.default_page_size, "rh_only": True}
    success, cve_pages = _fetch_vmaas_pages(VMAAS_CVES_ENDPOINT, req, "POST")
    if not success:
        return {}

    cves = {}
    result = {}
    for page in cve_pages:
        cves.update(page["cve_list"])
        result["last_change"] = page["last_change"]
    result["cves"] = cves

    LOGGER.info("Fetched %s CVEs from VMaaS", len(cves))
    return result


def _extract_updated_packages(page) -> dict[str, list[str]]:
    result = {}
    for repo_name, value in page["repository_list"].items():
        updated_packages = [name for pkgs in value for name in pkgs.get("updated_package_names", [])]
        result[repo_name] = updated_packages
    return result


def _fetch_vmaas_repos(modified_since: str) -> Dict[str, Any]:
    """Fetch repositories from VMaaS, based on the modification timestamp stored in DB"""
    req = {
        "repository_list": [".*"],
        "page": 1,
        "page_size": CFG.default_page_size,
        "modified_since": modified_since,
        "show_packages": True,
    }
    success, repos_pages = _fetch_vmaas_pages(VMAAS_REPOS_ENDPOINT, req, "POST")
    if not success:
        return {}

    result = {"latest_repo_change": None}
    repos = {}
    for page in repos_pages:
        repos |= _extract_updated_packages(page)
        result["last_change"] = page["last_change"]
        if page["latest_repo_change"]:
            ts = parser.parse(page["latest_repo_change"])
            if result["latest_repo_change"] is None or result["latest_repo_change"] < ts:
                result["latest_repo_change"] = ts
    result["repos"] = repos
    LOGGER.info("Fetched %d repos, updated since %s", len(repos), modified_since)
    return result


def sync_cve_md():
    """Sync all CVE metadata from VMaaS"""
    LOGGER.info("Syncing CVE metadata")
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            impact_id_map = {}
            cur.execute("select name, id from cve_impact")
            for impact_name, impact_id in cur.fetchall():
                impact_id_map[impact_name] = impact_id

            cur.execute("select id, cve from cve_metadata")
            cves_in_db = {}
            for cve_tuple in cur.fetchall():
                cves_in_db[cve_tuple[1]] = cve_tuple[0]

            cve_pages = _fetch_vmaas_cves()
            if not cve_pages:
                LOGGER.info("Fetched 0 CVEs from VMaaS, skipping CVE sync")
                return False

            cves = cve_pages["cves"]
            LOGGER.info("Importing CVE metadata")

            to_insert_update, to_delete = _process_cve_list(cves, cves_in_db, impact_id_map)

            _insert_update_cves(cur, to_insert_update)

            if to_delete:
                associated_cves = set()
                LOGGER.info("Deleting %s unnecessary CVE metadata", len(to_delete))
                cur.execute("""select distinct cve_id from system_vulnerabilities""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                cur.execute("""select distinct cve_id from cve_rule_mapping""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                cur.execute("""select distinct cve_id from vulnerable_package_cve""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                safety_delete = []
                unable_to_delete = []
                for cve_to_delete in to_delete:
                    cve_id = cves_in_db[cve_to_delete[0]]
                    if cve_id in associated_cves:
                        unable_to_delete.append(cve_to_delete[0])
                    else:
                        safety_delete.append(cve_id)
                if safety_delete:
                    execute_values(
                        cur,
                        """delete from cve_account_cache
                                            where cve_id in (%s)""",
                        list(zip(safety_delete)),
                        page_size=len(safety_delete),
                    )
                    execute_values(
                        cur,
                        """delete from cve_account_data
                                            where cve_id in (%s)""",
                        list(zip(safety_delete)),
                        page_size=len(safety_delete),
                    )
                    execute_values(
                        cur,
                        """delete from system_cve_data
                                            where cve_id in (%s)""",
                        list(zip(safety_delete)),
                        page_size=len(safety_delete),
                    )
                    execute_values(
                        cur, """delete from cve_metadata where id in (%s)""", list(zip(safety_delete)), page_size=len(safety_delete)
                    )
                    LOGGER.info("Finished deleting unnecessary CVE metadata")
                if unable_to_delete:
                    LOGGER.warning(
                        "Unable to delete %s cves (still referenced from other tables): %s", len(unable_to_delete), str(unable_to_delete)
                    )

            conn.commit()
            LOGGER.info("Finished syncing CVE metadata")
            return True


def _create_message_list(rows: list[tuple[str, str, str, str]]):
    return [
        {
            "type": "re-evaluate_system",
            "host": {"id": inventory_id, "org_id": org_id},
            "timestamp": str(datetime.now(timezone.utc)),
        }
        for _, inventory_id, org_id, _ in rows
    ]


def _prepare_messages(updated_repos: dict[str, list[str]], rows: list[tuple[str, str, str, str]], repo_based: bool):
    """
    row: (repo_name, inventory_id, org_id, vmaas_json)
    """
    if not repo_based:
        return _create_message_list(rows)

    systems_to_reevaluate = []
    for repo_name, inventory_id, org_id, vmaas_json in rows:
        system_packages = json.loads(vmaas_json).get("package_list", [])
        system_packages = set(map(lambda x: PACKAGE_RE.match(x).group("pn"), system_packages))
        #  intersection exists (there are some pkgs which changed) => reevaluate system
        if set(updated_repos[repo_name]) & system_packages:
            systems_to_reevaluate.append((None, inventory_id, org_id, None))

    return _create_message_list(systems_to_reevaluate)


def re_evaluate_systems():
    """Schedule re-evaluation for systems in DB."""
    with DatabasePoolConnection() as conn:
        updated_repos = []
        if CFG.enable_repo_based_re_evaluation:
            updated_repos: dict[str, list[str]] = _get_updated_repos(conn)
            if not updated_repos:
                LOGGER.info("Fetched 0 repos from VMaaS, skipping repo-based re-evaluation")
                return

        with NamedCursor(conn) as cur:
            if CFG.enable_repo_based_re_evaluation:
                LOGGER.info("Re-evaluating in repo-based mode")
                _select_repo_based_inventory_ids(cur, updated_repos.keys())
            else:
                LOGGER.info("Re-evaluating all systems")
                _select_all_inventory_ids(cur)
            loop = asyncio.get_event_loop()
            total_scheduled = 0
            while True:
                loop.run_until_complete(BATCH_SEMAPHORE.acquire())
                rows = cur.fetchmany(size=CFG.re_evaluation_kafka_batch_size)
                if not rows:
                    BATCH_SEMAPHORE.release()
                    break
                msgs = _prepare_messages(updated_repos, rows, CFG.enable_repo_based_re_evaluation)
                total_scheduled += len(msgs)
                future = EVALUATOR_QUEUE.send_list(msgs, loop=loop)
                future.add_done_callback(lambda x: BATCH_SEMAPHORE.release())
            LOGGER.info("%s systems scheduled for re-evaluation", total_scheduled)
        conn.commit()


def main():
    """Main VMaaS sync entrypoint."""
    init_logging()
    ensure_minimal_schema_version()
    LOGGER.info("Starting VMaaS sync.")
    with DatabasePool(1):
        sync_cve_md()
        re_evaluate_systems()
    asyncio.get_event_loop().run_until_complete(EVALUATOR_QUEUE.stop())
    LOGGER.info("VMaaS sync finished.")


if __name__ == "__main__":
    main()
