"""VMaaS sync module."""

import asyncio
import datetime as dt

from psycopg2.extras import execute_values

from common import constants, mqueue
from common.config import Config
from common.constants import VMAAS_CVES_ENDPOINT, VMAAS_REPOS_ENDPOINT
from common.database_handler import (DatabasePool, DatabasePoolConnection,
                                     NamedCursor)
from common.logging import get_logger, init_logging
from common.paging import paging
from common.utils import construct_cve_row, ensure_minimal_schema_version

CFG = Config()
LOGGER = get_logger(__name__)

EVALUATOR_QUEUE = mqueue.MQWriter(CFG.evaluator_recalc_topic)
BATCH_SEMAPHORE = asyncio.BoundedSemaphore(CFG.re_evaluation_kafka_batches)


def _get_last_repobased_eval_tms(cur):
    """Select last repo-based evaluation timestamp."""
    cur.execute("select value from timestamp_kv where name = %s", (constants.TIMESTAMP_LAST_REPO_BASED_EVAL,))
    ret = cur.fetchone()
    if ret:
        return ret[0]
    return None


def _vmaas_repos_modified_since(modified_since: str) -> list:
    """Get list of modified repose since `modified since`"""
    repos_json = {"repository_list": [".*"], "page": 1, "page_size": CFG.default_page_size,
                  "modified_since": modified_since}
    success, repos_pages = paging(VMAAS_REPOS_ENDPOINT, repos_json)
    if not success:
        return []
    repos = list(repos_pages["repository_list"])
    LOGGER.info("%d repos found updated since %s", len(repos), modified_since)
    return repos


def _set_last_repobased_eval_tms(cur, timestamp: dt.datetime):
    """Update last repo-based evaluation timestamp."""
    cur.execute("""insert into timestamp_kv (name, value) values (%s, %s)
                on conflict (name) do update set value = %s returning value, (xmax = 0) as inserted""",
                (constants.TIMESTAMP_LAST_REPO_BASED_EVAL, timestamp, timestamp))
    ret = cur.fetchone()
    return ret


def _get_updated_repos(conn) -> list:
    """Get repos updated since last repo-based evaluation"""
    with conn.cursor() as cur:
        modified_since_dt = _get_last_repobased_eval_tms(cur)
        # last modified timestamp
        if modified_since_dt is None:
            modified_since_dt = dt.datetime.utcfromtimestamp(0).replace(tzinfo=dt.timezone.utc)
            # modified time is current time
        repos = _vmaas_repos_modified_since(modified_since_dt.isoformat())
        # list of modified repos
        _set_last_repobased_eval_tms(cur, dt.datetime.now())
        return repos


def _select_repo_based_inventory_ids(cur, repos: list):
    """Select inventory-ids connected with inserted repos, don't fetch it."""
    if repos:
        cur.execute("""select sp.inventory_id, ra.account_number, ra.org_id from system_platform as sp
                        join rh_account as ra on sp.rh_account_id = ra.id
                        where sp.when_deleted is null and
                        sp.id in (select distinct system_id from system_repo where repo_id in
                        (select id from repo where name in %s)) order by sp.rh_account_id""", (tuple(repos),))
    else:
        cur.execute("""select * from system_repo where (1=0)""")  # ensure empty result


def _select_all_inventory_ids(cur):
    """Select all inventory-ids, don't fetch it."""
    cur.execute("""select sp.inventory_id, ra.account_number, ra.org_id from system_platform as sp
                    join rh_account as ra on sp.rh_account_id = ra.id
                    where sp.when_deleted is null order by sp.rh_account_id""")


def _process_cve_list(cves, cves_in_db, impact_id_map):
    """Processes cve list returned from VMaaS"""
    to_insert_update = [construct_cve_row(cves[cve], impact_id_map) for cve in sorted(cves)]
    to_delete = [(cve,) for cve in cves_in_db if cve not in sorted(cves)]
    return to_insert_update, to_delete


def _insert_update_cves(cur, to_insert_update):
    """Insert or update CVEs in the DB"""
    if to_insert_update:
        execute_values(cur, """insert into cve_metadata
                            (cve, description, impact_id, public_date, modified_date,
                            cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                            secondary_url, advisories_list)
                            values %s on conflict (cve) do update set
                                description = excluded.description,
                                impact_id = excluded.impact_id,
                                public_date = excluded.public_date,
                                modified_date = excluded.modified_date,
                                cvss3_score = excluded.cvss3_score,
                                cvss3_metrics = excluded.cvss3_metrics,
                                cvss2_score = excluded.cvss2_score,
                                cvss2_metrics = excluded.cvss2_metrics,
                                redhat_url = excluded.redhat_url,
                                secondary_url = excluded.secondary_url,
                                advisories_list = excluded.advisories_list""",
                       to_insert_update, page_size=len(to_insert_update))


def sync_cve_md():
    """Sync all CVE metadata from VMaaS"""
    LOGGER.info('Syncing CVE metadata')
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            impact_id_map = {}
            cur.execute("select name, id from cve_impact")
            for impact_name, impact_id in cur.fetchall():
                impact_id_map[impact_name] = impact_id
            cur.execute('select id, cve from cve_metadata')
            cves_in_db = {}
            for cve_tuple in cur.fetchall():
                cves_in_db[cve_tuple[1]] = cve_tuple[0]
            cve_json = {'cve_list': [".*"], 'page': 1, 'page_size': CFG.default_page_size, 'rh_only': True}
            success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
            if not success:
                return success
            cves = cve_pages['cve_list']
            LOGGER.info("Importing CVE metadata")

            to_insert_update, to_delete = _process_cve_list(cves, cves_in_db, impact_id_map)

            _insert_update_cves(cur, to_insert_update)

            if to_delete:
                associated_cves = set()
                LOGGER.info("Deleting %s unnecessary CVE metadata", len(to_delete))
                cur.execute("""select distinct cve_id from system_vulnerabilities""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                cur.execute("""select distinct cve_id from cve_rule_mapping""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                safety_delete = []
                unable_to_delete = []
                for cve_to_delete in to_delete:
                    cve_id = cves_in_db[cve_to_delete[0]]
                    if cve_id in associated_cves:
                        unable_to_delete.append(cve_to_delete[0])
                    else:
                        safety_delete.append(cve_id)
                if safety_delete:
                    execute_values(cur, """delete from cve_account_cache
                                            where cve_id in (%s)""",
                                   list(zip(safety_delete)), page_size=len(safety_delete))
                    execute_values(cur, """delete from cve_account_data
                                            where cve_id in (%s)""",
                                   list(zip(safety_delete)), page_size=len(safety_delete))
                    execute_values(cur, """delete from cve_metadata where id in (%s)""",
                                   list(zip(safety_delete)), page_size=len(safety_delete))
                    LOGGER.info('Finished deleting unnecessary CVE metadata')
                if unable_to_delete:
                    LOGGER.warning(
                        'Unable to delete %s cves (still referenced from system_vulnerabilities table or have rules): %s',
                        len(unable_to_delete), str(unable_to_delete))

            conn.commit()
            LOGGER.info('Finished syncing CVE metadata')
            return success


def re_evaluate_systems():
    """Schedule re-evaluation for systems in DB."""
    with DatabasePoolConnection() as conn:
        if CFG.enable_repo_based_re_evaluation:
            updated_repos = _get_updated_repos(conn)

        with NamedCursor(conn) as cur:
            if CFG.enable_repo_based_re_evaluation:
                LOGGER.info("Re-evaluating in repo-based mode")
                _select_repo_based_inventory_ids(cur, updated_repos)
            else:
                LOGGER.info("Re-evaluating all systems")
                _select_all_inventory_ids(cur)
            loop = asyncio.get_event_loop()
            total_scheduled = 0
            while True:
                loop.run_until_complete(BATCH_SEMAPHORE.acquire())
                rows = cur.fetchmany(size=CFG.re_evaluation_kafka_batch_size)
                if not rows:
                    BATCH_SEMAPHORE.release()
                    break
                msgs = [{"type": "re-evaluate_system", "host": {"id": inventory_id, "account": account_number, "org_id": org_id}}
                        for inventory_id, account_number, org_id in rows]
                total_scheduled += len(msgs)
                future = EVALUATOR_QUEUE.send_list(msgs, loop=loop)
                future.add_done_callback(lambda x: BATCH_SEMAPHORE.release())
            LOGGER.info("%s systems scheduled for re-evaluation", total_scheduled)
        conn.commit()


def main():
    """Main VMaaS sync entrypoint."""
    init_logging()
    ensure_minimal_schema_version()
    LOGGER.info("Starting VMaaS sync.")
    with DatabasePool(1):
        sync_cve_md()
        re_evaluate_systems()
    asyncio.get_event_loop().run_until_complete(EVALUATOR_QUEUE.stop())
    LOGGER.info("VMaaS sync finished.")


if __name__ == '__main__':
    main()
