"""VMaaS websocket listener module."""

import asyncio
import datetime as dt
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import os
import signal

from prometheus_client import Counter, start_http_server
from psycopg2.extras import execute_values

from tornado.ioloop import IOLoop, PeriodicCallback
from tornado.web import Application, RequestHandler
from tornado.websocket import websocket_connect

from common.database_handler import DatabasePool, DatabasePoolConnection, NamedCursor
from common.logging import init_logging, get_logger
from common.paging import paging
from common import mqueue, constants

WEBSOCKET_RECONNECT_INTERVAL = 60
LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8087')

REFRESH = Counter('ve_vmaas_sync_webscan_refreshes', '# of times VMaaS told us it had new data')
CNX_FAIL = Counter('ve_vmaas_sync_websocket_failures', '# of times VMaaS websocket closed on us')
CNX_RECONNECT = Counter('ve_vmaas_sync_websocket_recnx', '# of times attempted VMaaS websocket reconnect')

VMAAS_HOST = os.getenv('VMAAS_HOST', 'http://vmaas_webapp:8080')
VMAAS_CVES_ENDPOINT = "%s/api/v1/cves" % VMAAS_HOST
VMAAS_REPOS_ENDPOINT = "%s/api/v1/repos" % VMAAS_HOST
ENABLE_RE_EVALUATION = strtobool(os.getenv("ENABLE_RE_EVALUATION", "YES"))
ENABLE_REPO_BASED_RE_EVALUATION = strtobool(os.getenv("ENABLE_REPO_BASED_RE_EVALUATION", "NO"))
DEFAULT_PAGE_SIZE = int(os.getenv("DEFAULT_PAGE_SIZE", "5000"))

# how many systems to select in one batch and send to kafka
RE_EVALUATION_KAFKA_BATCH_SIZE = int(os.getenv("RE_EVALUATION_KAFKA_BATCH_SIZE", "10000"))
# how many batches to create at the same time
RE_EVALUATION_KAFKA_BATCHES = int(os.getenv("RE_EVALUATION_KAFKA_BATCHES", "10"))
RE_EVALUATION_KAFKA_BATCH_SEMAPHORE = asyncio.BoundedSemaphore(RE_EVALUATION_KAFKA_BATCHES)


def sync_cve_md(webhook_queue):  # pylint: disable=too-many-branches, too-many-statements
    """Sync all CVE metadata from VMaaS"""
    LOGGER.info('Syncing CVE metadata')
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            impact_id_map = {}
            cur.execute("select name, id from cve_impact")
            for impact_name, impact_id in cur.fetchall():
                impact_id_map[impact_name] = impact_id
            cur.execute('select cve from cve_metadata')
            cves_in_db = []
            for cve_tuple in cur.fetchall():
                cves_in_db.append(cve_tuple[0])
            cvss_buckets = {'0to3': 0, '3to7': 0, '7to10': 0}
            cve_json = {'cve_list': [".*"], 'page': 1, 'page_size': DEFAULT_PAGE_SIZE, 'rh_only': True}
            success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
            if not success:
                return success
            cves = cve_pages['cve_list']
            LOGGER.info("Importing CVE metadata")
            to_insert = []
            to_update = []
            to_delete = []
            for cve in cves:
                description = cves[cve]['description']
                impact_id = impact_id_map[cves[cve]['impact']]
                public_date = cves[cve]['public_date'] or None
                modified_date = cves[cve]['modified_date'] or None
                cvss3_score = float(cves[cve]['cvss3_score']) if cves[cve].get('cvss3_score') else None
                cvss3_metrics = cves[cve].get('cvss3_metrics')
                cvss2_score = float(cves[cve]['cvss2_score']) if cves[cve].get('cvss2_score') else None
                cvss2_metrics = cves[cve].get('cvss2_metrics')
                redhat_url = cves[cve].get('redhat_url', None)
                secondary_url = cves[cve].get('secondary_url', None)
                row = (cve, description, impact_id, public_date, modified_date, cvss3_score, cvss3_metrics,
                       cvss2_score, cvss2_metrics, redhat_url, secondary_url)
                if cve not in cves_in_db:
                    to_insert.append(row)
                    cvss_score = cvss3_score if cvss3_score is not None else cvss2_score
                    if cvss_score is None:
                        pass
                    elif cvss_score < 3:
                        cvss_buckets['0to3'] += 1
                    elif 3 <= cvss_score < 7:
                        cvss_buckets['3to7'] += 1
                    elif 7 <= cvss_score <= 10:
                        cvss_buckets['7to10'] += 1
                else:
                    to_update.append(row)
            to_delete = [(cve,) for cve in cves_in_db if cve not in cves]
            if to_insert:
                execute_values(cur, """insert into cve_metadata
                                        (cve, description, impact_id, public_date, modified_date,
                                        cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                        secondary_url)
                                        values %s""", to_insert, page_size=len(to_insert))
            if to_update:
                execute_values(cur, """update cve_metadata set description = data.description,
                                        impact_id = data.impact_id,
                                        public_date = cast(data.public_date as timestamp with time zone),
                                        modified_date = cast(data.modified_date as timestamp with time zone),
                                        cvss3_score = cast(data.cvss3_score as numeric),
                                        cvss3_metrics = data.cvss3_metrics,
                                        cvss2_score = cast(data.cvss2_score as numeric),
                                        cvss2_metrics = data.cvss2_metrics,
                                        redhat_url = data.redhat_url,
                                        secondary_url = data.secondary_url
                                        from (values %s) as data
                                        (cve, description, impact_id, public_date, modified_date,
                                        cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                        secondary_url)
                                        where cve_metadata.cve = data.cve""",
                               to_update, page_size=len(to_update))
            if to_delete:
                LOGGER.info("Deleting %s unnecessary CVE metadata", len(to_delete))
                execute_values(cur, """select cm.cve from cve_metadata cm
                                       where not exists(select sv.cve_id from system_vulnerabilities sv
                                       where cm.id = sv.cve_id)
                                       and cm.cve in (%s)""",
                               to_delete, page_size=len(to_delete))
                safety_delete = [cve_name_tuple[0] for cve_name_tuple in cur.fetchall()]
                unable_to_delete = [cve for cve in zip(*to_delete).__next__() if cve not in safety_delete]
                if safety_delete:
                    execute_values(cur, """delete from cve_account_data
                                           where cve_id in (select id from cve_metadata
                                           where cve in (%s))""",
                                   [i for i in zip(safety_delete)], page_size=len(safety_delete))
                    execute_values(cur, """delete from cve_metadata where cve in (%s)""",
                                   [i for i in zip(safety_delete)], page_size=len(safety_delete))
                    LOGGER.info('Finished deleting unnecessary CVE metadata')
                if unable_to_delete:
                    LOGGER.warning(
                        'Unable to delete %s cves (still referenced from system_vulnerabilities table): %s',
                        len(unable_to_delete), str(unable_to_delete))

            LOGGER.info("Finished importing CVE metadata (page: %s, page_size: %s, pages: %s)",
                        cve_pages['page'], cve_pages['page_size'], cve_pages['pages'])
            msgs = []
            time = dt.datetime.utcnow().isoformat()
            for level in cvss_buckets:
                if cvss_buckets[level] > 0:
                    msg = {
                        'application': 'vulnerability',
                        'event_type': 'new-cve',
                        'level': level,
                        'message': 'Discovered %s new CVEs with cvss score within %s radius' % (cvss_buckets[level],
                                                                                                level),
                        'timestamp': time,
                    }
                    msgs.append(msg)
            if msgs:
                cur.execute('select name from rh_account')
                for rh_account in cur.fetchall():
                    for msg in msgs:
                        msg.update({'account_id': rh_account[0]})
                        webhook_queue.send(msg)
            conn.commit()
            LOGGER.info('Finished syncing CVE metadata')
            return success


def delete_cve(cve_names: str) -> bool:
    """Delete CVEs"""
    if not cve_names:
        LOGGER.info('Need to specify CVE')
        return False
    cve_list = cve_names.split(',')
    LOGGER.info('Deleting %s CVE metadata', len(cve_list))
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            success = True
            execute_values(cur, """select id from cve_metadata where cve in (%s)""",
                           [cve for cve in zip(cve_list)], page_size=len(cve_list))
            cve_ids_to_delete = cur.fetchall()
            if cve_ids_to_delete:
                execute_values(cur, """delete from cve_account_data where cve_id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                execute_values(cur, """delete from system_vulnerabilities where cve_id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                execute_values(cur, """delete from cve_metadata where id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
            conn.commit()
    LOGGER.info('Finished deleting CVE metadata')
    return success


class HealthHandler(RequestHandler):
    """Handler class providing health status."""

    def data_received(self, chunk):
        pass

    def get(self):
        """Answer GET request."""
        self.finish()


class SyncHandler(RequestHandler):
    """Convenient API performing sync on demand."""

    def data_received(self, chunk):
        pass

    def put(self):
        """Answer PUT request."""
        sync_cve_md(self.application.webhook_queue)
        self.finish()


class ReEvaluateHandler(RequestHandler):
    """Convenient API to schedule re-evaluation for all systems."""

    def data_received(self, chunk):
        pass

    def put(self):
        """Answer PUT request."""
        asyncio.ensure_future(self.application.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
        self.finish()


class DeleteHandler(RequestHandler):
    """Convenient API to delete CVEs"""

    def data_received(self, chunk):
        pass

    def delete(self, **kwargs):
        """Answer DELETE request"""
        cve_names = kwargs['cve_names']
        delete_cve(cve_names)
        self.finish()


class ServerApplication(Application):
    """Websocket client application."""

    def __init__(self):
        handlers = [
            (r"/api/v1/monitoring/health/?", HealthHandler),
            (r"/api/v1/sync/?", SyncHandler),
            (r"/api/v1/re-evaluate/?", ReEvaluateHandler),
            (r"/api/v1/cves/?", DeleteHandler)
        ]
        Application.__init__(self, handlers)
        self.instance = IOLoop.instance()
        self.vmaas_websocket_url = "ws://%s/" % os.getenv("VMAAS_WEBSOCKET_HOST", "vmaas_websocket:8082")
        self.vmaas_websocket = None
        self.reconnect_callback = None
        self.evaluator_queue = None
        self.webhook_queue = None
        self.listener_queue = None

    def start(self):
        """Start websocket server."""
        # Sync CVEs always when app starts
        self.evaluator_queue = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)
        self.webhook_queue = mqueue.MQWriter(mqueue.WEBHOOKS_TOPIC)
        self.listener_queue = mqueue.MQWriter(mqueue.EVENTS_TOPIC)
        sync_cve_md(self.webhook_queue)
        self._websocket_reconnect()
        self.reconnect_callback = PeriodicCallback(self._websocket_reconnect,
                                                   WEBSOCKET_RECONNECT_INTERVAL * 1000)
        self.reconnect_callback.start()
        self.instance.start()

    async def stop(self):
        """Stop platform mock server."""
        await self.evaluator_queue.stop()
        await self.webhook_queue.stop()
        if self.vmaas_websocket is not None:
            self.vmaas_websocket.close()
            self.vmaas_websocket = None
            LOGGER.info("Websocket connection closed.")
        self.instance.stop()

    def _websocket_reconnect(self):
        """Connect to given websocket, set message handler and callback."""
        if self.vmaas_websocket is None:
            CNX_RECONNECT.inc()
            websocket_connect(self.vmaas_websocket_url,
                              on_message_callback=self._read_websocket_message,
                              callback=self._websocket_connect_status)

    def _websocket_connect_status(self, future):
        """Check if connection attempt succeeded."""
        try:
            result = future.result()
        except:  # noqa: E722 pylint: disable=bare-except
            result = None

        if result is None:
            # TODO: print the traceback as debug message when we use logging module instead of prints here
            CNX_FAIL.inc()
            LOGGER.warning("Unable to connect to: %s", self.vmaas_websocket_url)
        else:
            LOGGER.info("Connected to: %s", self.vmaas_websocket_url)
            result.write_message("subscribe-listener")

        self.vmaas_websocket = result

    @staticmethod
    def select_repo_based_inventory_ids(cur, repos: list):
        """Select inventory-ids connected with inserted repos, don't fetch it."""
        if repos:
            cur.execute("""select inventory_id from system_platform where id in
                           (select distinct system_id from system_repo where repo_id in
                           (select id from repo where name in %s))""", (tuple(repos),))
        else:
            cur.execute("""select * from system_repo where (1=0)""")  # ensure empty result

    @staticmethod
    def select_all_inventory_ids(cur):
        """Select all inventory-ids, don't fetch it."""
        cur.execute("select inventory_id from system_platform")

    @staticmethod
    def get_last_repobased_eval_tms(cur):
        """Select last repo-based evaluation timestamp."""
        cur.execute("select value from timestamp_kv where name = %s", (constants.TIMESTAMP_LAST_REPO_BASED_EVAL,))
        ret = cur.fetchone()
        if ret:
            return ret[0]
        return None

    @staticmethod
    def set_last_repobased_eval_tms(cur, timestamp: dt.datetime):
        """Update last repo-based evaluation timestamp."""
        cur.execute("""insert into timestamp_kv (name, value) values (%s, %s)
                    on conflict (name) do update set value = %s returning value, (xmax = 0) as inserted""",
                    (constants.TIMESTAMP_LAST_REPO_BASED_EVAL, timestamp, timestamp))
        ret = cur.fetchone()
        return ret

    @staticmethod
    def _vmaas_repos_modified_since(modified_since: str) -> list:
        """Get list of modified repose since `modified since`"""
        repos_json = {"repository_list": [".*"], "page": 1, "page_size": DEFAULT_PAGE_SIZE,
                      "modified_since": modified_since}
        success, repos_pages = paging(VMAAS_REPOS_ENDPOINT, repos_json)
        if not success:
            return []
        repos = [repo_name_key for repo_name_key in repos_pages["repository_list"]]
        LOGGER.info("%d repos found updated since %s", len(repos), modified_since)
        return repos

    def _get_updated_repos(self, conn) -> list:
        """Get repos updated since last repo-based evaluation"""
        with conn.cursor() as cur:
            modified_since_dt = self.get_last_repobased_eval_tms(cur)
            # last modified timestamp
            if modified_since_dt is None:
                modified_since_dt = dt.datetime.utcfromtimestamp(0).replace(tzinfo=dt.timezone.utc)
                # modified time is current time
            repos = self._vmaas_repos_modified_since(modified_since_dt.isoformat())
            # list of modified repos
            self.set_last_repobased_eval_tms(cur, dt.datetime.now())
            return repos

    async def re_evaluate_systems(self, repo_based: bool):
        """Schedule re-evaluation for all systems in DB."""
        with DatabasePoolConnection() as conn:
            if repo_based:
                updated_repos = self._get_updated_repos(conn)

            with NamedCursor(conn) as cur:
                if repo_based:
                    LOGGER.info("Re-evaluating in repo-based mode")
                    self.select_repo_based_inventory_ids(cur, updated_repos)
                else:
                    LOGGER.info("Re-evaluating all systems")
                    self.select_all_inventory_ids(cur)
                total_scheduled = 0
                while True:
                    await RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.acquire()
                    rows = cur.fetchmany(size=RE_EVALUATION_KAFKA_BATCH_SIZE)
                    if not rows:
                        RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release()
                        break
                    msgs = [{"type": "re-evaluate_system", "host": {"id": inventory_id}} for inventory_id, in rows]
                    total_scheduled += len(msgs)
                    future = self.evaluator_queue.send_list(msgs)
                    future.add_done_callback(lambda x: RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release())
                LOGGER.info("%s systems scheduled for re-evaluation", total_scheduled)
            conn.commit()

    def _read_websocket_message(self, message):
        """Read incoming websocket messages."""
        future = None
        if message is not None:
            if message == "webapps-refreshing":
                self.listener_queue.send("webapps-refreshing")
            elif message == "webapps-refreshed":
                REFRESH.inc()
                self.listener_queue.send("webapps-refreshed")
                LOGGER.info("VMaaS cache refreshed")
                sync_cve_md(self.webhook_queue)
                if ENABLE_RE_EVALUATION:
                    future = asyncio.ensure_future(self.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
                else:
                    LOGGER.info("Re-evaluation is disabled, skipping")
        else:
            CNX_FAIL.inc()
            LOGGER.warning("Connection to %s closed: %s (%s)", self.vmaas_websocket_url,
                           self.vmaas_websocket.close_reason, self.vmaas_websocket.close_code)
            self.vmaas_websocket = None
        return future


def main():
    """Main VMaaS listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    LOGGER.info("Starting VMaaS sync service.")
    with DatabasePool(1):
        app = ServerApplication()
        app.listen(8000)

        def terminate(*_):
            """Trigger shutdown."""
            LOGGER.info("Signal received, stopping application.")
            IOLoop.instance().add_callback_from_signal(app.stop)

        signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
        for sig in signals:
            signal.signal(sig, terminate)

        app.start()
    LOGGER.info("Shutting down.")


if __name__ == '__main__':
    main()
