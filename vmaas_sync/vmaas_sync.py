"""VMaaS websocket listener module."""

import asyncio
import datetime as dt
from distutils.util import strtobool  # pylint: disable=import-error, no-name-in-module
import os
import signal

from prometheus_client import Counter, start_http_server
from psycopg2.extras import execute_values, Json

from tornado.ioloop import IOLoop, PeriodicCallback
from tornado.web import Application, RequestHandler
from tornado.websocket import websocket_connect

from common.database_handler import DatabasePool, DatabasePoolConnection, NamedCursor
from common.logging import init_logging, get_logger
from common.paging import paging
from common.utils import ensure_minimal_schema_version
from common import mqueue, constants

WEBSOCKET_RECONNECT_INTERVAL = 60
LOGGER = get_logger(__name__)

PROMETHEUS_PORT = os.getenv('PROMETHEUS_PORT', '8087')

REFRESH = Counter('ve_vmaas_sync_webscan_refreshes', '# of times VMaaS told us it had new data')
CNX_FAIL = Counter('ve_vmaas_sync_websocket_failures', '# of times VMaaS websocket closed on us')
CNX_RECONNECT = Counter('ve_vmaas_sync_websocket_recnx', '# of times attempted VMaaS websocket reconnect')

VMAAS_HOST = os.getenv('VMAAS_HOST', 'http://vmaas_webapp:8080')
VMAAS_CVES_ENDPOINT = "%s/api/v1/cves" % VMAAS_HOST
VMAAS_REPOS_ENDPOINT = "%s/api/v1/repos" % VMAAS_HOST
ENABLE_RE_EVALUATION = strtobool(os.getenv("ENABLE_RE_EVALUATION", "YES"))
ENABLE_REPO_BASED_RE_EVALUATION = strtobool(os.getenv("ENABLE_REPO_BASED_RE_EVALUATION", "NO"))
DEFAULT_PAGE_SIZE = int(os.getenv("DEFAULT_PAGE_SIZE", "5000"))

# how many systems to select in one batch and send to kafka
RE_EVALUATION_KAFKA_BATCH_SIZE = int(os.getenv("RE_EVALUATION_KAFKA_BATCH_SIZE", "10000"))
# how many batches to create at the same time
RE_EVALUATION_KAFKA_BATCHES = int(os.getenv("RE_EVALUATION_KAFKA_BATCHES", "10"))
RE_EVALUATION_KAFKA_BATCH_SEMAPHORE = asyncio.BoundedSemaphore(RE_EVALUATION_KAFKA_BATCHES)


def construct_cve_row(cve, impact_id_map):
    """Helper function to construct CVE row to be insterted/updated in the DB"""
    description = cve['description']
    impact_id = impact_id_map[cve['impact']]
    public_date = cve['public_date'] or None
    modified_date = cve['modified_date'] or None
    cvss3_score = float(cve['cvss3_score']) if cve.get('cvss3_score') else None
    cvss3_metrics = cve.get('cvss3_metrics')
    cvss2_score = float(cve['cvss2_score']) if cve.get('cvss2_score') else None
    cvss2_metrics = cve.get('cvss2_metrics')
    redhat_url = cve.get('redhat_url', None)
    secondary_url = cve.get('secondary_url', None)
    advisories_list = Json(cve.get('errata_list', None))
    return (cve['synopsis'], description, impact_id, public_date, modified_date, cvss3_score, cvss3_metrics,
            cvss2_score, cvss2_metrics, redhat_url, secondary_url, advisories_list)


def insert_cves(cur, to_insert):
    """Insert CVEs into DB"""
    if to_insert:
        execute_values(cur, """insert into cve_metadata
                            (cve, description, impact_id, public_date, modified_date,
                            cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                            secondary_url, advisories_list)
                            values %s""", to_insert, page_size=len(to_insert))


def update_cves(cur, to_update):
    """Update already present CVEs in DB"""
    if to_update:
        execute_values(cur, """update cve_metadata set description = data.description,
                                impact_id = data.impact_id,
                                public_date = cast(data.public_date as timestamp with time zone),
                                modified_date = cast(data.modified_date as timestamp with time zone),
                                cvss3_score = cast(data.cvss3_score as numeric),
                                cvss3_metrics = data.cvss3_metrics,
                                cvss2_score = cast(data.cvss2_score as numeric),
                                cvss2_metrics = data.cvss2_metrics,
                                redhat_url = data.redhat_url,
                                secondary_url = data.secondary_url,
                                advisories_list = cast(data.advisories_list as JSONB)
                                from (values %s) as data
                                (cve, description, impact_id, public_date, modified_date,
                                cvss3_score, cvss3_metrics, cvss2_score, cvss2_metrics, redhat_url,
                                secondary_url, advisories_list)
                                where cve_metadata.cve = data.cve""",
                       to_update, page_size=len(to_update))


def process_cve_list(cves, cves_in_db, impact_id_map):
    """Processes cve list returned from VMaaS"""
    to_insert = []
    to_update = []
    to_delete = []
    for cve in cves:
        row = construct_cve_row(cves[cve], impact_id_map)
        if cve not in cves_in_db:
            to_insert.append(row)
        else:
            to_update.append(row)
    to_delete = [(cve,) for cve in cves_in_db if cve not in cves]
    return to_insert, to_update, to_delete


def sync_cve_md():
    """Sync all CVE metadata from VMaaS"""
    LOGGER.info('Syncing CVE metadata')
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            impact_id_map = {}
            cur.execute("select name, id from cve_impact")
            for impact_name, impact_id in cur.fetchall():
                impact_id_map[impact_name] = impact_id
            cur.execute('select id, cve from cve_metadata')
            cves_in_db = {}
            for cve_tuple in cur.fetchall():
                cves_in_db[cve_tuple[1]] = cve_tuple[0]
            cve_json = {'cve_list': [".*"], 'page': 1, 'page_size': DEFAULT_PAGE_SIZE, 'rh_only': True, 'errata_associated': True}
            success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
            if not success:
                return success
            cves = cve_pages['cve_list']
            LOGGER.info("Importing CVE metadata")

            to_insert, to_update, to_delete = process_cve_list(cves, cves_in_db, impact_id_map)

            insert_cves(cur, to_insert)
            update_cves(cur, to_update)

            if to_delete:
                associated_cves = set()
                LOGGER.info("Deleting %s unnecessary CVE metadata", len(to_delete))
                cur.execute("""select distinct cve_id from system_vulnerabilities""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                cur.execute("""select distinct cve_id from cve_rule_mapping""")
                for row in cur.fetchall():
                    associated_cves.add(row[0])
                safety_delete = []
                unable_to_delete = []
                for cve_to_delete in to_delete:
                    cve_id = cves_in_db[cve_to_delete[0]]
                    if cve_id in associated_cves:
                        unable_to_delete.append(cve_to_delete[0])
                    else:
                        safety_delete.append(cve_id)
                if safety_delete:
                    execute_values(cur, """delete from cve_account_data
                                           where cve_id in (%s)""",
                                   list(zip(safety_delete)), page_size=len(safety_delete))
                    execute_values(cur, """delete from cve_metadata where id in (%s)""",
                                   list(zip(safety_delete)), page_size=len(safety_delete))
                    LOGGER.info('Finished deleting unnecessary CVE metadata')
                if unable_to_delete:
                    LOGGER.warning(
                        'Unable to delete %s cves (still referenced from system_vulnerabilities table or have rules): %s',
                        len(unable_to_delete), str(unable_to_delete))
                    LOGGER.debug('Attempting to update information about %s', str(unable_to_delete))
                    cve_json = {'cve_list': unable_to_delete, 'page': 1, 'page_size': DEFAULT_PAGE_SIZE, 'rh_only': True}
                    success, cve_pages = paging(VMAAS_CVES_ENDPOINT, cve_json)
                    if not success:
                        return success
                    cves = cve_pages['cve_list']
                    _, to_update, _ = process_cve_list(cves, cves_in_db, impact_id_map)
                    update_cves(cur, to_update)

            conn.commit()
            LOGGER.info('Finished syncing CVE metadata')
            return success


def delete_cve(cve_names: str) -> bool:
    """Delete CVEs"""
    if not cve_names:
        LOGGER.info('Need to specify CVE')
        return False
    cve_list = cve_names.split(',')
    LOGGER.info('Deleting %s CVE metadata', len(cve_list))
    with DatabasePoolConnection() as conn:
        with conn.cursor() as cur:
            success = True
            execute_values(cur, """select id from cve_metadata where cve in (%s)""",
                           list(zip(cve_list)), page_size=len(cve_list))
            cve_ids_to_delete = cur.fetchall()
            if cve_ids_to_delete:
                execute_values(cur, """delete from cve_account_data where cve_id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                execute_values(cur, """delete from system_vulnerabilities where cve_id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
                execute_values(cur, """delete from cve_metadata where id in (%s)""",
                               cve_ids_to_delete, page_size=len(cve_ids_to_delete))
            conn.commit()
    LOGGER.info('Finished deleting CVE metadata')
    return success


class HealthHandler(RequestHandler):
    """Handler class providing health status."""

    def data_received(self, chunk):
        pass

    def get(self):
        """Answer GET request."""
        self.finish()


class SyncHandler(RequestHandler):
    """Convenient API performing sync on demand."""

    def data_received(self, chunk):
        pass

    def put(self):
        """Answer PUT request."""
        sync_cve_md()
        self.finish()


class ReEvaluateHandler(RequestHandler):
    """Convenient API to schedule re-evaluation for all systems."""

    def data_received(self, chunk):
        pass

    def put(self):
        """Answer PUT request."""
        asyncio.ensure_future(self.application.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
        self.finish()


class DeleteHandler(RequestHandler):
    """Convenient API to delete CVEs"""

    def data_received(self, chunk):
        pass

    def delete(self, **kwargs):
        """Answer DELETE request"""
        cve_names = kwargs['cve_names']
        delete_cve(cve_names)
        self.finish()


class ServerApplication(Application):
    """Websocket client application."""

    def __init__(self):
        handlers = [
            (r"/api/v1/monitoring/health/?", HealthHandler),
            (r"/api/v1/sync/?", SyncHandler),
            (r"/api/v1/re-evaluate/?", ReEvaluateHandler),
            (r"/api/v1/cves/?", DeleteHandler)
        ]
        Application.__init__(self, handlers)
        self.instance = IOLoop.instance()
        self.vmaas_websocket_url = "ws://%s/" % os.getenv("VMAAS_WEBSOCKET_HOST", "vmaas_websocket:8082")
        self.vmaas_websocket = None
        self.reconnect_callback = None
        self.evaluator_queue = None

    def start(self):
        """Start websocket server."""
        # Sync CVEs always when app starts
        self.evaluator_queue = mqueue.MQWriter(mqueue.EVALUATOR_TOPIC)
        sync_cve_md()
        self._websocket_reconnect()
        self.reconnect_callback = PeriodicCallback(self._websocket_reconnect,
                                                   WEBSOCKET_RECONNECT_INTERVAL * 1000)
        self.reconnect_callback.start()
        self.instance.start()

    async def stop(self):
        """Stop platform mock server."""
        await self.evaluator_queue.stop()
        if self.vmaas_websocket is not None:
            self.vmaas_websocket.close()
            self.vmaas_websocket = None
            LOGGER.info("Websocket connection closed.")
        self.instance.stop()

    def _websocket_reconnect(self):
        """Connect to given websocket, set message handler and callback."""
        if self.vmaas_websocket is None:
            CNX_RECONNECT.inc()
            websocket_connect(self.vmaas_websocket_url,
                              on_message_callback=self._read_websocket_message,
                              callback=self._websocket_connect_status)

    def _websocket_connect_status(self, future):
        """Check if connection attempt succeeded."""
        try:
            result = future.result()
        except:  # noqa: E722 pylint: disable=bare-except
            result = None

        if result is None:
            # TODO: print the traceback as debug message when we use logging module instead of prints here
            CNX_FAIL.inc()
            LOGGER.warning("Unable to connect to: %s", self.vmaas_websocket_url)
        else:
            LOGGER.info("Connected to: %s", self.vmaas_websocket_url)
            result.write_message("subscribe-listener")

        self.vmaas_websocket = result

    @staticmethod
    def select_repo_based_inventory_ids(cur, repos: list):
        """Select inventory-ids connected with inserted repos, don't fetch it."""
        if repos:
            cur.execute("""select inventory_id from system_platform where
                           when_deleted is null and
                           id in (select distinct system_id from system_repo where repo_id in
                           (select id from repo where name in %s))""", (tuple(repos),))
        else:
            cur.execute("""select * from system_repo where (1=0)""")  # ensure empty result

    @staticmethod
    def select_all_inventory_ids(cur):
        """Select all inventory-ids, don't fetch it."""
        cur.execute("select inventory_id from system_platform where when_deleted is null")

    @staticmethod
    def get_last_repobased_eval_tms(cur):
        """Select last repo-based evaluation timestamp."""
        cur.execute("select value from timestamp_kv where name = %s", (constants.TIMESTAMP_LAST_REPO_BASED_EVAL,))
        ret = cur.fetchone()
        if ret:
            return ret[0]
        return None

    @staticmethod
    def set_last_repobased_eval_tms(cur, timestamp: dt.datetime):
        """Update last repo-based evaluation timestamp."""
        cur.execute("""insert into timestamp_kv (name, value) values (%s, %s)
                    on conflict (name) do update set value = %s returning value, (xmax = 0) as inserted""",
                    (constants.TIMESTAMP_LAST_REPO_BASED_EVAL, timestamp, timestamp))
        ret = cur.fetchone()
        return ret

    @staticmethod
    def _vmaas_repos_modified_since(modified_since: str) -> list:
        """Get list of modified repose since `modified since`"""
        repos_json = {"repository_list": [".*"], "page": 1, "page_size": DEFAULT_PAGE_SIZE,
                      "modified_since": modified_since}
        success, repos_pages = paging(VMAAS_REPOS_ENDPOINT, repos_json)
        if not success:
            return []
        repos = list(repos_pages["repository_list"])
        LOGGER.info("%d repos found updated since %s", len(repos), modified_since)
        return repos

    def _get_updated_repos(self, conn) -> list:
        """Get repos updated since last repo-based evaluation"""
        with conn.cursor() as cur:
            modified_since_dt = self.get_last_repobased_eval_tms(cur)
            # last modified timestamp
            if modified_since_dt is None:
                modified_since_dt = dt.datetime.utcfromtimestamp(0).replace(tzinfo=dt.timezone.utc)
                # modified time is current time
            repos = self._vmaas_repos_modified_since(modified_since_dt.isoformat())
            # list of modified repos
            self.set_last_repobased_eval_tms(cur, dt.datetime.now())
            return repos

    async def re_evaluate_systems(self, repo_based: bool):
        """Schedule re-evaluation for all systems in DB."""
        with DatabasePoolConnection() as conn:
            if repo_based:
                updated_repos = self._get_updated_repos(conn)

            with NamedCursor(conn) as cur:
                if repo_based:
                    LOGGER.info("Re-evaluating in repo-based mode")
                    self.select_repo_based_inventory_ids(cur, updated_repos)
                else:
                    LOGGER.info("Re-evaluating all systems")
                    self.select_all_inventory_ids(cur)
                total_scheduled = 0
                while True:
                    await RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.acquire()
                    rows = cur.fetchmany(size=RE_EVALUATION_KAFKA_BATCH_SIZE)
                    if not rows:
                        RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release()
                        break
                    msgs = [{"type": "re-evaluate_system", "host": {"id": inventory_id}} for inventory_id, in rows]
                    total_scheduled += len(msgs)
                    future = self.evaluator_queue.send_list(msgs)
                    future.add_done_callback(lambda x: RE_EVALUATION_KAFKA_BATCH_SEMAPHORE.release())
                LOGGER.info("%s systems scheduled for re-evaluation", total_scheduled)
            conn.commit()

    def _read_websocket_message(self, message):
        """Read incoming websocket messages."""
        future = None
        if message is not None:
            if message == "webapps-refreshed":
                REFRESH.inc()
                LOGGER.info("VMaaS cache refreshed")
                sync_cve_md()
                if ENABLE_RE_EVALUATION:
                    future = asyncio.ensure_future(self.re_evaluate_systems(ENABLE_REPO_BASED_RE_EVALUATION))
                else:
                    LOGGER.info("Re-evaluation is disabled, skipping")
        else:
            CNX_FAIL.inc()
            LOGGER.warning("Connection to %s closed: %s (%s)", self.vmaas_websocket_url,
                           self.vmaas_websocket.close_reason, self.vmaas_websocket.close_code)
            self.vmaas_websocket = None
        return future


def main():
    """Main VMaaS listener entrypoint."""
    start_http_server(int(PROMETHEUS_PORT))
    init_logging()
    ensure_minimal_schema_version()
    LOGGER.info("Starting VMaaS sync service.")
    with DatabasePool(1):
        app = ServerApplication()
        app.listen(8000)

        def terminate(*_):
            """Trigger shutdown."""
            LOGGER.info("Signal received, stopping application.")
            IOLoop.instance().add_callback_from_signal(app.stop)

        signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
        for sig in signals:
            signal.signal(sig, terminate)

        app.start()
    LOGGER.info("Shutting down.")


if __name__ == '__main__':
    main()
